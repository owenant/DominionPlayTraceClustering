{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/owenant/PlayTraces/blob/main/DominionPlayTraceClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3d63dad0"
      },
      "outputs": [],
      "source": [
        "#This notebook calculates silhouette averages and clusters for Card Count and NGram playtraces for Dominion produced by the\n",
        "#TableTop Games framework"
      ],
      "id": "3d63dad0"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy3-UJsWmTH0",
        "outputId": "9b395c1e-defa-4d30-e2ea-f480a233065f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn-extra in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn-extra"
      ],
      "id": "Dy3-UJsWmTH0"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d792f3be",
        "outputId": "74984a7e-5538-4417-a1c3-92de63f0d615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pdb\n",
        "import re\n",
        "import nltk\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from itertools import product, permutations, combinations\n",
        "from nltk import ngrams\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from google.colab import drive\n",
        "import csv\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "id": "d792f3be"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duv7Pp_FAnZc",
        "outputId": "7eaec260-24c0-4321-d2ec-2e6a593c7e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_KMeans/Rand_vs_Rand_GPM250_DomFix_SD/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_KMedoids/Rand_vs_Rand_GPM250_DomFix_SD/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_DBSCAN/Rand_vs_Rand_GPM250_DomFix_SD/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_SPClustering/Rand_vs_Rand_GPM250_DomFix_SD/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/RoundAndScoreDistributions/Rand_vs_Rand_GPM250_DomFix_SD/' created successfully.\n"
          ]
        }
      ],
      "source": [
        "#list of clustering methods to use\n",
        "clustering_methods = ['KMeans', 'KMedoids', 'DBSCAN', 'SPClustering']\n",
        "\n",
        "#filenames and directory locations\n",
        "google_drive_parent_dir = \"gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/\"\n",
        "card_count_data_dir = google_drive_parent_dir + \"DataCardCount/\"\n",
        "ngram_data_dir = google_drive_parent_dir + \"DataNGrams/\"\n",
        "card_count_data_filename = card_count_data_dir + \"trace_logfile_Rand_vs_Rand_GPM250_DomFix_SD.txt\"\n",
        "ngram_data_filename = ngram_data_dir + \"ActionsReduced_Rand_vs_Rand_GPM250_DomFix_SD.csv\"\n",
        "tag_for_dir_and_filenames = 'Rand_vs_Rand_GPM250_DomFix_SD'\n",
        "#card_count_data_filename = card_count_data_dir + \"trace_logfile_Budget_500_vs_Budget_500_GPM100_SD_NoSelfPlay.txt\"\n",
        "#ngram_data_filename = ngram_data_dir + \"ActionsReduced_Budget500_vs_Budget500_GPM100_SD.csv\"\n",
        "#tag_for_dir_and_filenames = 'MCTS_b500_vs_b500_GPM100_SD'\n",
        "\n",
        "#create new directory for output files\n",
        "for method in clustering_methods:\n",
        "  new_dir_path = google_drive_parent_dir + 'Results_' + method + '/' + tag_for_dir_and_filenames + '/'\n",
        "  os.makedirs(new_dir_path, exist_ok=True)\n",
        "\n",
        "  # Verify that the directory has been created\n",
        "  if os.path.exists(new_dir_path):\n",
        "      print(f\"Directory '{new_dir_path}' created successfully.\")\n",
        "  else:\n",
        "      print(f\"Failed to create directory '{new_dir_path}'.\")\n",
        "\n",
        "#also create directory for round and score distributions\n",
        "new_dir_path = google_drive_parent_dir + 'RoundAndScoreDistributions/' + tag_for_dir_and_filenames + '/'\n",
        "os.makedirs(new_dir_path, exist_ok=True)\n",
        "\n",
        "# Verify that the directory has been created\n",
        "if os.path.exists(new_dir_path):\n",
        "    print(f\"Directory '{new_dir_path}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Failed to create directory '{new_dir_path}'.\")"
      ],
      "id": "duv7Pp_FAnZc"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lM1jnEiFsQwI"
      },
      "outputs": [],
      "source": [
        "#parameters for notebook execution\n",
        "\n",
        "#game IDs to remove if needed\n",
        "gamesids_to_delete = []\n",
        "\n",
        "#kingdom card set\n",
        "kingdom_set = 'SD'\n",
        "\n",
        "#parameters if using TAG input data\n",
        "logs_from_tag = True\n",
        "agent_names = ['Random', 'Random']\n",
        "games_per_matchup = 250\n",
        "no_self_play = True\n",
        "\n",
        "#parameters for grid search for clustering methods\n",
        "\n",
        "#number of clusters to check across all clustering methods (excluding DBSCAN)\n",
        "clusters_min = 2\n",
        "clusters_max = 5\n",
        "clusters_stepsize = 1\n",
        "\n",
        "#DBSCAN\n",
        "minpts_min = 25\n",
        "minpts_max = 250\n",
        "minpts_stepsize = 10\n",
        "epsilon_min = 0.05\n",
        "epsilon_max = 1\n",
        "epsilon_stepsize = 0.025\n",
        "\n",
        "#Spectral clustering K-Nearest Neighbours\n",
        "nearest_neighbours_min = 25\n",
        "nearest_neighbours_max = 250\n",
        "nearest_neighbours_stepsize = 10\n",
        "\n",
        "#Spectral clustering radial basis function\n",
        "gamma_min = 0.05\n",
        "gamma_max = 1\n",
        "gamma_stepsize = 0.025\n",
        "\n",
        "#number of N-gram types to search over\n",
        "ngram_min = 1\n",
        "ngram_max = 3\n",
        "ngram_stepsize = 1\n",
        "\n",
        "#values of k for l_k norm\n",
        "k_norms = [0.5, 1, 2]\n",
        "\n",
        "#threshold values for plotting probability distributions for N-Gram playtraces\n",
        "thresholds = {1: 0.01, 2:0.01, 3:0.015}"
      ],
      "id": "lM1jnEiFsQwI"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "m_oEYop-1Ma2"
      },
      "outputs": [],
      "source": [
        "#kingdom card types\n",
        "card_types_SD = ['ARTISAN', 'BANDIT', 'BUREAUCRAT', 'CHAPEL', 'FESTIVAL', 'GARDENS', 'SENTRY', 'THRONE_ROOM', 'WITCH',\n",
        "                 'WORKSHOP', 'CURSE', 'PROVINCE', 'DUCHY', 'ESTATE', 'GOLD', 'SILVER', 'COPPER']\n",
        "card_types_FG1E = ['CELLAR','MARKET','MILITIA','MINE','MOAT','REMODEL','SMITHY','VILLAGE',\n",
        "                'WOODCUTTER','WORKSHOP','CURSE','PROVINCE', 'DUCHY', 'ESTATE', 'GOLD', 'SILVER', 'COPPER']\n",
        "\n",
        "if kingdom_set == 'SD':\n",
        "  card_types = card_types_SD\n",
        "elif kingdom_set == 'FG1E':\n",
        "  card_types = card_types_FG1E\n",
        "else:\n",
        "  print('Unrecognised kingdom card set')"
      ],
      "id": "m_oEYop-1Ma2"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HxQYGYs8wAIV"
      },
      "outputs": [],
      "source": [
        "#functions to process data from TAG\n",
        "\n",
        "def gameID_to_matchup(game_id, player_no, matchup_list, no_games_per_matchup, min_game_id):\n",
        "    game_group = int((game_id - min_game_id)/no_games_per_matchup)\n",
        "    matchup = matchup_list[game_group]\n",
        "    agent1, agent2 = matchup\n",
        "    if player_no == 0:\n",
        "        return agent1\n",
        "    else:\n",
        "        return agent2\n",
        "\n",
        "def add_TAG_agent_names(agent_names, games_per_match_up, no_self_play, logs_from_tag, data):\n",
        "  NoOfGames = len(data['GameID'].unique())\n",
        "  min_GameID = data['GameID'].min()\n",
        "\n",
        "  #first generate match-ups\n",
        "  matchups = []\n",
        "  if no_self_play:\n",
        "    matchups = list(permutations(agent_names, 2))\n",
        "  else:\n",
        "      for agent1 in agent_names:\n",
        "          for agent2 in agent_names:\n",
        "              matchups.append((agent1, agent2))\n",
        "\n",
        "  #add agent names to data set\n",
        "  data['AgentName'] = data.apply(lambda row: gameID_to_matchup(row['GameID'], row['Player'], matchups, games_per_matchup, min_GameID), axis = 1)\n",
        "\n",
        "  #finally we also add the name of the agent of the opponent\n",
        "  min_GameID = data['GameID'].min()\n",
        "  data['Opponent'] = data.apply(lambda row: 1.0 if row['Player'] == 0.0 else 0.0, axis = 1)\n",
        "  if logs_from_tag:\n",
        "      data['AgentNameOpponent'] = data.apply(lambda row: gameID_to_matchup(row['GameID'], row['Opponent'], matchups, games_per_matchup, min_GameID), axis = 1)\n",
        "  else:\n",
        "      gameid_to_players_dict = data.groupby('GameID')['AgentName'].apply(list).to_dict()\n",
        "      data['AgentNameOpponent'] = data.apply(lambda row: other_dict_element(gameid_to_players_dict, row['GameID'], row['AgentName']), axis = 1)\n",
        "\n",
        "\n",
        "#functions to process card count data\n",
        "def copy_final_deck_at_game_end(group, roundMax, noPlayers):\n",
        "  #This function repeatedly copies the final decks of two players at the game end, so that the game is extended to\n",
        "  #have roundMax rounds\n",
        "  final_round = int(group['Round'].max())\n",
        "  if (roundMax-1) == final_round:\n",
        "      #in this case we dont need to extend the play trace\n",
        "      return group\n",
        "  else:\n",
        "      final_row_copy = pd.concat([group.iloc[-noPlayers:]] * ((roundMax-1) - final_round), ignore_index=True)\n",
        "      #we need to update the Round counter so that every other row it increments by one\n",
        "      final_row_copy['Round'] = [final_round + 1 + i // 2 for i in range(((roundMax-1) - final_round)*2)]\n",
        "      return pd.concat([group, final_row_copy], ignore_index=True)\n",
        "\n",
        "#given a dictionary whose elements are lists of length two, grab the other element not given by elem\n",
        "def other_dict_element(my_dict, my_key, my_elem):\n",
        "    index_of_given_element = my_dict[my_key].index(my_elem)\n",
        "    index_of_other_element =  1 if (index_of_given_element == 0) else 0\n",
        "    return my_dict[my_key][index_of_other_element]\n",
        "\n",
        "def process_card_count_data(cardcount_filename, agent_names, games_per_matchup,\n",
        "                            no_self_play, card_types, logs_from_tag, games_to_remove = []):\n",
        "  data  = pd.read_csv(cardcount_filename, sep = '\\t')\n",
        "  add_TAG_agent_names(agent_names, games_per_matchup, no_self_play, logs_from_tag, data)\n",
        "  index_cols = ['Player', 'GameID']\n",
        "  non_card_types_round_indep_cols = ['AgentName', 'AgentNameOpponent', 'Win', 'FinalScore', 'TotalRounds']\n",
        "  cols = index_cols + non_card_types_round_indep_cols + ['Round'] + card_types #final set of cols to keep\n",
        "  data = data[data['Turn'] == 1] #only want cards at end of round\n",
        "  data = data.loc[:, cols]\n",
        "\n",
        "  #remove given list of gameIDs\n",
        "  data = data[~data['GameID'].isin(games_to_remove)]\n",
        "\n",
        "  #freeze decks and copy to max round number\n",
        "  no_players = 2\n",
        "  gameLengths = data.groupby(['GameID'])['Round'].max()\n",
        "  maxNoOfRounds = int(gameLengths.max()) + 1 #round counter starts at zero\n",
        "  noOfGames = len(data['GameID'].unique())\n",
        "  data = data.groupby('GameID').apply(copy_final_deck_at_game_end, maxNoOfRounds, no_players).reset_index(drop = True)\n",
        "\n",
        "  #check shape of data\n",
        "  print(\"Card count shape check:\")\n",
        "  print(\"Expected no rows: \" + str(maxNoOfRounds*no_players*noOfGames))\n",
        "  print(\"Expected no of cols: \" + str(len(card_types)+8))\n",
        "  print(data.shape)\n",
        "\n",
        "  return data\n",
        "\n",
        "def flatten_card_count_data(cardcount_data, card_types):\n",
        "  #next we need to flatten our data so that each trace is a single row.\n",
        "  #We also drop the round label as it is redundant\n",
        "  #and it will get reintroduced when flattening through the revised column names\n",
        "\n",
        "  #first create dataframe consisting of only non card type data types that are round\n",
        "  #independent\n",
        "  index_cols = ['Player', 'GameID']\n",
        "  non_card_types_round_indep_cols = ['AgentName', 'AgentNameOpponent', 'Win', 'FinalScore', 'TotalRounds']\n",
        "  non_card_data_round_indep = cardcount_data[index_cols + non_card_types_round_indep_cols].drop_duplicates()\n",
        "\n",
        "  #next need to Group by Player and GameID and then flatten card data by round\n",
        "  traces_tmp = cardcount_data[index_cols + card_types]\n",
        "  gameLengths = cardcount_data.groupby(['GameID'])['Round'].max()\n",
        "  maxNoOfRounds = int(gameLengths.max()) + 1 #round counter starts at zero\n",
        "  cols = [card_types[i] + \"_R\" + str(r)\n",
        "          for r in range(0, maxNoOfRounds) for i in range(0, len(card_types))]\n",
        "\n",
        "  extended_traces_flat = traces_tmp.groupby(index_cols).apply(lambda df: df[card_types].values.flatten())\n",
        "  extended_traces_flat = pd.DataFrame(extended_traces_flat, columns = ['Trace']).reset_index()\n",
        "  extended_traces_flat = pd.concat([extended_traces_flat[index_cols], extended_traces_flat['Trace'].apply(pd.Series)], axis=1)\n",
        "  extended_traces_flat.columns = index_cols + cols\n",
        "\n",
        "  #next we add back in the round independent data\n",
        "  extended_traces_flat = pd.merge(non_card_data_round_indep, extended_traces_flat, on = index_cols)\n",
        "\n",
        "  return extended_traces_flat\n",
        "\n",
        "#functions to process NGram data\n",
        "def format_action(action, cardtypes):\n",
        "  #there are various types of actions we need to format to identify these we use\n",
        "  #regular expressions\n",
        "  pattern_list = []\n",
        "  pattern_list.append(re.compile(r'End Current Phase'))\n",
        "  pattern_list.append(re.compile(r'BuyCard: (' + '|'.join(cardtypes) + r') by player (0|1)'))\n",
        "  pattern_list.append(re.compile(r'(' + '|'.join(cardtypes) + r') : Player (0|1)'))\n",
        "  pattern_list.append(re.compile(r'GainCard: (' + '|'.join(cardtypes) + r') by player (0|1)'))\n",
        "  pattern_list.append(re.compile(r'Player (0|1) trashes a (' + '|'.join(cardtypes) + r') from (?:HAND|DISCARD)'))\n",
        "  pattern_list.append(re.compile(r'DoNothing'))\n",
        "  pattern_list.append(re.compile(r'Player (0|1) moves (' + '|'.join(cardtypes) + r') from HAND to DRAW of player (0|1) \\(visible: (?:true|false)\\)'))\n",
        "  pattern_list.append(re.compile(r'Reveals Hand'))\n",
        "  pattern_list.append(re.compile(r'Sentry .*$')) #captures playing a sentry and then discard/trash two cards\n",
        "  pattern_list.append(re.compile(r'Player (0|1) discards (' + '|'.join(cardtypes) + r')'))\n",
        "  pattern_list.append(re.compile(r'Player (0|1) reveals a (' + '|'.join(cardtypes) + r')'))\n",
        "\n",
        "  match_list = [None] * len(pattern_list)\n",
        "\n",
        "  pattern_to_string_map = ['ECP', 'BUY', 'PLAY', 'GAIN', 'TRASHES',\n",
        "                            'DONOTHING', 'MOVES', 'REVEALSHAND', 'PLAYSSENTRY',\n",
        "                            'DISCARDS', 'REVEALS']\n",
        "\n",
        "  for index in range(0, len(pattern_list)):\n",
        "    matched = pattern_list[index].match(action)\n",
        "    pattern_index = index\n",
        "    if matched != None:\n",
        "      break\n",
        "\n",
        "  if matched == None:\n",
        "    pdb.set_trace()\n",
        "    raise Exception(\"Can't match action description\")\n",
        "\n",
        "  if pattern_index in [0, 5, 7, 8]:\n",
        "    formatted_action =  pattern_to_string_map[pattern_index]\n",
        "  elif pattern_index in [1, 2, 3]:\n",
        "    matched_card =  matched.group(1)\n",
        "    formatted_action =  pattern_to_string_map[pattern_index]  + matched_card\n",
        "  else:\n",
        "    matched_card = matched.group(2)\n",
        "    formatted_action =  pattern_to_string_map[pattern_index]  + matched_card\n",
        "\n",
        "  return formatted_action\n",
        "\n",
        "def process_ngram_data(actions_filename, agent_names, games_per_match_up, no_self_play,\n",
        "                       card_types, ngram_min, ngram_max, ngram_stepsize, logs_from_tag, games_to_remove = []):\n",
        "  data  = pd.read_csv(actions_filename)\n",
        "  data = data[['GameID', 'Player', 'Round','Turn','ActionDescription']]\n",
        "  add_TAG_agent_names(agent_names, games_per_match_up, no_self_play, logs_from_tag, data)\n",
        "  data['ProcAction'] = data.apply(lambda row: format_action(row['ActionDescription'], card_types), axis = 1)\n",
        "  data = data.groupby(['GameID', 'Player','AgentName', 'AgentNameOpponent'])['ProcAction'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "  #remove given list of gameIDs\n",
        "  data = data[~data['GameID'].isin(games_to_remove)]\n",
        "\n",
        "  for n in range(ngram_min, ngram_max +1, ngram_stepsize):\n",
        "    col_name = 'NGrams_' + str(n)\n",
        "    data[col_name] = data.apply(lambda row: list(ngrams(nltk.word_tokenize(row['ProcAction']),n)), axis = 1)\n",
        "\n",
        "  return data\n"
      ],
      "id": "HxQYGYs8wAIV"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3I3J5tKvfoA",
        "outputId": "30f7cbb8-da76-495c-ec81-928f4ec12313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Card count shape check:\n",
            "Expected no rows: 62000\n",
            "Expected no of cols: 25\n",
            "(62000, 25)\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "#process data\n",
        "traces_cardcount = process_card_count_data(card_count_data_filename, agent_names, games_per_matchup, no_self_play, card_types, logs_from_tag, gamesids_to_delete)\n",
        "traces_ngrams = process_ngram_data(ngram_data_filename, agent_names, games_per_matchup, no_self_play,\n",
        "                       card_types, ngram_min, ngram_max, ngram_stepsize, logs_from_tag, gamesids_to_delete)\n",
        "print(len(traces_ngrams))"
      ],
      "id": "t3I3J5tKvfoA"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZguwBNUQeveK"
      },
      "outputs": [],
      "source": [
        "#function to plot score and round distributions and output to file\n",
        "def plot_score_round_distributions(outputfilename, card_count_data):\n",
        "  fig, axs = plt.subplots(2, 1)\n",
        "  grouped_data = card_count_data.groupby('GameID')\n",
        "  score_data = grouped_data['FinalScore'].unique().explode()\n",
        "  axs[0].hist(score_data, bins=np.arange(score_data.min(), score_data.max()+1))\n",
        "  axs[0].set_xlabel('Final score')\n",
        "  axs[0].set_ylabel('Number of games')\n",
        "  axs[0].set_title('Score distribution')\n",
        "  round_data = grouped_data['TotalRounds'].unique().explode()\n",
        "  axs[1].hist(round_data, bins=np.arange(round_data.min(), round_data.max()+1))\n",
        "  axs[1].set_xlabel('Number of rounds')\n",
        "  axs[1].set_ylabel('Number of games')\n",
        "  axs[1].set_title('Round distribution')\n",
        "  fig.tight_layout()\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n"
      ],
      "id": "ZguwBNUQeveK"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwExLtTpgSeO",
        "outputId": "04d586b8-a316-43a9-f141-583773390017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round and score distribution before outliers removed:\n"
          ]
        }
      ],
      "source": [
        "#remove outliers based on thresholds for score and length of game\n",
        "score_threshold = 300\n",
        "round_threshold = 300\n",
        "print(\"Round and score distribution before outliers removed:\")\n",
        "outputfilename = google_drive_parent_dir + 'RoundAndScoreDistributions/' + tag_for_dir_and_filenames + '/' + 'RoundAndScoreDistribution_' + tag_for_dir_and_filenames\n",
        "plot_score_round_distributions(outputfilename, traces_cardcount)\n",
        "\n",
        "traces_cardcount = traces_cardcount[(traces_cardcount['FinalScore'] <= score_threshold)\n",
        "                                           & (traces_cardcount['TotalRounds'] <= round_threshold)]\n",
        "new_game_id_list =  traces_cardcount['GameID'].unique()\n",
        "traces_ngrams = traces_ngrams[traces_ngrams['GameID'].isin(new_game_id_list)]"
      ],
      "id": "FwExLtTpgSeO"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSWZqXOBksgl",
        "outputId": "b231ff82-c8ab-4e38-8b2b-7176973dcd30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round and score distribution after outliers removed:\n"
          ]
        }
      ],
      "source": [
        "print(\"Round and score distribution after outliers removed:\")\n",
        "outputfilename = google_drive_parent_dir + 'RoundAndScoreDistributions/' + tag_for_dir_and_filenames + '/' + 'RoundAndScoreDistribution_no_outliers_' + tag_for_dir_and_filenames\n",
        "plot_score_round_distributions(outputfilename, traces_cardcount)"
      ],
      "id": "DSWZqXOBksgl"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6axA42kyBMfb",
        "outputId": "11523cec-1ab1-4277-de3f-bbec12d1b701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "#flatten card count data so that we have a playtrace per row\n",
        "traces_cardcount = flatten_card_count_data(traces_cardcount, card_types)\n",
        "print(len(traces_cardcount))\n",
        "print(len(traces_ngrams))"
      ],
      "id": "6axA42kyBMfb"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTi_VsyRI6Bl",
        "outputId": "cf24e376-398e-4418-dfba-80851d98e35b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of Ngrams for N=1:88\n",
            "Total number of Ngrams for N=2:1225\n",
            "Total number of Ngrams for N=3:5884\n"
          ]
        }
      ],
      "source": [
        "#Compute the all ngrams list by taking the total list of all observed n-grams for this tournament\n",
        "all_ngrams_list = {}\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  col_name = 'NGrams_' + str(n)\n",
        "  all_ngrams_list[n] = []\n",
        "  for row in traces_ngrams[col_name]:\n",
        "    for gram in row:\n",
        "      if gram not in all_ngrams_list[n]:\n",
        "        all_ngrams_list[n].append(gram)\n",
        "  print(\"Total number of Ngrams for N=\" + str(n) + \":\" + str(len(all_ngrams_list[n])))"
      ],
      "id": "oTi_VsyRI6Bl"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0b646b11"
      },
      "outputs": [],
      "source": [
        "#functions to support NGram analysis\n",
        "\n",
        "#function to compute N-gram probabilities, returns either an array with probability values\n",
        "#in the same order as ngrams in ngrams_all, or a dictionary with the n-grams as key\n",
        "#Unobserved ngrams (i.e. ngrams in ngrams_all, that are not in the trace) are assigned\n",
        "#a default probability of zero.\n",
        "def calc_probabilities(ngrams_trace, ngrams_all, convertToArray = False):\n",
        "    # Compute the frequency of ngrams in the trace\n",
        "    frequency_counter = Counter(ngrams_trace)\n",
        "\n",
        "    #calculate frequencies of all ngrams in ngrams_all that appear in the playtrace\n",
        "    event_count = {gram: frequency_counter.get(gram, 0) for gram in ngrams_all}\n",
        "\n",
        "    #normalise each entry with the number of n-grams observed for that trace, to convert\n",
        "    #counts into probabilities\n",
        "    trace_n_gram_count = sum(frequency_counter.values())\n",
        "    probs = {key: value / (1.0*trace_n_gram_count) for key, value in event_count.items()}\n",
        "\n",
        "    if convertToArray:\n",
        "        probs = np.array(list(probs.values()))\n",
        "\n",
        "    return probs\n",
        "\n",
        "#function to take a probability dictionary and create an array\n",
        "def prob_dict_to_array(prob_dict):\n",
        "    return np.array(list(prob_dict.values()))\n",
        "\n",
        "#funciton to take probability array and convert to dictionary with n-grams as keys\n",
        "#assumes ordering has been maintained\n",
        "def prob_array_to_dict(prob_array, ngrams_all):\n",
        "    prob_dict = {}\n",
        "    index = 0\n",
        "    for gram in ngrams_all:\n",
        "        prob_dict[gram] = prob_array[index]\n",
        "        index+=1\n",
        "    return prob_dict\n",
        "\n",
        "#find the common set of ngrams between two probability dictionaries, with probabilities\n",
        "#above a given threshold\n",
        "def return_common_ngrams_above_threshold(prob_dict1, prob_dict2, threshold):\n",
        "    common_ngrams = []\n",
        "    #look for entries in the first dictionary with non-zero values\n",
        "    for key, value in prob_dict1.items():\n",
        "        if value > threshold:\n",
        "            common_ngrams.append(key)\n",
        "    #repeat for the second dictionary but avoiding duplicates\n",
        "    for key, value in prob_dict2.items():\n",
        "        if (value > threshold) and (key not in common_ngrams):\n",
        "             common_ngrams.append(key)\n",
        "    return common_ngrams\n",
        "\n",
        "#convert a list of ngram tuples into a list of strings\n",
        "def convert_ngram_tuples_to_strings(ngrams_list):\n",
        "    ngrams_str = []\n",
        "    for tuple_item in ngrams_list:\n",
        "        tuple_str = ''\n",
        "        for index, element in enumerate(tuple_item):\n",
        "            if index != (len(tuple_item)-1):\n",
        "                tuple_str += element + '|'\n",
        "            else:\n",
        "                tuple_str += element\n",
        "        ngrams_str.append(tuple_str)\n",
        "    return ngrams_str"
      ],
      "id": "0b646b11"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "aac5d703"
      },
      "outputs": [],
      "source": [
        "#add columns to trace data containing arrays for probability data\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  col_name_1 = 'ProbDict_' + str(n)\n",
        "  col_name_2 = 'ProbArray_' + str(n)\n",
        "  col_name_3 = 'NGrams_' + str(n)\n",
        "  traces_ngrams[col_name_1] = traces_ngrams.apply(lambda row: calc_probabilities(row[col_name_3], all_ngrams_list[n], False), axis = 1)\n",
        "  traces_ngrams[col_name_2] = traces_ngrams.apply(lambda row: calc_probabilities(row[col_name_3], all_ngrams_list[n], True), axis = 1)"
      ],
      "id": "aac5d703"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba9b0aaa",
        "outputId": "31fa96e1-0b58-48c2-e292-02d7419c92ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "0.9999999999999988\n"
          ]
        }
      ],
      "source": [
        "#a few quick sense checks\n",
        "example_dict = traces_ngrams['ProbDict_2'].iloc[0]\n",
        "example_array = traces_ngrams['ProbArray_2'].iloc[0]\n",
        "\n",
        "#check translation functions work\n",
        "example_dict_converted_to_array = prob_dict_to_array(example_dict)\n",
        "example_array_converted_to_dict = prob_array_to_dict(example_array, all_ngrams_list[2])\n",
        "\n",
        "print(np.array_equal(example_dict_converted_to_array, example_array))\n",
        "print(example_array_converted_to_dict == example_dict)\n",
        "\n",
        "#check probability array is normalised\n",
        "print(sum(example_array))"
      ],
      "id": "ba9b0aaa"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "R_q5YHlOfdQy"
      },
      "outputs": [],
      "source": [
        "#remove columns that are unnecessary for clustering algorithms and distance measure calculations\n",
        "cols = ['Player', 'GameID', 'AgentName', 'AgentNameOpponent', 'Win', 'FinalScore', 'TotalRounds']\n",
        "traces_cardcount_slim = traces_cardcount.drop(cols, axis = 1)"
      ],
      "id": "R_q5YHlOfdQy"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "baf6b8d0"
      },
      "outputs": [],
      "source": [
        "#calculate distance and affinity matrices for jenen-shannon and l_k norm distance functions\n",
        "def symm_distance_matrix(df, distance_func, func_param = False):\n",
        "    traces = df.tolist()\n",
        "    index_combinations = list(combinations(range(len(traces)), 2))\n",
        "\n",
        "    #we branch here so we can use both lk-norm and Jensen-Shannon in this function\n",
        "    if not func_param:\n",
        "      distance_values = [distance_func(traces[i],traces[j]) for i, j in index_combinations]\n",
        "    else:\n",
        "      distance_values = [distance_func(func_param, traces[i],traces[j]) for i, j in index_combinations]\n",
        "\n",
        "    num_rows = len(df)\n",
        "    distance_matrix = pd.DataFrame(index=range(num_rows), columns=range(num_rows))\n",
        "\n",
        "    for (i, j), distance_value in zip(index_combinations, distance_values):\n",
        "        distance_matrix.at[i, j] = distance_value\n",
        "        distance_matrix.at[j, i] = distance_value  # mirror the value\n",
        "\n",
        "    return distance_matrix.fillna(0)  # fill NaN values with zeros for diagonal elements\n",
        "\n",
        "#affinity function used when computing fully connected affinity matrix\n",
        "def connected_affinity_func(x, gamma):\n",
        "  return np.exp(-gamma * (x**2))\n",
        "\n",
        "#calculate fully connected affinity matrix\n",
        "def connected_affinity_matrix(dist_matrix, gamma):\n",
        "  eps = 0.00000001\n",
        "  matrix = np.vectorize(connected_affinity_func)(dist_matrix, gamma)\n",
        "  #note we put a lower bound on the entries in the infinity matrix to make sure\n",
        "  #we have a fully connected graph\n",
        "  return np.vectorize(max)(matrix, eps)\n",
        "\n",
        "#calculate k-nearest neighbour affinity matrix\n",
        "def knn_affinity_matrix(dist_matrix, k):\n",
        "  # Find indices of k-nearest neighbors for each data point\n",
        "  neigh = NearestNeighbors(n_neighbors=k + 1, metric='precomputed').fit(dist_matrix)\n",
        "  _, indices = neigh.kneighbors()\n",
        "\n",
        "  # Create knn affinity matrix\n",
        "  aff_matrix = np.zeros_like(dist_matrix, dtype=float)\n",
        "  for i in range(dist_matrix.shape[0]):\n",
        "      aff_matrix[i, indices[i, 1:]] = 1.0  # Assign 1 to the k-nearest neighbors\n",
        "\n",
        "  # Make the matrix symmetric\n",
        "  aff_matrix = 0.5 * (aff_matrix + aff_matrix.T)\n",
        "\n",
        "  return aff_matrix\n",
        "\n",
        "#function to calculate Jensen-Shannon distance\n",
        "def kl_divergence(p, q):\n",
        "  eps = 1e-10\n",
        "  return np.sum(np.where(p < eps, 0, np.where(q < eps, 0, p * np.log((p + eps) / (1.0 * q + eps)))))\n",
        "\n",
        "def jensen_shannon_distance(p, q):\n",
        "  m = 0.5 * (p + q)\n",
        "  return 0.5 * (kl_divergence(p, m) + kl_divergence(q, m))\n",
        "\n",
        "#function to compute l_k norm, here p and q are arrays of doubles\n",
        "def l_k_norm(k, p, q):\n",
        "  return np.sum(np.abs(p - q) ** k) ** (1/(1.0*k))\n",
        "\n",
        "#calculate distance, fully connected and k-nearest neighbour affinity matrices for l_k norm for card count playtraces\n",
        "dist_matrices = {}\n",
        "connected_affinity_matrices = {}\n",
        "knn_affinity_matrices = {}\n",
        "traces_cardcount_arrays = traces_cardcount_slim.apply(lambda row: np.array(row), axis=1)\n",
        "#pdb.set_trace()\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  connected_affinity_matrices[key_norm] = {}\n",
        "  knn_affinity_matrices[key_norm] = {}\n",
        "  dist_matrices[key_norm] = symm_distance_matrix(traces_cardcount_arrays, l_k_norm, k)\n",
        "  for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "    key_gamma =  'gamma_' + str(gamma)\n",
        "    connected_affinity_matrices[key_norm][key_gamma] = connected_affinity_matrix(dist_matrices[key_norm], gamma)\n",
        "  for nn in range(nearest_neighbours_min, nearest_neighbours_max, nearest_neighbours_stepsize):\n",
        "    key_nn =  'knn_' + str(nn)\n",
        "    knn_affinity_matrices[key_norm][key_nn] = knn_affinity_matrix(dist_matrices[key_norm], nn)\n",
        "\n",
        "#for dbcan we also need to normalise the playtraces to reduce the space we need to search over\n",
        "#epsilon to between zero and one. Note that the Jensen-Shannon distance measure is by defintion less than one\n",
        "#so we only need to do this for our card count playtraces\n",
        "traces_cardcount_slim_normalised = {}\n",
        "dist_matrices_normalised = {}\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  traces_cardcount_slim_normalised[key_norm] = traces_cardcount_slim.apply(lambda row: np.array(row)/l_k_norm(k, np.array(row), np.zeros(len(np.array(row)))), axis = 1)\n",
        "  dist_matrices_normalised[key_norm] = symm_distance_matrix(traces_cardcount_slim_normalised[key_norm], l_k_norm, k)\n",
        "\n",
        "#calculate distance, fully connected and k-nearest neighbour affinity matrices for N-Gram playtraces\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  key_gram = 'N_Gram_' + str(n)\n",
        "  connected_affinity_matrices[key_gram] = {}\n",
        "  knn_affinity_matrices[key_gram] = {}\n",
        "  col_name = 'ProbArray_' + str(n)\n",
        "  dist_matrices[key_gram] = symm_distance_matrix(traces_ngrams[col_name], jensen_shannon_distance)\n",
        "  for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "    key_gamma =  'gamma_' + str(gamma)\n",
        "    connected_affinity_matrices[key_gram][key_gamma] = connected_affinity_matrix(dist_matrices[key_gram], gamma)\n",
        "  for nn in range(nearest_neighbours_min, nearest_neighbours_max, nearest_neighbours_stepsize):\n",
        "    key_nn =  'knn_' + str(nn)\n",
        "    knn_affinity_matrices[key_gram][key_nn] = knn_affinity_matrix(dist_matrices[key_gram], nn)"
      ],
      "id": "baf6b8d0"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Z_Jtc-amEd5Q"
      },
      "outputs": [],
      "source": [
        "#functions to perfom clustering analysis\n",
        "def sa_kmedoids(dist_matrix, num_clusters):\n",
        "  clusterer = KMedoids(n_clusters=num_clusters,\n",
        "                       metric='precomputed',\n",
        "                       method='pam',\n",
        "                       init='k-medoids++',\n",
        "                       max_iter=300,\n",
        "                       random_state= 0).fit(dist_matrix)\n",
        "  #when we use precomputed as the metric, the algorithm returns the indices for the cluster centres only\n",
        "  return clusterer.inertia_, clusterer.medoid_indices_, clusterer.labels_\n",
        "\n",
        "def sa_kmeans(data, num_clusters):\n",
        "  clusterer = KMeans(n_clusters=num_clusters,\n",
        "                      init='k-means++',\n",
        "                      n_init= 'warn',\n",
        "                      max_iter=300,\n",
        "                      tol=0.0001,\n",
        "                      verbose=0,\n",
        "                      random_state=0,\n",
        "                      copy_x=True,\n",
        "                      algorithm='lloyd').fit(data)\n",
        "  return clusterer.inertia_, clusterer.cluster_centers_, clusterer.labels_\n",
        "\n",
        "def sa_dbscan(dist_matrix, minPts, epsilon):\n",
        "  dbscan_clustering = DBSCAN(eps= epsilon, min_samples= minPts, metric = 'precomputed').fit(dist_matrix)\n",
        "  return dbscan_clustering.labels_\n",
        "\n",
        "#spectral clcustering using pre-computed affinity matrx\n",
        "def sa_spectral_clustering_AM(affinity_matrix, num_clusters):\n",
        "  spec_clustering_AM = SpectralClustering(n_clusters= num_clusters,\n",
        "                                        random_state=0,\n",
        "                                        affinity = 'precomputed',\n",
        "                                        assign_labels='kmeans').fit(affinity_matrix)\n",
        "  return spec_clustering_AM.labels_"
      ],
      "id": "Z_Jtc-amEd5Q"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "7u9WU-Qiea27"
      },
      "outputs": [],
      "source": [
        "#functions to generate plots and output files\n",
        "def output_silhouette_plot(outputfilename, silhouette_samples, silhouette_avg, cluster_labels):\n",
        "  n_clusters = len(np.unique(cluster_labels))\n",
        "\n",
        "  #Create a subplot with 1 row and 1 columns\n",
        "  fig, ax1 = plt.subplots(1,1, clear = True)\n",
        "  fig.set_size_inches(7, 3.5)\n",
        "\n",
        "  # The silhouette coefficient can range from -1, 1\n",
        "  ax1.set_xlim([-0.1, 1])\n",
        "  # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "  # plots of individual clusters, to demarcate them clearly.\n",
        "  ax1.set_ylim([0, len(silhouette_samples) + (n_clusters + 1) * 10])\n",
        "\n",
        "  y_lower = 10\n",
        "  for i in range(n_clusters):\n",
        "      # Aggregate the silhouette scores for samples belonging to\n",
        "      # cluster i, and sort them\n",
        "      ith_cluster_silhouette_values = silhouette_samples[cluster_labels == i]\n",
        "\n",
        "      ith_cluster_silhouette_values.sort()\n",
        "\n",
        "      size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "      y_upper = y_lower + size_cluster_i\n",
        "\n",
        "      color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "      ax1.fill_betweenx(\n",
        "          np.arange(y_lower, y_upper),\n",
        "          0,\n",
        "          ith_cluster_silhouette_values,\n",
        "          facecolor=color,\n",
        "          edgecolor=color,\n",
        "          alpha=0.7,\n",
        "      )\n",
        "\n",
        "      # Label the silhouette plots with their cluster numbers at the middle\n",
        "      ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "      # Compute the new y_lower for next plot\n",
        "      y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "  #ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "  ax1.set_xlabel(\"The silhouette coefficient values for K=\" + str(n_clusters))\n",
        "  ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "  # The vertical line for average silhouette score of all the values\n",
        "  ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "  ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "  ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "  #plt.suptitle(\n",
        "  #    \"Silhouette analysis for \" + cluster_method_str + \" clustering\",\n",
        "  #    fontsize=14,\n",
        "  #    fontweight=\"bold\",\n",
        "  #)\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "#output list of dictionary values\n",
        "def output_dictionary(outputfilename, dict):\n",
        "  with open(outputfilename + '.csv', 'w', newline='') as csv_file:\n",
        "    writer = csv.DictWriter(csv_file, fieldnames= dict.keys())\n",
        "    # Write the header\n",
        "    writer.writeheader()\n",
        "    # Write the data\n",
        "    writer.writerow(dict)\n",
        "\n",
        "#output string, value pairs to file\n",
        "def output_values(outputfilename, values_list, valueslabels_list):\n",
        "  with open(outputfilename + '.csv', 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for index in range(0, len(values_list)):\n",
        "      writer.writerow([valueslabels_list[index], values_list[index]])\n",
        "\n",
        "def output_inertia_plot(outputfilename, inertia_vals_dict, scalar):\n",
        "  #scale the inertia vals, typically the scalar value will be the inertia for clustering on one cluster\n",
        "  inertia_vals_array = np.array([value for value in inertia_vals_dict.values()])\n",
        "  inertia_vals_scaled = inertia_vals_array/scalar\n",
        "  cluster_list = np.array([cluster for cluster in inertia_vals_dict.keys()])\n",
        "  inertia_vals_scaled = np.insert(inertia_vals_scaled, 0, 1.0)\n",
        "  cluster_list = np.insert(cluster_list, 0, 1)\n",
        "\n",
        "  #plot as a line plot\n",
        "  fig, ax = plt.subplots(num=1,clear=True)\n",
        "  ax.plot(cluster_list, inertia_vals_scaled)\n",
        "  ax.set_xticks(cluster_list)\n",
        "  ax.set_xlabel(\"Number of clusters\")\n",
        "  ax.set_ylabel(\"Scaled inertia\")\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "#function to plot card count playtraces\n",
        "def cardcount_playtrace_comparison(outputfilename, trace_list, label_list, card_types, legendOn = True, ylimit = 0, output_to_screen = False):\n",
        "  #look at evolution of number of cards of each type per round.\n",
        "  #traces should all be of the same length\n",
        "  maxRounds =   int(trace_list[0].shape[1]/17)\n",
        "  noOfCardTypes = len(card_types)\n",
        "  noOfSubPlotCols = 5\n",
        "  noOfSubPlotRows = max(2, math.floor(noOfCardTypes/noOfSubPlotCols) + 1)\n",
        "  fig, axs = plt.subplots(noOfSubPlotRows, noOfSubPlotCols, figsize = (10,10))\n",
        "  for i in range(0,noOfSubPlotRows):\n",
        "      for j in range(0,noOfSubPlotCols):\n",
        "          cardIndex = noOfSubPlotRows*j + i\n",
        "          if cardIndex >= len(card_types):\n",
        "              axs[i,j].set_visible(False)\n",
        "          else:\n",
        "              card_type = card_types[cardIndex]\n",
        "              card_col = [card_type + \"_R\" + str(r) for r in range(0,maxRounds)]\n",
        "              card_max = 0\n",
        "              for (index, trace) in enumerate(trace_list):\n",
        "                  axs[i,j].plot(range(0,maxRounds), trace[card_col].iloc[0], label = label_list[index])\n",
        "                  tmp_card_max = int(trace[card_col].iloc[0].max())\n",
        "                  if tmp_card_max > card_max:\n",
        "                      card_max = tmp_card_max\n",
        "\n",
        "              #set labels and limits\n",
        "              axs[i,j].set_title(card_type)\n",
        "              axs[i,j].set_xlabel('Round')\n",
        "              if ylimit == 0:\n",
        "                  axs[i,j].set_ylim((0,card_max+2))\n",
        "              else:\n",
        "                  axs[i,j].set_ylim((0,1))\n",
        "              #axs[i,j].set_ylim((0,card_max))\n",
        "              axs[i,j].set_xticks(ticks = range(0, maxRounds,10))\n",
        "\n",
        "          #tighten subplots layout\n",
        "          fig.tight_layout()\n",
        "\n",
        "  #add overal legend to figure\n",
        "  if legendOn:\n",
        "      axs[0,noOfSubPlotRows - 1].legend(loc = (1.2,-0.8))\n",
        "\n",
        "  if output_to_screen:\n",
        "    plt.show()\n",
        "  else:\n",
        "    #output to file\n",
        "    plt.savefig(outputfilename +'.png', format = 'png')\n",
        "    plt.close()\n",
        "\n",
        "#plot cluster centroids based on cardcount playtraces, that are outputted directly from sklearn clustering algorithms\n",
        "def plot_cluster_centroids(outputfilename, cluster_centers, card_types, legendOn = True, ylimit = 0, output_to_screen = False):\n",
        "  #cluster centres outputted from sklearn are 2D arrays with rows corresponding to clusters and columns corresponding to length of trace\n",
        "  no_of_clusters = cluster_centers.shape[0]\n",
        "  maxRounds = int(cluster_centres.shape[1]/17)\n",
        "  df_cluster_centres = pd.DataFrame(cluster_centers)\n",
        "  cols = [card_types[i] + \"_R\" + str(r) for r in range(0, maxRounds)\n",
        "          for i in range(0, len(card_types))]\n",
        "  df_cluster_centres.columns = cols\n",
        "\n",
        "  trace_list = []\n",
        "  label_list = []\n",
        "  for n in range(0, no_of_clusters):\n",
        "      trace_list.append(pd.DataFrame(df_cluster_centres.iloc[n]).transpose())\n",
        "      label_list.append(str('Cluster ') + str(n) + str(' Centroid'))\n",
        "  cardcount_playtrace_comparison(outputfilename, trace_list, label_list, card_types, legendOn, ylimit, output_to_screen)\n",
        "\n",
        "#output a selection of cluster metrics. This includes:\n",
        "#1. Portion of traces in each cluster that have a given agent name\n",
        "#2. Portion of traces in a cluster that were from the player that moved first\n",
        "#3. Portfolio of traces in a cluster that won\n",
        "def ouput_cluster_metrics(outputfilename, traces, cluster_labels):\n",
        "  tmp_traces = traces.copy()\n",
        "  tmp_traces['Cluster'] = cluster_labels\n",
        "\n",
        "  #loop over attibutes to compute distributions\n",
        "  df_result = pd.DataFrame()\n",
        "  for att in ['AgentName', 'Player', 'Win']:\n",
        "    result = tmp_traces.groupby(['Cluster', att]).size().unstack().fillna(0)\n",
        "    results_percentage = result.div(result.sum(axis=1), axis=0) * 100\n",
        "    if att == 'AgentName':\n",
        "      df_result = results_percentage\n",
        "    else:\n",
        "      if att == 'Player':\n",
        "        results_percentage.columns = ['First Player', 'Second Player']\n",
        "      else:\n",
        "        if len(results_percentage.columns) == 2:\n",
        "          results_percentage.columns = ['Win', 'Loss']\n",
        "        else:\n",
        "          results_percentage.columns = ['Win', 'Draw', 'Loss']\n",
        "      df_result = df_result.join(results_percentage)\n",
        "\n",
        "  #output to file\n",
        "  df_result.to_csv(outputfilename + '.csv')\n",
        "\n",
        "#convert a list of ngram tuples into a list of strings, used in plotting function below\n",
        "def convert_ngram_tuples_to_strings(ngrams_list):\n",
        "  ngrams_str = []\n",
        "  for tuple_item in ngrams_list:\n",
        "      tuple_str = ''\n",
        "      for index, element in enumerate(tuple_item):\n",
        "          if index != (len(tuple_item)-1):\n",
        "              tuple_str += element + '|'\n",
        "          else:\n",
        "              tuple_str += element\n",
        "      ngrams_str.append(tuple_str)\n",
        "  return ngrams_str\n",
        "\n",
        "#function to plot N-Gram distributions side by side\n",
        "def plot_distribution_comparison(outputfilename, prob_dicts, labels, threshold = 0):\n",
        "  #find a common domain where all probability values are greater than a given threshold\n",
        "  common_ngrams = []\n",
        "  for prob_dict in prob_dicts:\n",
        "    for key, value in prob_dict.items():\n",
        "      if (value > threshold) and (key not in common_ngrams):\n",
        "          common_ngrams.append(key)\n",
        "\n",
        "  #extract probability arrays for these common n-grams\n",
        "  prob_arrays = []\n",
        "  for prob_dict in prob_dicts:\n",
        "    prob_dict_reduced = {key: prob_dict[key] for key in common_ngrams}\n",
        "    prob_arrays.append(prob_dict_to_array(prob_dict_reduced))\n",
        "\n",
        "  #next plot probability distributions\n",
        "\n",
        "  #need to convert common_ngrams into a list of strings as opposed to tuples containing strings\n",
        "  common_ngrams_str = convert_ngram_tuples_to_strings(common_ngrams)\n",
        "\n",
        "  #plot discrete probability distributions side by side\n",
        "\n",
        "  # Set the width of the bars\n",
        "  bar_width = 0.35\n",
        "\n",
        "  #set spacing between bars representing different n-grams\n",
        "  spacing = 0.1\n",
        "\n",
        "  no_distributions = len(prob_arrays)\n",
        "  no_n_grams = len(common_ngrams)\n",
        "  counter = 0\n",
        "  x_values = {}\n",
        "  for prob_array in prob_arrays:\n",
        "    x_values[counter] = np.zeros(no_n_grams)\n",
        "    if counter == 0:\n",
        "      x_values[counter][0] = spacing + bar_width/2.0\n",
        "      for i in range(1, len(prob_array)):\n",
        "        x_values[counter][i] = x_values[counter][0] + (bar_width * (no_distributions -1) + spacing + bar_width) * i\n",
        "    else:\n",
        "      for i in range(0, len(prob_array)):\n",
        "        x_values[counter][i] = x_values[counter-1][i] + bar_width\n",
        "\n",
        "    plt.bar(x_values[counter], prob_array, width=bar_width, label = labels[counter])\n",
        "    counter += 1\n",
        "\n",
        "  #for ticks place them in the middle of each set of distributions for single n-gram\n",
        "  x_ticks = np.zeros(no_n_grams)\n",
        "  for i in range(0, no_n_grams):\n",
        "    avg_coord = 0\n",
        "    for no_dists in range(0, no_distributions):\n",
        "      avg_coord += x_values[no_dists][i]\n",
        "    x_ticks[i] = avg_coord/no_distributions\n",
        "\n",
        "  plt.xticks(x_ticks, common_ngrams_str)\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.ylim(threshold)\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "\n",
        "  #output to file\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "#find the highest density data points in a data set - i.e. the point with the largest number of neighbours within a distance eps,\n",
        "#this is done by cluster. Note that the choice of eps that helps with clustering is not a good choice of eps\n",
        "#to find a high denisty point.\n",
        "def find_highest_density_datapts(dist_matrix, cluster_labels):\n",
        "  indices_by_cluster = {}\n",
        "  for cluster in np.unique(cluster_labels):\n",
        "    if cluster == -1:\n",
        "      #ignore noise\n",
        "      continue\n",
        "    else:\n",
        "      #now we can remove and index items based on this index\n",
        "      indices_to_remove = [i for i, value in enumerate(cluster_labels) if value != cluster]\n",
        "      cluster_dist_matrix = dist_matrix.drop(index=indices_to_remove, columns=indices_to_remove)\n",
        "\n",
        "      #in order to find the highest density points we set espilon to be twice the smallest distance\n",
        "      pos_distances = np.array(cluster_dist_matrix)\n",
        "      pos_distances = pos_distances[pos_distances != 0]\n",
        "      lower_dist = np.min(pos_distances)\n",
        "      eps = np.min(pos_distances) * 2 #this is an arbitrary choice\n",
        "\n",
        "      #create a NearestNeighbors model with the precomputed distance matrix\n",
        "      nn_model = NearestNeighbors(radius=eps, metric='precomputed')\n",
        "      nn_model.fit(cluster_dist_matrix)\n",
        "\n",
        "      # Find neighbors within epsilon for each data point\n",
        "      neighbors = nn_model.radius_neighbors(return_distance=False)\n",
        "\n",
        "      # Calculate the number of neighbors for each data point\n",
        "      num_neighbors = [len(neighbors_i) for neighbors_i in neighbors]\n",
        "\n",
        "      # Find the indices of the data point with the most neighbors\n",
        "      #note we need to take the argmax and compare to the revised index for our data set\n",
        "      max_neighbors_indices = np.argmax(num_neighbors)\n",
        "      revised_index = dist_matrix.index.drop(indices_to_remove)\n",
        "      final_max_neighbor_index = revised_index[max_neighbors_indices]\n",
        "\n",
        "      #note these indices are still wrt to original full dist matrix and trace list\n",
        "      indices_by_cluster[cluster] = final_max_neighbor_index\n",
        "\n",
        "  return indices_by_cluster\n",
        "\n",
        "#compute averages over playtraces for each cluster and determines the point in each cluster\n",
        "#closest to the mean. This is done in this function using the l_k_norm\n",
        "def closest_to_mean_l_k_norm(traces, cluster_labels, k):\n",
        "  indices_for_cluster_center = {}\n",
        "  for cluster in np.unique(cluster_labels):\n",
        "    if cluster == -1:\n",
        "      #skip noise in DBSCAN case\n",
        "      continue\n",
        "    else:\n",
        "      # Select data points belonging to the current cluster\n",
        "      cluster_data = traces[cluster_labels == cluster]\n",
        "\n",
        "      # Compute mean vector for the cluster\n",
        "      cluster_mean = np.mean(cluster_data, axis=0)\n",
        "\n",
        "      # Find the index of the point closest to the mean in the cluster\n",
        "      cluster_data['dist_to_mean'] = cluster_data.apply(lambda row: l_k_norm(k, np.array(row), np.array(cluster_mean)), axis = 1)\n",
        "      indices_for_cluster_center[cluster] = cluster_data['dist_to_mean'].idxmin()\n",
        "  return indices_for_cluster_center\n",
        "\n",
        "#same as above except for Jensen-Shannon distance\n",
        "def closest_to_mean_js_norm(traces, cluster_labels):\n",
        "  indices_for_cluster_center = {}\n",
        "  for cluster in np.unique(cluster_labels):\n",
        "    if cluster == -1:\n",
        "      #skip noise in DBSCAN case\n",
        "      continue\n",
        "    else:\n",
        "      # Select data points belonging to the current cluster\n",
        "      cluster_data = traces[cluster_labels == cluster]\n",
        "\n",
        "      # Compute mean vector for the cluster\n",
        "      cluster_mean = np.mean(cluster_data, axis=0) # is this still normalised? Should be....and also doesnt really matter\n",
        "\n",
        "      # Find the index of the point closest to the mean in the cluster\n",
        "      cluster_data['dist_to_mean'] = cluster_data.apply(lambda row: jensen_shannon_distance(np.array(row), np.array(cluster_mean)))\n",
        "      indices_for_cluster_center[cluster] = cluster_data['dist_to_mean'].idxmin()\n",
        "  return indices_for_cluster_center\n",
        "\n",
        "def mds_2d_cluster_plot(outputfilename, dist_matrix, cluster_labels, pts_closest_to_mean = {}, medoid_indices = {}, high_density_indices = {}, output_to_screen = False):\n",
        "  # Metric Multi-dimensional Scaling (MDS)\n",
        "  mds = MDS(n_components=2, metric = True, dissimilarity='precomputed', random_state= 0 )\n",
        "  embeddings = mds.fit_transform(dist_matrix)\n",
        "  #pdb.set_trace()\n",
        "\n",
        "  # Create a scatter plot\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Define colors for each cluster label and high-density point\n",
        "  cluster_colors = {-1: 'black', 0: 'red', 1: 'blue', 2: 'green', 3: 'purple', 4: 'grey', 5: 'brown', 6: 'orange', 7: 'cyan', 8: 'magenta'}  # Add more colors if needed\n",
        "  density_color = 'yellow'\n",
        "  means_color = 'teal'\n",
        "  medoids_color = 'teal'\n",
        "\n",
        "  #Q: We are using cluster labels and indices derived from original N-dimensional data set and applying to embeddings. Is this OK?\n",
        "\n",
        "  # Plot points with cluster labels\n",
        "  for cluster_label in np.unique(cluster_labels):\n",
        "    indices = cluster_labels == cluster_label\n",
        "    if cluster_label == -1:\n",
        "      marker_style = '+'\n",
        "      label_txt = 'Noise'\n",
        "    else:\n",
        "      marker_style = 'o'\n",
        "      label_txt = f'Cluster {cluster_label}'\n",
        "    ax.scatter(embeddings[indices, 0], embeddings[indices, 1], c=cluster_colors[cluster_label], label= label_txt, marker= marker_style, s=50)\n",
        "\n",
        "  # Plot points with high density labels\n",
        "  if len(high_density_indices) > 0:\n",
        "    density_indices = list(high_density_indices.values())\n",
        "    ax.scatter(embeddings[density_indices, 0], embeddings[density_indices, 1], c=density_color, label='High Density Points', marker='x', s=50, edgecolors='black')\n",
        "\n",
        "  #plot points closest to mean\n",
        "  if len(pts_closest_to_mean) > 0:\n",
        "    mean_indices = list(pts_closest_to_mean.values())\n",
        "    ax.scatter(embeddings[mean_indices, 0], embeddings[mean_indices, 1], c=means_color, label='Cluster Means', marker='v', s=50, edgecolors='black')\n",
        "\n",
        "  #medoids\n",
        "  if len(medoid_indices) > 0:\n",
        "    medoid_pts = list(medoid_indices.values())\n",
        "    ax.scatter(embeddings[medoid_pts, 0], embeddings[medoid_pts, 1], c=medoids_color, label='Cluster Medoids', marker='v', s=50, edgecolors='black')\n",
        "\n",
        "  # Set labels and title\n",
        "  ax.set_xlabel('MDS Dimension 1')\n",
        "  ax.set_ylabel('MDS Dimension 2')\n",
        "  ax.set_title('MDS Plot with Cluster Labels')\n",
        "  ax.legend()\n",
        "\n",
        "  #output to file\n",
        "  if output_to_screen:\n",
        "    plt.show()\n",
        "  else:\n",
        "    plt.savefig(outputfilename +'.png', format = 'png')\n",
        "    plt.close()\n",
        "\n",
        "  #calculate normalised stress\n",
        "  normalisation_factor = np.sum(np.sum(np.square(dist_matrix)))\n",
        "  normalised_stress = np.sqrt(mds.stress_/normalisation_factor)\n",
        "\n",
        "  #also return the stress value from the MDS algorithm for reference\n",
        "  return normalised_stress"
      ],
      "id": "7u9WU-Qiea27"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmLapEm9JbMU",
        "outputId": "1b33eece-6ca9-494f-d4d9-69fc8dbb7c2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x79eca3165b10>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "#We switch off interactive mode for matplotlib as plots will be output to file\n",
        "plt.ioff()"
      ],
      "id": "DmLapEm9JbMU"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya_QVuU6LZ0O",
        "outputId": "8a74d481-2581-4539-f08d-de4d843e0100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming K-means clustering for card count playtraces using euclidean norm...\n"
          ]
        }
      ],
      "source": [
        "#perfom K-means clustering. We only do this for card-count playtraces due to restriction on using Euclidean playtraces\n",
        "print(\"Perfoming K-means clustering for card count playtraces using euclidean norm...\")\n",
        "sil_avg = {}\n",
        "inertia = {}\n",
        "stress = {}\n",
        "key_norm = 'CardCount_lknorm_2'\n",
        "for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "  inertia[n_clusters], cluster_centres, cluster_labels = sa_kmeans(traces_cardcount_slim, n_clusters)\n",
        "  sil_avg[n_clusters] = silhouette_score(traces_cardcount_slim, cluster_labels)\n",
        "  sil_coeffs = silhouette_samples(traces_cardcount_slim, cluster_labels)\n",
        "\n",
        "  #output silhouette plots\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "  output_silhouette_plot(outputfilename, sil_coeffs, sil_avg[n_clusters], cluster_labels)\n",
        "\n",
        "  #output cluster centroids\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "  plot_cluster_centroids(outputfilename, cluster_centres, card_types)\n",
        "\n",
        "  #output cluster metrics\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'cluster_metrics_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "  ouput_cluster_metrics(outputfilename, traces_cardcount, cluster_labels)\n",
        "\n",
        "  #output MDS plots\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'MDS_plot_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "  closest_to_mean_indices = closest_to_mean_l_k_norm(traces_cardcount_slim, cluster_labels, 2)\n",
        "  stress[n_clusters] = mds_2d_cluster_plot(outputfilename, dist_matrices[key_norm], cluster_labels, closest_to_mean_indices)\n",
        "\n",
        "#output silhouette averages to file\n",
        "outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'silhouette_averages_KMeans_CardCount_' + tag_for_dir_and_filenames\n",
        "output_dictionary(outputfilename, sil_avg)\n",
        "\n",
        "#output stress values to file\n",
        "outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'stress_values_from_MDS_KMeans_CardCount_' + tag_for_dir_and_filenames\n",
        "output_dictionary(outputfilename, stress)\n",
        "\n",
        "#output a scaled inertia value plot\n",
        "outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'inertia_plot_KMeans_CardCount_' + tag_for_dir_and_filenames\n",
        "scalar, _, _ = sa_kmeans(traces_cardcount_slim, 1)\n",
        "output_inertia_plot(outputfilename, inertia, scalar)"
      ],
      "id": "Ya_QVuU6LZ0O"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "KLvqSxnkLQ1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a5ea06-c2b8-4a3c-f79e-198554ae3843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming K-medoids clustering for card count playtraces using l_k-norm...\n"
          ]
        }
      ],
      "source": [
        "#perform K-medoids clustering for card count playtraces.\n",
        "print(\"Perfoming K-medoids clustering for card count playtraces using l_k-norm...\")\n",
        "\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  sil_avg = {}\n",
        "  inertia = {}\n",
        "  stress = {}\n",
        "  for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "    inertia[n_clusters], cluster_indices, cluster_labels = sa_kmedoids(dist_matrices[key_norm], n_clusters)\n",
        "    sil_avg[n_clusters] = silhouette_score(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "    sil_coeffs = silhouette_samples(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, sil_coeffs, sil_avg[n_clusters], cluster_labels)\n",
        "\n",
        "    #output cluster centroids, converting indices to playtraces\n",
        "    cluster_centres = traces_cardcount_slim.iloc[cluster_indices]\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    plot_cluster_centroids(outputfilename, cluster_centres, card_types)\n",
        "\n",
        "    #output cluster metrics\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'cluster_metrics_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    ouput_cluster_metrics(outputfilename, traces_cardcount, cluster_labels)\n",
        "\n",
        "    #output MDS plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'MDS_plot_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    medoids = {i: value for i, value in enumerate(cluster_indices)}\n",
        "    stress[n_clusters] = mds_2d_cluster_plot(outputfilename, dist_matrices[key_norm], cluster_labels, pts_closest_to_mean = {}, medoid_indices = medoids, high_density_indices = {})\n",
        "\n",
        "  #output silhouette averages to file\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'silhouette_averages_KMedoids_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "  output_dictionary(outputfilename, sil_avg)\n",
        "\n",
        "  #output a scaled inertia value plot\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'inertia_plot_KMedoids_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "  scalar, _, _ = sa_kmedoids(dist_matrices[key_norm], 1)\n",
        "  output_inertia_plot(outputfilename, inertia, scalar)\n",
        "\n",
        "  #output stress values to file\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'stress_values_from_MDS_KMedoids_CardCount' + tag_for_dir_and_filenames\n",
        "  output_dictionary(outputfilename, stress)"
      ],
      "id": "KLvqSxnkLQ1B"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "QxE9HSXDc5Ne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af8b2d7-5eaf-470a-e19b-1fb90f5319a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming K-medoids clustering for N-Gram playtraces using Jensen-Shannon norm...\n"
          ]
        }
      ],
      "source": [
        "#perform K-medoids clustering for N-Gram playtraces.\n",
        "print(\"Perfoming K-medoids clustering for N-Gram playtraces using Jensen-Shannon norm...\")\n",
        "\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  key_gram = 'N_Gram_' + str(n)\n",
        "  sil_avg = {}\n",
        "  inertia = {}\n",
        "  stress = {}\n",
        "  for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "    inertia[n_clusters], cluster_indices, cluster_labels = sa_kmedoids(dist_matrices[key_gram], n_clusters)\n",
        "    sil_avg[n_clusters] = silhouette_score(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "    sil_coeffs = silhouette_samples(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_KMedoids_NGram_' + str(n) + '_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, sil_coeffs, sil_avg[n_clusters], cluster_labels)\n",
        "\n",
        "    #output cluster centroids (probability distributions in this case)\n",
        "    col_name = 'ProbDict_' + str(n)\n",
        "    cluster_centres = traces_ngrams[col_name].iloc[cluster_indices]\n",
        "    labels = ['Cluster ' + str(n) for n in range(0, n_clusters)]\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_KMedoids_NGram_' + str(n) + '_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    plot_distribution_comparison(outputfilename, cluster_centres, labels, thresholds[n])\n",
        "\n",
        "    #output MDS plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'MDS_plot_KMedoids_NGram_' + str(n) + '_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    medoids = {i: value for i, value in enumerate(cluster_indices)}\n",
        "    stress[n_clusters] = mds_2d_cluster_plot(outputfilename, dist_matrices[key_gram], cluster_labels, pts_closest_to_mean = {}, medoid_indices = medoids, high_density_indices = {})\n",
        "\n",
        "  #output silhouette averages to file\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'silhouette_averages_KMedoids_NGram_' + str(n) + '_' + tag_for_dir_and_filenames\n",
        "  output_dictionary(outputfilename, sil_avg)\n",
        "\n",
        "  #output a scaled inertia value plot\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'inertia_plot_KMedoids_NGram_' + str(n) + '_' + tag_for_dir_and_filenames\n",
        "  scalar, _, _ = sa_kmedoids(dist_matrices[key_gram], 1)\n",
        "  output_inertia_plot(outputfilename, inertia, scalar)\n",
        "\n",
        "  #output stress values to file\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'stress_values_from_MDS_KMedoids_CardCount' + tag_for_dir_and_filenames\n",
        "  output_dictionary(outputfilename, stress)\n"
      ],
      "id": "QxE9HSXDc5Ne"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "lrVCiE9W75Ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ddba9fe-42cc-4a4c-d174-949974792513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming DBSCAN clustering for card count playtraces using l_k-norm...\n"
          ]
        }
      ],
      "source": [
        "#perform DBSCAN clustering for card count playtraces using l_k-norm\n",
        "print(\"Perfoming DBSCAN clustering for card count playtraces using l_k-norm...\")\n",
        "\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  #note that for DBSCAN, inertia is not a valid metric (dependent on spherical clusters), so we look for the\n",
        "  #best silhouette average and just output this result.\n",
        "  best_sil_avg = -10000\n",
        "  best_sil_coeffs = None\n",
        "  best_minpts = 0\n",
        "  best_epsilon = 0\n",
        "  best_cluster_labels = None\n",
        "  best_noise_ratio = 0 #this is the noise ratio in the case with the highest silhouette average\n",
        "  for minpts in range(minpts_min, minpts_max, minpts_stepsize):\n",
        "    for eps in np.arange(epsilon_min, epsilon_max, epsilon_stepsize):\n",
        "      cluster_labels = sa_dbscan(dist_matrices_normalised[key_norm], minpts, eps)\n",
        "      #in DBSCAN anything with a label of '-1' is treated as noise\n",
        "      #so we need to do the following:\n",
        "      #1. Keep a record of the portion of traces that are classified as noise, too many and the results should be ignored\n",
        "      #2. Filter our traces to remove traces that are considered as noise, prior to computing the silhouette average\n",
        "      noise_ratio = np.sum(cluster_labels == -1)/len(traces_cardcount_slim)\n",
        "      cluster_labels_no_noise = cluster_labels[cluster_labels > -1]\n",
        "      indices_to_remove = [i for i, value in enumerate(cluster_labels) if value == -1]\n",
        "      dist_matrix_no_noise = dist_matrices_normalised[key_norm].drop(index=indices_to_remove, columns=indices_to_remove)\n",
        "      no_clusters_found = len(np.unique(cluster_labels_no_noise))\n",
        "      if (no_clusters_found < 2):\n",
        "        #in this case we cannot compute a silhouette score and we just output the centroids\n",
        "        #how do we determine the best choice of hyperparameters in this case?\n",
        "        i = 0 #add necessary code here\n",
        "      else:\n",
        "        sil_avg = silhouette_score(dist_matrix_no_noise, cluster_labels_no_noise, metric = 'precomputed')\n",
        "        if sil_avg > best_sil_avg:\n",
        "          best_sil_avg = sil_avg\n",
        "          best_sil_coeffs = silhouette_samples(dist_matrix_no_noise, cluster_labels_no_noise, metric = 'precomputed')\n",
        "          best_minpts = minpts\n",
        "          best_epsilon = eps\n",
        "          best_cluster_labels_with_noise = cluster_labels\n",
        "          best_cluster_labels_no_noise = cluster_labels_no_noise\n",
        "          best_noise_ratio = noise_ratio\n",
        "          best_no_clusters = len(np.unique(cluster_labels_no_noise))\n",
        "\n",
        "  #output silhouette plots\n",
        "  if (best_sil_avg > 0):\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_DBSCAN/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_DBSCAN_CardCount_eps_' + str(round(best_epsilon,2)) + '_minpts_' + str(best_minpts) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels_no_noise)\n",
        "\n",
        "    #output MDS plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_DBSCAN/' + tag_for_dir_and_filenames + '/' + 'MDS_plot_DBSCAN_CardCount_eps_' + str(round(best_epsilon,2)) + '_minpts_' + str(best_minpts) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    closest_to_mean_indices = closest_to_mean_l_k_norm(traces_cardcount_slim, best_cluster_labels_with_noise, k)\n",
        "    stress = mds_2d_cluster_plot(outputfilename, dist_matrices[key_norm], best_cluster_labels_with_noise, closest_to_mean_indices)\n",
        "    #stress = mds_2d_cluster_plot(outputfilename, dist_matrices_normalised[key_norm], best_cluster_labels_with_noise, high_density_indices, pts_closest_to_mean = closest_to_mean_indices)\n",
        "\n",
        "    #output cluster centroids, converting indices to playtraces\n",
        "    closest_to_mean_indices_arr = [value for value in closest_to_mean_indices.values()]\n",
        "    cluster_centres = traces_cardcount_slim.iloc[closest_to_mean_indices_arr]\n",
        "    labels = ['Cluster ' + str(n) for n in range(0, len(closest_to_mean_indices_arr))]\n",
        "    cluster_centres = traces_cardcount_slim.iloc[closest_to_mean_indices_arr]\n",
        "    outputfilename = google_drive_parent_dir + 'Results_DBSCAN/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_DBSCAN_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    plot_cluster_centroids(outputfilename, cluster_centres, card_types)\n",
        "\n",
        "    #output values\n",
        "    outputfilename = google_drive_parent_dir + 'Results_DBSCAN/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_DBSCAN_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    values = [best_sil_avg, best_minpts, best_epsilon, best_no_clusters, best_noise_ratio, stress]\n",
        "    valueslabels_list = ['sil_avg', 'minpts', 'epsilon', 'clusters', 'noise ratio', 'MDS_stress']\n",
        "    output_values(outputfilename, values, valueslabels_list)\n",
        "\n",
        "  else:\n",
        "    print(\"No clustering found for DBSCAN for card count playtraces with l_\" + str(k) + \"-norm\")\n",
        "    #TODO: Anything else we can do here?\n"
      ],
      "id": "lrVCiE9W75Ok"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "uerIDa81inOj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "911f7ada-39ea-4a43-9f18-14a8a7dc7af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming DBSCAN clustering for N-Gram playtraces using Jensen-Shannon norm...\n",
            "No clustering found for DBSCAN for N-gram playtraces with N=1\n",
            "No clustering found for DBSCAN for N-gram playtraces with N=3\n"
          ]
        }
      ],
      "source": [
        "print(\"Perfoming DBSCAN clustering for N-Gram playtraces using Jensen-Shannon norm...\")\n",
        "\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  key_gram = 'N_Gram_' + str(n)\n",
        "  best_sil_avg = -10000\n",
        "  best_sil_coeffs = None\n",
        "  best_minpts = 0\n",
        "  best_epsilon = 0\n",
        "  best_cluster_labels = None\n",
        "  best_noise_ratio = 0 #this is the noise ratio in the case with the highest silhouette average\n",
        "  for minpts in range(minpts_min, minpts_max, minpts_stepsize):\n",
        "    for eps in np.arange(epsilon_min, epsilon_max, epsilon_stepsize):\n",
        "      cluster_labels = sa_dbscan(dist_matrices[key_gram], minpts, eps)\n",
        "      noise_ratio = np.sum(cluster_labels == -1)/len(traces_ngrams)\n",
        "      cluster_labels_no_noise = cluster_labels[cluster_labels > -1]\n",
        "      indices_to_remove = [i for i, value in enumerate(cluster_labels) if value == -1]\n",
        "      dist_matrix_no_noise = dist_matrices[key_gram].drop(index=indices_to_remove, columns=indices_to_remove)\n",
        "      no_clusters_found = len(np.unique(cluster_labels_no_noise))\n",
        "      if (no_clusters_found < 2):\n",
        "        #in this case we cannot compute a silhouette score and we just output the centroids\n",
        "        #how do we determine the best choice of hyperparameters in this case?\n",
        "        i = 0 #add necessary code here\n",
        "      else:\n",
        "        sil_avg = silhouette_score(dist_matrix_no_noise, cluster_labels_no_noise, metric = 'precomputed')\n",
        "        if sil_avg > best_sil_avg:\n",
        "          best_sil_avg = sil_avg\n",
        "          best_sil_coeffs = silhouette_samples(dist_matrix_no_noise, cluster_labels_no_noise, metric = 'precomputed')\n",
        "          best_minpts = minpts\n",
        "          best_epsilon = eps\n",
        "          best_cluster_labels = cluster_labels_no_noise\n",
        "          best_noise_ratio = noise_ratio\n",
        "\n",
        "  #output silhouette plots\n",
        "  if (best_sil_avg > 0):\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_DBSCAN/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_DBSCAN_NGram_N_' + str(n) + '_eps_' + str(round(best_epsilon,2)) + '_minpts_' + str(best_minpts) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "    #output MDS plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_DBSCAN/' + tag_for_dir_and_filenames + '/' + 'MDS_plot_DBSCAN_NGram_N_' + str(n) + '_eps_' + str(round(best_epsilon,2)) + '_minpts_' + str(best_minpts) + '_' + tag_for_dir_and_filenames\n",
        "    closest_to_mean_indices = closest_to_mean_js_norm(traces_ngrams['ProbArray_' + str(n)], best_cluster_labels_with_noise)\n",
        "    stress = mds_2d_cluster_plot(outputfilename, dist_matrices[key_gram], best_cluster_labels_with_noise, closest_to_mean_indices)\n",
        "\n",
        "    #output cluster centroids (probability distributions in this case)\n",
        "    col_name = 'ProbDict_' + str(n)\n",
        "    closest_to_mean_indices_arr = [value for value in closest_to_mean_indices.values()]\n",
        "    cluster_centres = traces_ngrams[col_name].iloc[closest_to_mean_indices_arr]\n",
        "    labels = ['Cluster ' + str(n) for n in range(0, len(closest_to_mean_indices_arr))]\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_DBSCAN_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    plot_distribution_comparison(outputfilename, cluster_centres, labels, thresholds[n])\n",
        "\n",
        "    #output values\n",
        "    outputfilename = google_drive_parent_dir + 'Results_DBSCAN/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_DBSCAN_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    values = [best_sil_avg, best_minpts, best_epsilon, best_noise_ratio, stress]\n",
        "    valueslabels_list = ['sil_avg', 'minpts', 'epsilon', 'noise ratio', 'MDS_stress']\n",
        "    output_values(outputfilename, values, valueslabels_list)\n",
        "\n",
        "  else:\n",
        "    print(\"No clustering found for DBSCAN for N-gram playtraces with N=\" + str(n))\n",
        "    #TODO: Anything else we can do here?"
      ],
      "id": "uerIDa81inOj"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBK7CVg0mKmg",
        "outputId": "517aeb2b-698b-4760-b5c5-5b47d88fc892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing spectral clustering for card count playtraces using fully connected graphs with l_k-norm...\n",
            "No clustering found for Spectral Clustering with fully connected affinity matrix for card count playtraces with l_0.5-norm\n",
            "No clustering found for Spectral Clustering with fully connected affinity matrix for card count playtraces with l_1-norm\n",
            "No clustering found for Spectral Clustering with fully connected affinity matrix for card count playtraces with l_2-norm\n"
          ]
        }
      ],
      "source": [
        "print(\"Performing spectral clustering for card count playtraces using fully connected graphs with l_k-norm...\")\n",
        "\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  best_sil_avg = -10000\n",
        "  best_sil_coeffs = None\n",
        "  best_gamma = 0\n",
        "  best_no_clusters = 0\n",
        "  best_cluster_labels = None\n",
        "  for no_clusters in range(clusters_min, clusters_max, clusters_stepsize):\n",
        "    for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "      key_gamma =  'gamma_' + str(gamma)\n",
        "      cluster_labels = sa_spectral_clustering_AM(connected_affinity_matrices[key_norm][key_gamma], no_clusters)\n",
        "      sil_avg = silhouette_score(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "      if sil_avg > best_sil_avg:\n",
        "        best_sil_avg = sil_avg\n",
        "        best_sil_coeffs = silhouette_samples(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "        best_gamma = gamma\n",
        "        best_no_clusters = no_clusters\n",
        "        best_cluster_labels = cluster_labels\n",
        "\n",
        "  #output silhouette plots\n",
        "  if (best_sil_avg > 0):\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_SPCluster_AM_CardCount_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "    #output MDS plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'MDS_plot_SPCluster_AM_CardCount_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    closest_to_mean_indices = closest_to_mean_l_k_norm(traces_cardcount_slim, best_cluster_labels, k)\n",
        "    stress = mds_2d_cluster_plot(outputfilename, dist_matrices[key_norm], best_cluster_labels, closest_to_mean_indices)\n",
        "\n",
        "    #output cluster centroids, converting indices to playtraces\n",
        "    closest_to_mean_indices_arr = [value for value in closest_to_mean_indices.values()]\n",
        "    cluster_centres = traces_cardcount_slim.iloc[closest_to_mean_indices_arr]\n",
        "    labels = ['Cluster ' + str(n) for n in range(0, len(closest_to_mean_indices_arr))]\n",
        "    cluster_centres = traces_cardcount_slim.iloc[closest_to_mean_indices_arr]\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_SPCluster_AM_CardCount_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    plot_cluster_centroids(outputfilename, cluster_centres, card_types)\n",
        "\n",
        "    #output values\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_SPCluster_AM_CardCount_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    values = [best_sil_avg, best_gamma, best_no_clusters, stress]\n",
        "    valueslabels_list = ['sil_avg', 'gamma', 'clusters', 'MDS_stress']\n",
        "    output_values(outputfilename, values, valueslabels_list)\n",
        "  else:\n",
        "    print(\"No clustering found for Spectral Clustering with fully connected affinity matrix for card count playtraces with l_\" + str(k) + \"-norm\")\n",
        "    #TODO: Anything else we can do here?"
      ],
      "id": "WBK7CVg0mKmg"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "-6kOLd7d5Ts_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb77990-e024-4512-e7de-e694bb541448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing spectral clustering for N-Gram playtraces using fully connected graphs with Jensen-Shannon distance metric...\n"
          ]
        }
      ],
      "source": [
        "print(\"Performing spectral clustering for N-Gram playtraces using fully connected graphs with Jensen-Shannon distance metric...\")\n",
        "\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  key_gram = 'N_Gram_' + str(n)\n",
        "  best_sil_avg = -10000\n",
        "  best_sil_coeffs = None\n",
        "  best_gamma = 0\n",
        "  best_no_clusters = 0\n",
        "  best_cluster_labels = None\n",
        "  for no_clusters in range(clusters_min, clusters_max, clusters_stepsize):\n",
        "    for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "      key_gamma =  'gamma_' + str(gamma)\n",
        "      cluster_labels = sa_spectral_clustering_AM(connected_affinity_matrices[key_gram][key_gamma], no_clusters)\n",
        "      sil_avg = silhouette_score(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "      if sil_avg > best_sil_avg:\n",
        "        best_sil_avg = sil_avg\n",
        "        best_sil_coeffs = silhouette_samples(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "        best_gamma = gamma\n",
        "        best_no_clusters = no_clusters\n",
        "        best_cluster_labels = cluster_labels\n",
        "\n",
        "  #output silhouette plots\n",
        "  if (best_sil_avg > 0):\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_SPCluster_AM_NGram_' + str(n) +'_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "    #output MDS plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'MDS_plot_SPCluster_AM_NGram_' + str(n) +'_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    closest_to_mean_indices = closest_to_mean_js_norm(traces_ngrams['ProbArray_' + str(n)], best_cluster_labels)\n",
        "    stress = mds_2d_cluster_plot(outputfilename, dist_matrices[key_gram], best_cluster_labels, closest_to_mean_indices)\n",
        "\n",
        "    #output cluster centroids (probability distributions in this case)\n",
        "    col_name = 'ProbDict_' + str(n)\n",
        "    closest_to_mean_indices_arr = [value for value in closest_to_mean_indices.values()]\n",
        "    cluster_centres = traces_ngrams[col_name].iloc[closest_to_mean_indices_arr]\n",
        "    labels = ['Cluster ' + str(n) for n in range(0, len(closest_to_mean_indices_arr))]\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_SPCluster_AM_NGram_' + str(n) +'_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    plot_distribution_comparison(outputfilename, cluster_centres, labels, thresholds[n])\n",
        "\n",
        "    #output values\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_SPCluster_AM_NGram_' + str(n) +'_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    values = [best_sil_avg, best_gamma, best_no_clusters, stress]\n",
        "    valueslabels_list = ['sil_avg', 'gamma', 'clusters', 'MDS_stress']\n",
        "    output_values(outputfilename, values, valueslabels_list)\n",
        "  else:\n",
        "    print(\"No clustering found for Spectral Clustering with fully connected affinity matrix for NGram playtraces with N=\" + str(n))\n",
        "    #TODO: Anything else we can do here?\n",
        "\n",
        "  #TODO:Visualisation and cluster centroids"
      ],
      "id": "-6kOLd7d5Ts_"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "NP4YJ48R_XtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6a79cf-b596-4994-b2a3-b1fbe4154340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing spectral clustering for card count playtraces using K-Nearest Neighbours affinity matrix with l_k-norm...\n"
          ]
        }
      ],
      "source": [
        "print(\"Performing spectral clustering for card count playtraces using K-Nearest Neighbours affinity matrix with l_k-norm...\")\n",
        "\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  best_sil_avg = -10000\n",
        "  best_sil_coeffs = None\n",
        "  best_nn = 0\n",
        "  best_no_clusters = 0\n",
        "  best_cluster_labels = None\n",
        "  for no_clusters in range(clusters_min, clusters_max, clusters_stepsize):\n",
        "    for nn in np.arange(nearest_neighbours_min, nearest_neighbours_max, nearest_neighbours_stepsize):\n",
        "      key_knn =  'knn_' + str(nn)\n",
        "      cluster_labels = sa_spectral_clustering_AM(knn_affinity_matrices[key_norm][key_knn], no_clusters)\n",
        "      sil_avg = silhouette_score(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "      if sil_avg > best_sil_avg:\n",
        "        best_sil_avg = sil_avg\n",
        "        best_sil_coeffs = silhouette_samples(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "        best_nn = nn\n",
        "        best_no_clusters = no_clusters\n",
        "        best_cluster_labels = cluster_labels\n",
        "\n",
        "  #output silhouette plots\n",
        "  if (best_sil_avg > 0):\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_SPCluster_KNN_CardCount_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "    #output MDS plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'MDS_plot_SPCluster_KNN_CardCount_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    closest_to_mean_indices = closest_to_mean_l_k_norm(traces_cardcount_slim, best_cluster_labels, k)\n",
        "    stress = mds_2d_cluster_plot(outputfilename, dist_matrices[key_norm], best_cluster_labels, closest_to_mean_indices)\n",
        "\n",
        "    #output cluster centroids, converting indices to playtraces\n",
        "    closest_to_mean_indices_arr = [value for value in closest_to_mean_indices.values()]\n",
        "    cluster_centres = traces_cardcount_slim.iloc[closest_to_mean_indices_arr]\n",
        "    labels = ['Cluster ' + str(n) for n in range(0, len(closest_to_mean_indices_arr))]\n",
        "    cluster_centres = traces_cardcount_slim.iloc[closest_to_mean_indices_arr]\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_SPCluster_KNN_CardCount_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    plot_cluster_centroids(outputfilename, cluster_centres, card_types)\n",
        "\n",
        "    #output values\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_SPCluster_KNN_CardCount_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    values = [best_sil_avg, best_nn, best_no_clusters, stress]\n",
        "    valueslabels_list = ['sil_avg', 'k-NearestNeighbours', 'clusters', 'MDS_stress']\n",
        "    output_values(outputfilename, values, valueslabels_list)\n",
        "  else:\n",
        "    print(\"No clustering found for Spectral Clustering with k_Nearest Neighbours affinity matrix for card count playtraces with l_\" + str(k) + \"-norm\")\n",
        "    #TODO: Anything else we can do here?\n"
      ],
      "id": "NP4YJ48R_XtQ"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "xnjklWxeqxIy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7783a90-7eec-47bd-a652-daa5bbb4e762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing spectral clustering for N-Gram playtraces using K-Nearest Neighbours affinity matrix with Jensen-Shannon...\n"
          ]
        }
      ],
      "source": [
        "print(\"Performing spectral clustering for N-Gram playtraces using K-Nearest Neighbours affinity matrix with Jensen-Shannon...\")\n",
        "\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  key_gram = 'N_Gram_' + str(n)\n",
        "  best_sil_avg = -10000\n",
        "  best_sil_coeffs = None\n",
        "  best_nn = 0\n",
        "  best_no_clusters = 0\n",
        "  best_cluster_labels = None\n",
        "  for no_clusters in range(clusters_min, clusters_max, clusters_stepsize):\n",
        "    for nn in np.arange(nearest_neighbours_min, nearest_neighbours_max, nearest_neighbours_stepsize):\n",
        "      key_knn =  'knn_' + str(nn)\n",
        "      cluster_labels = sa_spectral_clustering_AM(knn_affinity_matrices[key_gram][key_knn], no_clusters)\n",
        "      sil_avg = silhouette_score(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "      if sil_avg > best_sil_avg:\n",
        "        best_sil_avg = sil_avg\n",
        "        best_sil_coeffs = silhouette_samples(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "        best_nn = nn\n",
        "        best_no_clusters = no_clusters\n",
        "        best_cluster_labels = cluster_labels\n",
        "\n",
        "  #output silhouette plots\n",
        "  if (best_sil_avg > 0):\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_SPCluster_AM_NGram_' + str(n) + '_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "    #output MDS plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'MDS_plot_SPCluster_AM_NGram_' + str(n) + '_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    closest_to_mean_indices = closest_to_mean_js_norm(traces_ngrams['ProbArray_' + str(n)], best_cluster_labels)\n",
        "    stress = mds_2d_cluster_plot(outputfilename, dist_matrices[key_gram], best_cluster_labels, closest_to_mean_indices)\n",
        "\n",
        "    #output cluster centroids (probability distributions in this case)\n",
        "    col_name = 'ProbDict_' + str(n)\n",
        "    closest_to_mean_indices_arr = [value for value in closest_to_mean_indices.values()]\n",
        "    cluster_centres = traces_ngrams[col_name].iloc[closest_to_mean_indices_arr]\n",
        "    labels = ['Cluster ' + str(n) for n in range(0, len(closest_to_mean_indices_arr))]\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_SPCluster_AM_NGram_' + str(n) + '_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    plot_distribution_comparison(outputfilename, cluster_centres, labels, thresholds[n])\n",
        "\n",
        "    #output values\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPClustering/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_SPCluster_AM_NGram_' + str(n) + '_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    values = [best_sil_avg, best_nn, best_no_clusters, stress]\n",
        "    valueslabels_list = ['sil_avg', 'k-NearestNeighbours', 'clusters', 'MDS_stress']\n",
        "    output_values(outputfilename, values, valueslabels_list)\n",
        "  else:\n",
        "    print(\"No clustering found for Spectral Clustering with k_Nearest Neighbours affinity matrix for N-Gram playtraces with N=\" + str(n))\n",
        "    #TODO: Anything else we can do here?\n"
      ],
      "id": "xnjklWxeqxIy"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "PEAYxyffv74V"
      },
      "outputs": [],
      "source": [],
      "id": "PEAYxyffv74V"
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}