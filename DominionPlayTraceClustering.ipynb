{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/owenant/PlayTraces/blob/main/DominionPlayTraceClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3d63dad0",
      "metadata": {
        "id": "3d63dad0"
      },
      "outputs": [],
      "source": [
        "#This notebook calculates silhouette averages and clusters for Card Count and NGram playtraces for Dominion produced by the\n",
        "#TableTop Games framework"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn-extra"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy3-UJsWmTH0",
        "outputId": "77fe9a89-62f8-4683-b92d-50a7b60e67d5"
      },
      "id": "Dy3-UJsWmTH0",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn-extra\n",
            "  Downloading scikit_learn_extra-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.3.0)\n",
            "Installing collected packages: scikit-learn-extra\n",
            "Successfully installed scikit-learn-extra-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d792f3be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d792f3be",
        "outputId": "4545df78-799f-498f-fd59-ef0e64c3419c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pdb\n",
        "import re\n",
        "import nltk\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product, permutations, combinations\n",
        "from nltk import ngrams\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from google.colab import drive\n",
        "import csv\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list of clustering methods to use\n",
        "clustering_methods = ['KMeans', 'KMedoids', 'DBSCAN', 'SPCluster_AM','SPCluster_KNN']\n",
        "\n",
        "#filenames and directory locations\n",
        "google_drive_parent_dir = \"gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/\"\n",
        "card_count_data_dir = google_drive_parent_dir + \"DataCardCount/\"\n",
        "ngram_data_dir = google_drive_parent_dir + \"DataNGrams/\"\n",
        "card_count_data_filename = card_count_data_dir + \"trace_logfile_BMWG_vs_DW_GPM100.txt\"\n",
        "ngram_data_filename = ngram_data_dir + \"ActionsReduced_BMWG_vs_DW_GPM100.csv\"\n",
        "tag_for_dir_and_filenames = 'BMWG_vs_DW_GPM100'\n",
        "#card_count_data_filename = card_count_data_dir + \"trace_logfile_Budget_500_vs_Budget_500_GPM100_SD_NoSelfPlay.txt\"\n",
        "#ngram_data_filename = ngram_data_dir + \"ActionsReduced_Budget500_vs_Budget500_GPM100_SD.csv\"\n",
        "#tag_for_dir_and_filenames = 'MCTS_b500_vs_b500_GPM100_SD'\n",
        "\n",
        "#create new directory for output files\n",
        "for method in clustering_methods:\n",
        "  new_dir_path = google_drive_parent_dir + 'Results_' + method + '/' + tag_for_dir_and_filenames + '/'\n",
        "  os.makedirs(new_dir_path, exist_ok=True)\n",
        "\n",
        "  # Verify that the directory has been created\n",
        "  if os.path.exists(new_dir_path):\n",
        "      print(f\"Directory '{new_dir_path}' created successfully.\")\n",
        "  else:\n",
        "      print(f\"Failed to create directory '{new_dir_path}'.\")\n",
        "\n",
        "#also create directory for round and score distributions\n",
        "new_dir_path = google_drive_parent_dir + 'RoundAndScoreDistributions/' + tag_for_dir_and_filenames + '/'\n",
        "os.makedirs(new_dir_path, exist_ok=True)\n",
        "\n",
        "# Verify that the directory has been created\n",
        "if os.path.exists(new_dir_path):\n",
        "    print(f\"Directory '{new_dir_path}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Failed to create directory '{new_dir_path}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duv7Pp_FAnZc",
        "outputId": "40ab3e75-6202-42e9-d89c-c0fc47a5f78c"
      },
      "id": "duv7Pp_FAnZc",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_KMeans/BMWG_vs_DW_GPM100/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_KMedoids/BMWG_vs_DW_GPM100/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_DBSCAN/BMWG_vs_DW_GPM100/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_SPCluster_AM/BMWG_vs_DW_GPM100/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_SPCluster_KNN/BMWG_vs_DW_GPM100/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/RoundAndScoreDistributions/BMWG_vs_DW_GPM100/' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters for notebook execution\n",
        "\n",
        "#kingdom card set\n",
        "kingdom_set = 'SD'\n",
        "\n",
        "#parameters if using TAG input data\n",
        "logs_from_tag = True\n",
        "agent_names = ['BMWG', 'DW']\n",
        "games_per_matchup = 100\n",
        "no_self_play = True\n",
        "\n",
        "#parameters for grid search for clustering methods\n",
        "\n",
        "#number of clusters to check across all clustering methods (excluding DBSCAN)\n",
        "clusters_min = 2\n",
        "clusters_max = 5\n",
        "clusters_stepsize = 1\n",
        "\n",
        "#DBSCAN\n",
        "minpts_min = 5\n",
        "minpts_max = 50\n",
        "minpts_stepsize = 5\n",
        "epsilon_min = 0.1\n",
        "epsilon_max = 1\n",
        "epsilon_stepsize = 0.1\n",
        "\n",
        "#Spectral clustering K-Nearest Neighbours\n",
        "nearest_neighbours_min = 5\n",
        "nearest_neighbours_max = 50\n",
        "nearest_neighbours_stepsize = 5\n",
        "\n",
        "#Spectral clustering radial basis function\n",
        "gamma_min = 0.1\n",
        "gamma_max = 1\n",
        "gamma_stepsize = 0.1\n",
        "\n",
        "#number of N-gram types to search over\n",
        "ngram_min = 1\n",
        "ngram_max = 2\n",
        "ngram_stepsize = 1\n",
        "\n",
        "#values of k for l_k norm\n",
        "k_norms = [0.1, 0.5, 1, 2]\n",
        "\n",
        "#threshold values for plotting probability distributions for N-Gram playtraces\n",
        "thresholds = {n: 0.01 for n in range(ngram_min, ngram_max + 1, ngram_stepsize)}"
      ],
      "metadata": {
        "id": "lM1jnEiFsQwI"
      },
      "id": "lM1jnEiFsQwI",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kingdom card types\n",
        "card_types_SD = ['ARTISAN', 'BANDIT', 'BUREAUCRAT', 'CHAPEL', 'FESTIVAL', 'GARDENS', 'SENTRY', 'THRONE_ROOM', 'WITCH',\n",
        "                 'WORKSHOP', 'CURSE', 'PROVINCE', 'DUCHY', 'ESTATE', 'GOLD', 'SILVER', 'COPPER']\n",
        "card_types_FG1E = ['CELLAR','MARKET','MILITIA','MINE','MOAT','REMODEL','SMITHY','VILLAGE',\n",
        "                'WOODCUTTER','WORKSHOP','CURSE','PROVINCE', 'DUCHY', 'ESTATE', 'GOLD', 'SILVER', 'COPPER']\n",
        "\n",
        "if kingdom_set == 'SD':\n",
        "  card_types = card_types_SD\n",
        "elif kingdom_set == 'FG1E':\n",
        "  card_types = card_types_FG1E\n",
        "else:\n",
        "  print('Unrecognised kingdom card set')"
      ],
      "metadata": {
        "id": "m_oEYop-1Ma2"
      },
      "id": "m_oEYop-1Ma2",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#functions to process data from TAG\n",
        "\n",
        "def gameID_to_matchup(game_id, player_no, matchup_list, no_games_per_matchup, min_game_id):\n",
        "    game_group = int((game_id - min_game_id)/no_games_per_matchup)\n",
        "    matchup = matchup_list[game_group]\n",
        "    agent1, agent2 = matchup\n",
        "    if player_no == 0:\n",
        "        return agent1\n",
        "    else:\n",
        "        return agent2\n",
        "\n",
        "def add_TAG_agent_names(agent_names, games_per_match_up, no_self_play, logs_from_tag, data):\n",
        "  NoOfGames = len(data['GameID'].unique())\n",
        "  min_GameID = data['GameID'].min()\n",
        "\n",
        "  #first generate match-ups\n",
        "  matchups = []\n",
        "  if no_self_play:\n",
        "    matchups = list(permutations(agent_names, 2))\n",
        "  else:\n",
        "      for agent1 in agent_names:\n",
        "          for agent2 in agent_names:\n",
        "              matchups.append((agent1, agent2))\n",
        "\n",
        "  #add agent names to data set\n",
        "  data['AgentName'] = data.apply(lambda row: gameID_to_matchup(row['GameID'], row['Player'], matchups, games_per_matchup, min_GameID), axis = 1)\n",
        "\n",
        "  #finally we also add the name of the agent of the opponent\n",
        "  min_GameID = data['GameID'].min()\n",
        "  data['Opponent'] = data.apply(lambda row: 1.0 if row['Player'] == 0.0 else 0.0, axis = 1)\n",
        "  if logs_from_tag:\n",
        "      data['AgentNameOpponent'] = data.apply(lambda row: gameID_to_matchup(row['GameID'], row['Opponent'], matchups, games_per_matchup, min_GameID), axis = 1)\n",
        "  else:\n",
        "      gameid_to_players_dict = data.groupby('GameID')['AgentName'].apply(list).to_dict()\n",
        "      data['AgentNameOpponent'] = data.apply(lambda row: other_dict_element(gameid_to_players_dict, row['GameID'], row['AgentName']), axis = 1)\n",
        "\n",
        "\n",
        "#functions to process card count data\n",
        "def copy_final_deck_at_game_end(group, roundMax, noPlayers):\n",
        "  #This function repeatedly copies the final decks of two players at the game end, so that the game is extended to\n",
        "  #have roundMax rounds\n",
        "  final_round = int(group['Round'].max())\n",
        "  if (roundMax-1) == final_round:\n",
        "      #in this case we dont need to extend the play trace\n",
        "      return group\n",
        "  else:\n",
        "      final_row_copy = pd.concat([group.iloc[-noPlayers:]] * ((roundMax-1) - final_round), ignore_index=True)\n",
        "      #we need to update the Round counter so that every other row it increments by one\n",
        "      final_row_copy['Round'] = [final_round + 1 + i // 2 for i in range(((roundMax-1) - final_round)*2)]\n",
        "      return pd.concat([group, final_row_copy], ignore_index=True)\n",
        "\n",
        "#given a dictionary whose elements are lists of length two, grab the other element not given by elem\n",
        "def other_dict_element(my_dict, my_key, my_elem):\n",
        "    index_of_given_element = my_dict[my_key].index(my_elem)\n",
        "    index_of_other_element =  1 if (index_of_given_element == 0) else 0\n",
        "    return my_dict[my_key][index_of_other_element]\n",
        "\n",
        "def process_card_count_data(cardcount_filename, agent_names, games_per_matchup,\n",
        "                            no_self_play, card_types, logs_from_tag):\n",
        "  data  = pd.read_csv(cardcount_filename, sep = '\\t')\n",
        "  add_TAG_agent_names(agent_names, games_per_matchup, no_self_play, logs_from_tag, data)\n",
        "  index_cols = ['Player', 'GameID']\n",
        "  non_card_types_round_indep_cols = ['AgentName', 'AgentNameOpponent', 'Win', 'FinalScore', 'TotalRounds']\n",
        "  cols = index_cols + non_card_types_round_indep_cols + ['Round'] + card_types #final set of cols to keep\n",
        "  data = data[data['Turn'] == 1] #only want cards at end of round\n",
        "  data = data.loc[:, cols]\n",
        "\n",
        "  #freeze decks and copy to max round number\n",
        "  no_players = 2\n",
        "  gameLengths = data.groupby(['GameID'])['Round'].max()\n",
        "  maxNoOfRounds = int(gameLengths.max()) + 1 #round counter starts at zero\n",
        "  noOfGames = len(data['GameID'].unique())\n",
        "  data = data.groupby('GameID').apply(copy_final_deck_at_game_end, maxNoOfRounds, no_players).reset_index(drop = True)\n",
        "\n",
        "  #check shape of data\n",
        "  print(\"Card count shape check:\")\n",
        "  print(\"Expected no rows: \" + str(maxNoOfRounds*no_players*noOfGames))\n",
        "  print(\"Expected no of cols: \" + str(len(card_types)+8))\n",
        "  print(data.shape)\n",
        "\n",
        "  return data\n",
        "\n",
        "def flatten_card_count_data(cardcount_data, card_types):\n",
        "  #next we need to flatten our data so that each trace is a single row.\n",
        "  #We also drop the round label as it is redundant\n",
        "  #and it will get reintroduced when flattening through the revised column names\n",
        "\n",
        "  #first create dataframe consisting of only non card type data types that are round\n",
        "  #independent\n",
        "  index_cols = ['Player', 'GameID']\n",
        "  non_card_types_round_indep_cols = ['AgentName', 'AgentNameOpponent', 'Win', 'FinalScore', 'TotalRounds']\n",
        "  non_card_data_round_indep = cardcount_data[index_cols + non_card_types_round_indep_cols].drop_duplicates()\n",
        "\n",
        "  #next need to Group by Player and GameID and then flatten card data by round\n",
        "  traces_tmp = cardcount_data[index_cols + card_types]\n",
        "  gameLengths = cardcount_data.groupby(['GameID'])['Round'].max()\n",
        "  maxNoOfRounds = int(gameLengths.max()) + 1 #round counter starts at zero\n",
        "  cols = [card_types[i] + \"_R\" + str(r)\n",
        "          for r in range(0, maxNoOfRounds) for i in range(0, len(card_types))]\n",
        "\n",
        "  extended_traces_flat = traces_tmp.groupby(index_cols).apply(lambda df: df[card_types].values.flatten())\n",
        "  extended_traces_flat = pd.DataFrame(extended_traces_flat, columns = ['Trace']).reset_index()\n",
        "  extended_traces_flat = pd.concat([extended_traces_flat[index_cols], extended_traces_flat['Trace'].apply(pd.Series)], axis=1)\n",
        "  extended_traces_flat.columns = index_cols + cols\n",
        "\n",
        "  #next we add back in the round independent data\n",
        "  extended_traces_flat = pd.merge(non_card_data_round_indep, extended_traces_flat, on = index_cols)\n",
        "\n",
        "  return extended_traces_flat\n",
        "\n",
        "#functions to process NGram data\n",
        "def format_action(action, cardtypes):\n",
        "  #there are various types of actions we need to format to identify these we use\n",
        "  #regular expressions\n",
        "  pattern_list = []\n",
        "  pattern_list.append(re.compile(r'End Current Phase'))\n",
        "  pattern_list.append(re.compile(r'BuyCard: (' + '|'.join(cardtypes) + r') by player (0|1)'))\n",
        "  pattern_list.append(re.compile(r'(' + '|'.join(cardtypes) + r') : Player (0|1)'))\n",
        "  pattern_list.append(re.compile(r'GainCard: (' + '|'.join(cardtypes) + r') by player (0|1)'))\n",
        "  pattern_list.append(re.compile(r'Player (0|1) trashes a (' + '|'.join(cardtypes) + r') from (?:HAND|DISCARD)'))\n",
        "  pattern_list.append(re.compile(r'DoNothing'))\n",
        "  pattern_list.append(re.compile(r'Player (0|1) moves (' + '|'.join(cardtypes) + r') from HAND to DRAW of player (0|1) \\(visible: (?:true|false)\\)'))\n",
        "  pattern_list.append(re.compile(r'Reveals Hand'))\n",
        "  pattern_list.append(re.compile(r'Sentry .*$')) #captures playing a sentry and then discard/trash two cards\n",
        "  pattern_list.append(re.compile(r'Player (0|1) discards (' + '|'.join(cardtypes) + r')'))\n",
        "  pattern_list.append(re.compile(r'Player (0|1) reveals a (' + '|'.join(cardtypes) + r')'))\n",
        "\n",
        "  match_list = [None] * len(pattern_list)\n",
        "\n",
        "  pattern_to_string_map = ['ECP', 'BUY', 'PLAY', 'GAIN', 'TRASHES',\n",
        "                            'DONOTHING', 'MOVES', 'REVEALSHAND', 'PLAYSSENTRY',\n",
        "                            'DISCARDS', 'REVEALS']\n",
        "\n",
        "  for index in range(0, len(pattern_list)):\n",
        "    matched = pattern_list[index].match(action)\n",
        "    pattern_index = index\n",
        "    if matched != None:\n",
        "      break\n",
        "\n",
        "  if matched == None:\n",
        "    pdb.set_trace()\n",
        "    raise Exception(\"Can't match action description\")\n",
        "\n",
        "  if pattern_index in [0, 5, 7, 8]:\n",
        "    formatted_action =  pattern_to_string_map[pattern_index]\n",
        "  elif pattern_index in [1, 2, 3]:\n",
        "    matched_card =  matched.group(1)\n",
        "    formatted_action =  pattern_to_string_map[pattern_index]  + matched_card\n",
        "  else:\n",
        "    matched_card = matched.group(2)\n",
        "    formatted_action =  pattern_to_string_map[pattern_index]  + matched_card\n",
        "\n",
        "  return formatted_action\n",
        "\n",
        "def process_ngram_data(actions_filename, agent_names, games_per_match_up, no_self_play,\n",
        "                       card_types, ngram_min, ngram_max, ngram_stepsize, logs_from_tag):\n",
        "  data  = pd.read_csv(actions_filename)\n",
        "  data = data[['GameID', 'Player', 'Round','Turn','ActionDescription']]\n",
        "  add_TAG_agent_names(agent_names, games_per_match_up, no_self_play, logs_from_tag, data)\n",
        "  data['ProcAction'] = data.apply(lambda row: format_action(row['ActionDescription'], card_types), axis = 1)\n",
        "  data = data.groupby(['GameID', 'Player','AgentName', 'AgentNameOpponent'])['ProcAction'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "  for n in range(ngram_min, ngram_max +1, ngram_stepsize):\n",
        "    col_name = 'NGrams_' + str(n)\n",
        "    data[col_name] = data.apply(lambda row: list(ngrams(nltk.word_tokenize(row['ProcAction']),n)), axis = 1)\n",
        "\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "HxQYGYs8wAIV"
      },
      "id": "HxQYGYs8wAIV",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process data\n",
        "traces_cardcount = process_card_count_data(card_count_data_filename, agent_names, games_per_matchup, no_self_play, card_types, logs_from_tag)\n",
        "traces_ngrams = process_ngram_data(ngram_data_filename, agent_names, games_per_matchup, no_self_play,\n",
        "                       card_types, ngram_min, ngram_max, ngram_stepsize, logs_from_tag)\n",
        "print(len(traces_ngrams))"
      ],
      "metadata": {
        "id": "t3I3J5tKvfoA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4526901f-382b-4db0-907a-e30155e94b8e"
      },
      "id": "t3I3J5tKvfoA",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Card count shape check:\n",
            "Expected no rows: 12000\n",
            "Expected no of cols: 25\n",
            "(12000, 25)\n",
            "400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to plot score and round distributions and output to file\n",
        "def plot_score_round_distributions(outputfilename, card_count_data):\n",
        "  fig, axs = plt.subplots(2, 1)\n",
        "  grouped_data = card_count_data.groupby('GameID')\n",
        "  score_data = grouped_data['FinalScore'].unique().explode()\n",
        "  axs[0].hist(score_data, bins=np.arange(score_data.min(), score_data.max()+1))\n",
        "  axs[0].set_xlabel('Final score')\n",
        "  axs[0].set_ylabel('Number of games')\n",
        "  axs[0].set_title('Score distribution')\n",
        "  round_data = grouped_data['TotalRounds'].unique().explode()\n",
        "  axs[1].hist(round_data, bins=np.arange(round_data.min(), round_data.max()+1))\n",
        "  axs[1].set_xlabel('Number of rounds')\n",
        "  axs[1].set_ylabel('Number of games')\n",
        "  axs[1].set_title('Round distribution')\n",
        "  fig.tight_layout()\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n"
      ],
      "metadata": {
        "id": "ZguwBNUQeveK"
      },
      "id": "ZguwBNUQeveK",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove outliers based on thresholds for score and length of game\n",
        "score_threshold = 300\n",
        "round_threshold = 300\n",
        "print(\"Round and score distribution before outliers removed:\")\n",
        "outputfilename = google_drive_parent_dir + 'RoundAndScoreDistributions/' + tag_for_dir_and_filenames + '/' + 'RoundAndScoreDistribution_' + tag_for_dir_and_filenames\n",
        "plot_score_round_distributions(outputfilename, traces_cardcount)\n",
        "\n",
        "traces_cardcount = traces_cardcount[(traces_cardcount['FinalScore'] <= score_threshold)\n",
        "                                           & (traces_cardcount['TotalRounds'] <= round_threshold)]\n",
        "new_game_id_list =  traces_cardcount['GameID'].unique()\n",
        "traces_ngrams = traces_ngrams[traces_ngrams['GameID'].isin(new_game_id_list)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwExLtTpgSeO",
        "outputId": "1b60b858-2ddb-4ea4-fa53-8b5a4bc7b058"
      },
      "id": "FwExLtTpgSeO",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round and score distribution before outliers removed:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Round and score distribution after outliers removed:\")\n",
        "outputfilename = google_drive_parent_dir + 'RoundAndScoreDistributions/' + tag_for_dir_and_filenames + '/' + 'RoundAndScoreDistribution_no_outliers_' + tag_for_dir_and_filenames\n",
        "plot_score_round_distributions(outputfilename, traces_cardcount)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSWZqXOBksgl",
        "outputId": "aecfe96f-8749-43e3-8a94-252cc01e08dd"
      },
      "id": "DSWZqXOBksgl",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round and score distribution after outliers removed:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flatten card count data so that we have a playtrace per row\n",
        "traces_cardcount = flatten_card_count_data(traces_cardcount, card_types)\n",
        "print(len(traces_cardcount))\n",
        "print(len(traces_ngrams))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6axA42kyBMfb",
        "outputId": "b5aaf903-fb27-4d4a-eaaf-acb93084b511"
      },
      "id": "6axA42kyBMfb",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400\n",
            "400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute the all ngrams list by taking the total list of all observed n-grams for this tournament\n",
        "all_ngrams_list = {}\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  col_name = 'NGrams_' + str(n)\n",
        "  all_ngrams_list[n] = []\n",
        "  for row in traces_ngrams[col_name]:\n",
        "    for gram in row:\n",
        "      if gram not in all_ngrams_list[n]:\n",
        "        all_ngrams_list[n].append(gram)\n",
        "  print(\"Total number of Ngrams for N=\" + str(n) + \":\" + str(len(all_ngrams_list[n])))"
      ],
      "metadata": {
        "id": "oTi_VsyRI6Bl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d57a1c7e-68da-4a8f-ff90-352bc0303d4a"
      },
      "id": "oTi_VsyRI6Bl",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of Ngrams for N=1:9\n",
            "Total number of Ngrams for N=2:29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0b646b11",
      "metadata": {
        "id": "0b646b11"
      },
      "outputs": [],
      "source": [
        "#functions to support NGram analysis\n",
        "\n",
        "#function to compute N-gram probabilities, returns either an array with probability values\n",
        "#in the same order as ngrams in ngrams_all, or a dictionary with the n-grams as key\n",
        "#Unobserved ngrams (i.e. ngrams in ngrams_all, that are not in the trace) are assigned\n",
        "#a default probability of zero.\n",
        "def calc_probabilities(ngrams_trace, ngrams_all, convertToArray = False):\n",
        "    # Compute the frequency of ngrams in the trace\n",
        "    frequency_counter = Counter(ngrams_trace)\n",
        "\n",
        "    #calculate frequencies of all ngrams in ngrams_all that appear in the playtrace\n",
        "    event_count = {gram: frequency_counter.get(gram, 0) for gram in ngrams_all}\n",
        "\n",
        "    #normalise each entry with the number of n-grams observed for that trace, to convert\n",
        "    #counts into probabilities\n",
        "    trace_n_gram_count = sum(frequency_counter.values())\n",
        "    probs = {key: value / (1.0*trace_n_gram_count) for key, value in event_count.items()}\n",
        "\n",
        "    if convertToArray:\n",
        "        probs = np.array(list(probs.values()))\n",
        "\n",
        "    return probs\n",
        "\n",
        "#function to take a probability dictionary and create an array\n",
        "def prob_dict_to_array(prob_dict):\n",
        "    return np.array(list(prob_dict.values()))\n",
        "\n",
        "#funciton to take probability array and convert to dictionary with n-grams as keys\n",
        "#assumes ordering has been maintained\n",
        "def prob_array_to_dict(prob_array, ngrams_all):\n",
        "    prob_dict = {}\n",
        "    index = 0\n",
        "    for gram in ngrams_all:\n",
        "        prob_dict[gram] = prob_array[index]\n",
        "        index+=1\n",
        "    return prob_dict\n",
        "\n",
        "#find the common set of ngrams between two probability dictionaries, with probabilities\n",
        "#above a given threshold\n",
        "def return_common_ngrams_above_threshold(prob_dict1, prob_dict2, threshold):\n",
        "    common_ngrams = []\n",
        "    #look for entries in the first dictionary with non-zero values\n",
        "    for key, value in prob_dict1.items():\n",
        "        if value > threshold:\n",
        "            common_ngrams.append(key)\n",
        "    #repeat for the second dictionary but avoiding duplicates\n",
        "    for key, value in prob_dict2.items():\n",
        "        if (value > threshold) and (key not in common_ngrams):\n",
        "             common_ngrams.append(key)\n",
        "    return common_ngrams\n",
        "\n",
        "#convert a list of ngram tuples into a list of strings\n",
        "def convert_ngram_tuples_to_strings(ngrams_list):\n",
        "    ngrams_str = []\n",
        "    for tuple_item in ngrams_list:\n",
        "        tuple_str = ''\n",
        "        for index, element in enumerate(tuple_item):\n",
        "            if index != (len(tuple_item)-1):\n",
        "                tuple_str += element + '|'\n",
        "            else:\n",
        "                tuple_str += element\n",
        "        ngrams_str.append(tuple_str)\n",
        "    return ngrams_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "aac5d703",
      "metadata": {
        "id": "aac5d703"
      },
      "outputs": [],
      "source": [
        "#add columns to trace data containing arrays for probability data\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  col_name_1 = 'ProbDict_' + str(n)\n",
        "  col_name_2 = 'ProbArray_' + str(n)\n",
        "  col_name_3 = 'NGrams_' + str(n)\n",
        "  traces_ngrams[col_name_1] = traces_ngrams.apply(lambda row: calc_probabilities(row[col_name_3], all_ngrams_list[n], False), axis = 1)\n",
        "  traces_ngrams[col_name_2] = traces_ngrams.apply(lambda row: calc_probabilities(row[col_name_3], all_ngrams_list[n], True), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ba9b0aaa",
      "metadata": {
        "id": "ba9b0aaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a06c918-802e-40fe-ac3a-875a98e57c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "#a few quick sense checks\n",
        "example_dict = traces_ngrams['ProbDict_2'].iloc[0]\n",
        "example_array = traces_ngrams['ProbArray_2'].iloc[0]\n",
        "\n",
        "#check translation functions work\n",
        "example_dict_converted_to_array = prob_dict_to_array(example_dict)\n",
        "example_array_converted_to_dict = prob_array_to_dict(example_array, all_ngrams_list[2])\n",
        "\n",
        "print(np.array_equal(example_dict_converted_to_array, example_array))\n",
        "print(example_array_converted_to_dict == example_dict)\n",
        "\n",
        "#check probability array is normalised\n",
        "print(sum(example_array))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove columns that are unnecessary for clustering algorithms and distance measure calculations\n",
        "cols = ['Player', 'GameID', 'AgentName', 'AgentNameOpponent', 'Win', 'FinalScore', 'TotalRounds']\n",
        "traces_cardcount_slim = traces_cardcount.drop(cols, axis = 1)"
      ],
      "metadata": {
        "id": "R_q5YHlOfdQy"
      },
      "id": "R_q5YHlOfdQy",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "baf6b8d0",
      "metadata": {
        "id": "baf6b8d0"
      },
      "outputs": [],
      "source": [
        "#calculate distance and affinity matrices for jenen-shannon and l_k norm distance functions\n",
        "def symm_distance_matrix(df, distance_func, func_param = False):\n",
        "    traces = df.tolist()\n",
        "    index_combinations = list(combinations(range(len(traces)), 2))\n",
        "\n",
        "    #we branch here so we can use both lk-norm and Jensen-Shannon in this function\n",
        "    if not func_param:\n",
        "      distance_values = [distance_func(traces[i],traces[j]) for i, j in index_combinations]\n",
        "    else:\n",
        "      distance_values = [distance_func(func_param, traces[i],traces[j]) for i, j in index_combinations]\n",
        "\n",
        "    num_rows = len(df)\n",
        "    distance_matrix = pd.DataFrame(index=range(num_rows), columns=range(num_rows))\n",
        "\n",
        "    for (i, j), distance_value in zip(index_combinations, distance_values):\n",
        "        distance_matrix.at[i, j] = distance_value\n",
        "        distance_matrix.at[j, i] = distance_value  # mirror the value\n",
        "\n",
        "    return distance_matrix.fillna(0)  # fill NaN values with zeros for diagonal elements\n",
        "\n",
        "#affinity function used when computing fully connected affinity matrix\n",
        "def connected_affinity_func(x, gamma):\n",
        "  return np.exp(-gamma * (x**2))\n",
        "\n",
        "#calculate fully connected affinity matrix\n",
        "def connected_affinity_matrix(dist_matrix, gamma):\n",
        "  eps = 0.00000001\n",
        "  matrix = np.vectorize(connected_affinity_func)(dist_matrix, gamma)\n",
        "  #note we put a lower bound on the entries in the infinity matrix to make sure\n",
        "  #we have a fully connected graph\n",
        "  return np.vectorize(max)(matrix, eps)\n",
        "\n",
        "#calculate k-nearest neighbour affinity matrix\n",
        "def knn_affinity_matrix(dist_matrix, k):\n",
        "  # Find indices of k-nearest neighbors for each data point\n",
        "  neigh = NearestNeighbors(n_neighbors=k + 1, metric='precomputed').fit(dist_matrix)\n",
        "  _, indices = neigh.kneighbors()\n",
        "\n",
        "  # Create knn affinity matrix\n",
        "  aff_matrix = np.zeros_like(dist_matrix, dtype=float)\n",
        "  for i in range(dist_matrix.shape[0]):\n",
        "      aff_matrix[i, indices[i, 1:]] = 1.0  # Assign 1 to the k-nearest neighbors\n",
        "\n",
        "  # Make the matrix symmetric\n",
        "  aff_matrix = 0.5 * (aff_matrix + aff_matrix.T)\n",
        "\n",
        "  return aff_matrix\n",
        "\n",
        "#function to calculate Jensen-Shannon distance\n",
        "def kl_divergence(p, q):\n",
        "  eps = 1e-10\n",
        "  return np.sum(np.where(p < eps, 0, np.where(q < eps, 0, p * np.log((p + eps) / (1.0 * q + eps)))))\n",
        "\n",
        "def jensen_shannon_distance(p, q):\n",
        "  m = 0.5 * (p + q)\n",
        "  return 0.5 * (kl_divergence(p, m) + kl_divergence(q, m))\n",
        "\n",
        "#function to compute l_k norm, here p and q are arrays of doubles\n",
        "def l_k_norm(k, p, q):\n",
        "  return np.sum(np.abs(p - q) ** k) ** (1/(1.0*k))\n",
        "\n",
        "#calculate distance, fully connected and k-nearest neighbour affinity matrices for l_k norm for card count playtraces\n",
        "dist_matrices = {}\n",
        "connected_affinity_matrices = {}\n",
        "knn_affinity_matrices = {}\n",
        "traces_cardcount_arrays = traces_cardcount_slim.apply(lambda row: np.array(row), axis=1)\n",
        "#pdb.set_trace()\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  connected_affinity_matrices[key_norm] = {}\n",
        "  knn_affinity_matrices[key_norm] = {}\n",
        "  dist_matrices[key_norm] = symm_distance_matrix(traces_cardcount_arrays, l_k_norm, k)\n",
        "  for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "    key_gamma =  'gamma_' + str(gamma)\n",
        "    connected_affinity_matrices[key_norm][key_gamma] = connected_affinity_matrix(dist_matrices[key_norm], gamma)\n",
        "  for nn in range(nearest_neighbours_min, nearest_neighbours_max, nearest_neighbours_stepsize):\n",
        "    key_nn =  'knn_' + str(nn)\n",
        "    knn_affinity_matrices[key_norm][key_nn] = knn_affinity_matrix(dist_matrices[key_norm], nn)\n",
        "\n",
        "#for dbcan we also need to normalise the playtraces to reduce the space we need to search over\n",
        "#epsilon to between zero and one. Note that the Jensen-Shannon distance measure is by defintion less than one\n",
        "#so we only need to do this for our card count playtraces\n",
        "traces_cardcount_slim_normalised = traces_cardcount_slim.apply(lambda row: row/np.linalg.norm(row), axis = 1)\n",
        "traces_cardcount_slim_normalised_arrays = traces_cardcount_slim_normalised.apply(lambda row: np.array(row), axis=1)\n",
        "dist_matrices_normalised = {}\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  dist_matrices_normalised[key_norm] = symm_distance_matrix(traces_cardcount_slim_normalised_arrays, l_k_norm, k)\n",
        "\n",
        "#calculate distance, fully connected and k-nearest neighbour affinity matrices for N-Gram playtraces\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  key_gram = 'N_Gram_' + str(n)\n",
        "  connected_affinity_matrices[key_gram] = {}\n",
        "  knn_affinity_matrices[key_gram] = {}\n",
        "  col_name = 'ProbArray_' + str(n)\n",
        "  dist_matrices[key_gram] = symm_distance_matrix(traces_ngrams[col_name], jensen_shannon_distance)\n",
        "  for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "    key_gamma =  'gamma_' + str(gamma)\n",
        "    connected_affinity_matrices[key_gram][key_gamma] = connected_affinity_matrix(dist_matrices[key_gram], gamma)\n",
        "  for nn in range(nearest_neighbours_min, nearest_neighbours_max, nearest_neighbours_stepsize):\n",
        "    key_nn =  'knn_' + str(nn)\n",
        "    knn_affinity_matrices[key_gram][key_nn] = knn_affinity_matrix(dist_matrices[key_gram], nn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#functions to perfom clustering analysis\n",
        "def sa_kmedoids(dist_matrix, num_clusters):\n",
        "  clusterer = KMedoids(n_clusters=num_clusters,\n",
        "                       metric='precomputed',\n",
        "                       method='pam',\n",
        "                       init='k-medoids++',\n",
        "                       max_iter=300,\n",
        "                       random_state= 0).fit(dist_matrix)\n",
        "  #when we use precomputed as the metric, the algorithm returns the indices for the cluster centres only\n",
        "  return clusterer.inertia_, clusterer.medoid_indices_, clusterer.labels_\n",
        "\n",
        "def sa_kmeans(data, num_clusters):\n",
        "  clusterer = KMeans(n_clusters=num_clusters,\n",
        "                      init='k-means++',\n",
        "                      n_init= 'warn',\n",
        "                      max_iter=300,\n",
        "                      tol=0.0001,\n",
        "                      verbose=0,\n",
        "                      random_state=0,\n",
        "                      copy_x=True,\n",
        "                      algorithm='lloyd').fit(data)\n",
        "  return clusterer.inertia_, clusterer.cluster_centers_, clusterer.labels_\n",
        "\n",
        "def sa_dbscan(dist_matrix, minPts, epsilon):\n",
        "  dbscan_clustering = DBSCAN(eps= epsilon, min_samples= minPts, metric = 'precomputed').fit(dist_matrix)\n",
        "  return dbscan_clustering.labels_\n",
        "\n",
        "#spectral clcustering using pre-computed affinity matrx\n",
        "def sa_spectral_clustering_AM(affinity_matrix, num_clusters):\n",
        "  spec_clustering_AM = SpectralClustering(n_clusters= num_clusters,\n",
        "                                        random_state=0,\n",
        "                                        affinity = 'precomputed',\n",
        "                                        assign_labels='kmeans').fit(affinity_matrix)\n",
        "  return spec_clustering_AM.labels_"
      ],
      "metadata": {
        "id": "Z_Jtc-amEd5Q"
      },
      "id": "Z_Jtc-amEd5Q",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#functions to generate plots and output files\n",
        "def output_silhouette_plot(outputfilename, silhouette_samples, silhouette_avg, cluster_labels):\n",
        "  n_clusters = len(np.unique(cluster_labels))\n",
        "\n",
        "  #Create a subplot with 1 row and 1 columns\n",
        "  fig, ax1 = plt.subplots(1,1, clear = True)\n",
        "  fig.set_size_inches(7, 3.5)\n",
        "\n",
        "  # The silhouette coefficient can range from -1, 1\n",
        "  ax1.set_xlim([-0.1, 1])\n",
        "  # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "  # plots of individual clusters, to demarcate them clearly.\n",
        "  ax1.set_ylim([0, len(silhouette_samples) + (n_clusters + 1) * 10])\n",
        "\n",
        "  y_lower = 10\n",
        "  for i in range(n_clusters):\n",
        "      # Aggregate the silhouette scores for samples belonging to\n",
        "      # cluster i, and sort them\n",
        "      ith_cluster_silhouette_values = silhouette_samples[cluster_labels == i]\n",
        "\n",
        "      ith_cluster_silhouette_values.sort()\n",
        "\n",
        "      size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "      y_upper = y_lower + size_cluster_i\n",
        "\n",
        "      color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "      ax1.fill_betweenx(\n",
        "          np.arange(y_lower, y_upper),\n",
        "          0,\n",
        "          ith_cluster_silhouette_values,\n",
        "          facecolor=color,\n",
        "          edgecolor=color,\n",
        "          alpha=0.7,\n",
        "      )\n",
        "\n",
        "      # Label the silhouette plots with their cluster numbers at the middle\n",
        "      ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "      # Compute the new y_lower for next plot\n",
        "      y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "  #ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "  ax1.set_xlabel(\"The silhouette coefficient values for K=\" + str(n_clusters))\n",
        "  ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "  # The vertical line for average silhouette score of all the values\n",
        "  ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "  ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "  ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "  #plt.suptitle(\n",
        "  #    \"Silhouette analysis for \" + cluster_method_str + \" clustering\",\n",
        "  #    fontsize=14,\n",
        "  #    fontweight=\"bold\",\n",
        "  #)\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "#output list of silhouette averages with a key (e.g. no of clusters)\n",
        "def output_silhouette_avgs(outputfilename, silhouette_avgs_dict):\n",
        "  #output silhouette averages to file\n",
        "  with open(outputfilename + '.csv', 'w', newline='') as csv_file:\n",
        "    fieldnames = silhouette_avgs_dict.keys()\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    # Write the header\n",
        "    writer.writeheader()\n",
        "    # Write the data\n",
        "    writer.writerow(silhouette_avgs_dict)\n",
        "\n",
        "  return None\n",
        "\n",
        "#output single best silhouette average and corresponding parameter values\n",
        "def output_best_silhouette_average_and_params(outputfilename, best_sil_avg, param_list, paramlabel_list):\n",
        "  with open(outputfilename + '.csv', 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow(['Best Silhouette Average: ', best_sil_avg])\n",
        "    for index in range(0, len(param_list)):\n",
        "      writer.writerow([paramlabel_list[index], param_list[index]])\n",
        "\n",
        "def output_inertia_plot(outputfilename, inertia_vals_dict, scalar):\n",
        "  #scale the inertia vals, typically the scalar value will be the inertia for clustering on one cluster\n",
        "  inertia_vals_array = np.array([value for value in inertia_vals_dict.values()])\n",
        "  inertia_vals_scaled = inertia_vals_array/scalar\n",
        "  cluster_list = np.array([cluster for cluster in inertia_vals_dict.keys()])\n",
        "  inertia_vals_scaled = np.insert(inertia_vals_scaled, 0, 1.0)\n",
        "  cluster_list = np.insert(cluster_list, 0, 1)\n",
        "\n",
        "  #plot as a line plot\n",
        "  fig, ax = plt.subplots(num=1,clear=True)\n",
        "  ax.plot(cluster_list, inertia_vals_scaled)\n",
        "  ax.set_xticks(cluster_list)\n",
        "  ax.set_xlabel(\"Number of clusters\")\n",
        "  ax.set_ylabel(\"Scaled inertia\")\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "#function to plot card count playtraces\n",
        "def cardcount_playtrace_comparison(outputfilename, trace_list, label_list, card_types, legendOn = True, ylimit = 0):\n",
        "  #look at evolution of number of cards of each type per round.\n",
        "  #traces should all be of the same length\n",
        "  maxRounds =   int(trace_list[0].shape[1]/17)\n",
        "  noOfCardTypes = len(card_types)\n",
        "  noOfSubPlotCols = 5\n",
        "  noOfSubPlotRows = max(2, math.floor(noOfCardTypes/noOfSubPlotCols) + 1)\n",
        "  fig, axs = plt.subplots(noOfSubPlotRows, noOfSubPlotCols, figsize = (10,10))\n",
        "  for i in range(0,noOfSubPlotRows):\n",
        "      for j in range(0,noOfSubPlotCols):\n",
        "          cardIndex = noOfSubPlotRows*j + i\n",
        "          if cardIndex >= len(card_types):\n",
        "              axs[i,j].set_visible(False)\n",
        "          else:\n",
        "              card_type = card_types[cardIndex]\n",
        "              card_col = [card_type + \"_R\" + str(r) for r in range(0,maxRounds)]\n",
        "              card_max = 0\n",
        "              for (index, trace) in enumerate(trace_list):\n",
        "                  axs[i,j].plot(range(0,maxRounds), trace[card_col].iloc[0], label = label_list[index])\n",
        "                  tmp_card_max = int(trace[card_col].iloc[0].max())\n",
        "                  if tmp_card_max > card_max:\n",
        "                      card_max = tmp_card_max\n",
        "\n",
        "              #set labels and limits\n",
        "              axs[i,j].set_title(card_type)\n",
        "              axs[i,j].set_xlabel('Round')\n",
        "              if ylimit == 0:\n",
        "                  axs[i,j].set_ylim((0,card_max+2))\n",
        "              else:\n",
        "                  axs[i,j].set_ylim((0,1))\n",
        "              #axs[i,j].set_ylim((0,card_max))\n",
        "              axs[i,j].set_xticks(ticks = range(0, maxRounds,10))\n",
        "\n",
        "          #tighten subplots layout\n",
        "          fig.tight_layout()\n",
        "\n",
        "  #add overal legend to figure\n",
        "  if legendOn:\n",
        "      axs[0,noOfSubPlotRows - 1].legend(loc = (1.2,-0.8))\n",
        "\n",
        "  #output to file\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "#plot cluster centroids based on cardcount playtraces, that are outputted directly from sklearn clustering algorithms\n",
        "def plot_cluster_centroids(outputfilename, cluster_centers, card_types, legendOn = True, ylimit = 0):\n",
        "  #cluster centres outputted from sklearn are 2D arrays with rows corresponding to clusters and columns corresponding to length of trace\n",
        "  no_of_clusters = cluster_centres.shape[0]\n",
        "  maxRounds = int(cluster_centres.shape[1]/17)\n",
        "  df_cluster_centres = pd.DataFrame(cluster_centers)\n",
        "  cols = [card_types[i] + \"_R\" + str(r) for r in range(0, maxRounds)\n",
        "          for i in range(0, len(card_types))]\n",
        "  df_cluster_centres.columns = cols\n",
        "\n",
        "  trace_list = []\n",
        "  label_list = []\n",
        "  for n in range(0, no_of_clusters):\n",
        "      trace_list.append(pd.DataFrame(df_cluster_centres.iloc[n]).transpose())\n",
        "      label_list.append(str('Cluster ') + str(n) + str(' Centroid'))\n",
        "  cardcount_playtrace_comparison(outputfilename, trace_list, label_list, card_types, legendOn, ylimit)\n",
        "\n",
        "#output a selection of cluster metrics. This includes:\n",
        "#1. Portion of traces in each cluster that have a given agent name\n",
        "#2. Portion of traces in a cluster that were from the player that moved first\n",
        "#3. Portfolio of traces in a cluster that won\n",
        "def ouput_cluster_metrics(outputfilename, traces, cluster_labels):\n",
        "  tmp_traces = traces.copy()\n",
        "  tmp_traces['Cluster'] = cluster_labels\n",
        "\n",
        "  #loop over attibutes to compute distributions\n",
        "  df_result = pd.DataFrame()\n",
        "  for att in ['AgentName', 'Player', 'Win']:\n",
        "    result = tmp_traces.groupby(['Cluster', att]).size().unstack().fillna(0)\n",
        "    results_percentage = result.div(result.sum(axis=1), axis=0) * 100\n",
        "    if att == 'AgentName':\n",
        "      df_result = results_percentage\n",
        "    else:\n",
        "      if att == 'Player':\n",
        "        results_percentage.columns = ['First Player', 'Second Player']\n",
        "      else:\n",
        "        if len(results_percentage.columns) == 2:\n",
        "          results_percentage.columns = ['Win', 'Loss']\n",
        "        else:\n",
        "          results_percentage.columns = ['Win', 'Draw', 'Loss']\n",
        "      df_result = df_result.join(results_percentage)\n",
        "\n",
        "  #output to file\n",
        "  df_result.to_csv(outputfilename + '.csv')\n",
        "\n",
        "#convert a list of ngram tuples into a list of strings, used in plotting function below\n",
        "def convert_ngram_tuples_to_strings(ngrams_list):\n",
        "  ngrams_str = []\n",
        "  for tuple_item in ngrams_list:\n",
        "      tuple_str = ''\n",
        "      for index, element in enumerate(tuple_item):\n",
        "          if index != (len(tuple_item)-1):\n",
        "              tuple_str += element + '|'\n",
        "          else:\n",
        "              tuple_str += element\n",
        "      ngrams_str.append(tuple_str)\n",
        "  return ngrams_str\n",
        "\n",
        "#function to plot N-Gram distributions side by side\n",
        "def plot_distribution_comparison(outputfilename, prob_dicts, labels, threshold = 0):\n",
        "  #find a common domain where all probability values are greater than a given threshold\n",
        "  common_ngrams = []\n",
        "  for prob_dict in prob_dicts:\n",
        "    for key, value in prob_dict.items():\n",
        "      if (value > threshold) and (key not in common_ngrams):\n",
        "          common_ngrams.append(key)\n",
        "\n",
        "  #extract probability arrays for these common n-grams\n",
        "  prob_arrays = []\n",
        "  for prob_dict in prob_dicts:\n",
        "    prob_dict_reduced = {key: prob_dict[key] for key in common_ngrams}\n",
        "    prob_arrays.append(prob_dict_to_array(prob_dict_reduced))\n",
        "\n",
        "  #next plot probability distributions\n",
        "\n",
        "  #need to convert common_ngrams into a list of strings as opposed to tuples containing strings\n",
        "  common_ngrams_str = convert_ngram_tuples_to_strings(common_ngrams)\n",
        "\n",
        "  #plot discrete probability distributions side by side\n",
        "\n",
        "  # Set the width of the bars\n",
        "  bar_width = 0.35\n",
        "\n",
        "  counter = 0\n",
        "  x_values_list = []\n",
        "  for prob_array in prob_arrays:\n",
        "    # Calculate the x-coordinates for the bars\n",
        "    if counter == 0:\n",
        "      x_values = np.arange(len(common_ngrams_str))\n",
        "    else:\n",
        "      x_values = x_values_list[counter - 1] + bar_width\n",
        "    x_values_list.append(x_values)\n",
        "\n",
        "    plt.bar(x_values, prob_array, width=bar_width, label = labels[counter])\n",
        "    counter += 1\n",
        "\n",
        "  plt.xticks(x_values_list[0] + bar_width * len(prob_arrays) / 2, common_ngrams_str)\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.ylim(threshold)\n",
        "  plt.legend()\n",
        "\n",
        "  #output to file\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "7u9WU-Qiea27"
      },
      "id": "7u9WU-Qiea27",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We switch off interactive mode for matplotlib as plots will be output to file\n",
        "plt.ioff()"
      ],
      "metadata": {
        "id": "DmLapEm9JbMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d8b13e0-3ff5-494a-8192-5d673616b58a"
      },
      "id": "DmLapEm9JbMU",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x78390c4a5570>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perfom K-means clustering. We only do this for card-count playtraces due to restriction on using Euclidean playtraces\n",
        "print(\"Perfoming K-means clustering for card count playtraces using euclidean norm...\")\n",
        "sil_avg = {}\n",
        "inertia = {}\n",
        "for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "  inertia[n_clusters], cluster_centres, cluster_labels = sa_kmeans(traces_cardcount_slim, n_clusters)\n",
        "  sil_avg[n_clusters] = silhouette_score(traces_cardcount_slim, cluster_labels)\n",
        "  sil_coeffs = silhouette_samples(traces_cardcount_slim, cluster_labels)\n",
        "\n",
        "  #output silhouette plots\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "  output_silhouette_plot(outputfilename, sil_coeffs, sil_avg[n_clusters], cluster_labels)\n",
        "\n",
        "  #output cluster centroids\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "  plot_cluster_centroids(outputfilename, cluster_centres, card_types)\n",
        "\n",
        "  #output cluster metrics\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'cluster_metrics_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "  ouput_cluster_metrics(outputfilename, traces_cardcount, cluster_labels)\n",
        "\n",
        "#output silhouette averages to file\n",
        "outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'silhouette_averages_KMeans_CardCount_' + tag_for_dir_and_filenames\n",
        "output_silhouette_avgs(outputfilename, sil_avg)\n",
        "\n",
        "#output a scaled inertia value plot\n",
        "outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'inertia_plot_KMeans_CardCount_' + tag_for_dir_and_filenames\n",
        "scalar, _, _ = sa_kmeans(traces_cardcount_slim, 1)\n",
        "output_inertia_plot(outputfilename, inertia, scalar)\n"
      ],
      "metadata": {
        "id": "Ya_QVuU6LZ0O"
      },
      "id": "Ya_QVuU6LZ0O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform K-medoids clustering for card count playtraces.\n",
        "print(\"Perfoming K-medoids clustering for card count playtraces using l_k-norm...\")\n",
        "\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  sil_avg = {}\n",
        "  inertia = {}\n",
        "  for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "    inertia[n_clusters], cluster_indices, cluster_labels = sa_kmedoids(dist_matrices[key_norm], n_clusters)\n",
        "    sil_avg[n_clusters] = silhouette_score(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "    sil_coeffs = silhouette_samples(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, sil_coeffs, sil_avg[n_clusters], cluster_labels)\n",
        "\n",
        "    #output cluster centroids, converting indices to playtraces\n",
        "    cluster_centres = traces_cardcount_slim.iloc[cluster_indices]\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    plot_cluster_centroids(outputfilename, cluster_centres, card_types)\n",
        "\n",
        "    #output cluster metrics\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'cluster_metrics_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    ouput_cluster_metrics(outputfilename, traces_cardcount, cluster_labels)\n",
        "\n",
        "  #output silhouette averages to file\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'silhouette_averages_KMedoids_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "  output_silhouette_avgs(outputfilename, sil_avg)\n",
        "\n",
        "  #output a scaled inertia value plot\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'inertia_plot_KMedoids_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "  scalar, _, _ = sa_kmedoids(dist_matrices[key_norm], 1)\n",
        "  output_inertia_plot(outputfilename, inertia, scalar)"
      ],
      "metadata": {
        "id": "KLvqSxnkLQ1B"
      },
      "id": "KLvqSxnkLQ1B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform K-medoids clustering for N-Gram playtraces.\n",
        "print(\"Perfoming K-medoids clustering for N-Gram playtraces using Jensen-Shannon norm...\")\n",
        "\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  key_gram = 'N_Gram_' + str(n)\n",
        "  sil_avg = {}\n",
        "  inertia = {}\n",
        "  for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "    inertia[n_clusters], cluster_indices, cluster_labels = sa_kmedoids(dist_matrices[key_gram], n_clusters)\n",
        "    sil_avg[n_clusters] = silhouette_score(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "    sil_coeffs = silhouette_samples(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_KMedoids_NGram_' + str(n) + '_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, sil_coeffs, sil_avg[n_clusters], cluster_labels)\n",
        "\n",
        "    #output cluster centroids (probability distributions in this case)\n",
        "    col_name = 'ProbDict_' + str(n)\n",
        "    cluster_centres = traces_ngrams[col_name].iloc[cluster_indices]\n",
        "    labels = ['Cluster ' + str(n) for n in range(0, n_clusters)]\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_KMedoids_NGram_' + str(n) + '_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    plot_distribution_comparison(outputfilename, cluster_centres, labels, thresholds[n])\n",
        "\n",
        "  #output silhouette averages to file\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'silhouette_averages_KMedoids_NGram_' + str(n) + '_' + tag_for_dir_and_filenames\n",
        "  output_silhouette_avgs(outputfilename, sil_avg)\n",
        "\n",
        "  #output a scaled inertia value plot\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'inertia_plot_KMedoids_NGram_' + str(n) + '_' + tag_for_dir_and_filenames\n",
        "  scalar, _, _ = sa_kmedoids(dist_matrices[key_gram], 1)\n",
        "  output_inertia_plot(outputfilename, inertia, scalar)\n"
      ],
      "metadata": {
        "id": "QxE9HSXDc5Ne"
      },
      "id": "QxE9HSXDc5Ne",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform DBSCAN clustering for card count playtraces using l_k-norm\n",
        "print(\"Perfoming DBSCAN clustering for card count playtraces using l_k-norm...\")\n",
        "\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  #note that for DBSCAN, inertia is not a valid metric (dependent on spherical clusters), so we look for the\n",
        "  #best silhouette average and just output this result.\n",
        "  best_sil_avg = -10000\n",
        "  best_sil_coeffs = None\n",
        "  best_minpts = 0\n",
        "  best_epsilon = 0\n",
        "  best_cluster_labels = None\n",
        "  best_noise_ratio = 0 #this is the noise ratio in the case with the highest silhouette average\n",
        "  for minpts in range(minpts_min, minpts_max, minpts_stepsize):\n",
        "    for eps in np.arange(epsilon_min, epsilon_max, epsilon_stepsize):\n",
        "      cluster_labels = sa_dbscan(dist_matrices_normalised[key_norm], minpts, eps)\n",
        "      #in DBSCAN anything with a label of '-1' is treated as noise\n",
        "      #so we need to do the following:\n",
        "      #1. Keep a record of the portion of traces that are classified as noise, too many and the results should be ignored\n",
        "      #2. Filter our traces to remove traces that are considered as noise, prior to computing the silhouette average\n",
        "      noise_ratio = np.sum(cluster_labels == -1)/len(traces_cardcount_slim_normalised)\n",
        "      cluster_labels_no_noise = cluster_labels[cluster_labels > -1]\n",
        "      indices_to_remove = [i for i, value in enumerate(cluster_labels) if value == -1]\n",
        "      dist_matrix_no_noise = dist_matrices_normalised[key_norm].drop(index=indices_to_remove, columns=indices_to_remove)\n",
        "      no_clusters_found = len(np.unique(cluster_labels_no_noise))\n",
        "      if (no_clusters_found < 2):\n",
        "        #in this case we cannot compute a silhouette score and we just output the centroids\n",
        "        #how do we determine the best choice of hyperparameters in this case?\n",
        "        i = 0 #add necessary code here\n",
        "      else:\n",
        "        sil_avg = silhouette_score(dist_matrix_no_noise, cluster_labels_no_noise, metric = 'precomputed')\n",
        "        if sil_avg > best_sil_avg:\n",
        "          best_sil_avg = sil_avg\n",
        "          best_sil_coeffs = silhouette_samples(dist_matrix_no_noise, cluster_labels_no_noise, metric = 'precomputed')\n",
        "          best_minpts = minpts\n",
        "          best_epsilon = eps\n",
        "          best_cluster_labels = cluster_labels_no_noise\n",
        "          best_noise_ratio = noise_ratio\n",
        "\n",
        "  #output silhouette plots\n",
        "  if (best_sil_avg > 0):\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_DBSCAN/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_DBSCAN_CardCount_eps_' + str(round(best_epsilon,2)) + '_minpts_' + str(best_minpts) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "    #output best silhouette average result\n",
        "    outputfilename = google_drive_parent_dir + 'Results_DBSCAN/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_DBSCAN_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    params = [best_minpts, best_epsilon, best_noise_ratio]\n",
        "    paramlabels_list = ['minpts', 'epsilon', 'noise ratio']\n",
        "    output_best_silhouette_average_and_params(outputfilename, best_sil_avg, params, paramlabels_list)\n",
        "  else:\n",
        "    print(\"No clustering found for DBSCAN for card count playtraces with l_\" + str(k) + \"-norm\")\n",
        "    #TODO: Anything else we can do here?\n",
        "\n",
        "  #TODO:Visualisation and cluster centroids\n"
      ],
      "metadata": {
        "id": "lrVCiE9W75Ok"
      },
      "id": "lrVCiE9W75Ok",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Perfoming DBSCAN clustering for N-Gram playtraces using Jensen-Shannon norm...\")\n",
        "\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  key_gram = 'N_Gram_' + str(n)\n",
        "  best_sil_avg = -10000\n",
        "  best_sil_coeffs = None\n",
        "  best_minpts = 0\n",
        "  best_epsilon = 0\n",
        "  best_cluster_labels = None\n",
        "  best_noise_ratio = 0 #this is the noise ratio in the case with the highest silhouette average\n",
        "  for minpts in range(minpts_min, minpts_max, minpts_stepsize):\n",
        "    for eps in np.arange(epsilon_min, epsilon_max, epsilon_stepsize):\n",
        "      cluster_labels = sa_dbscan(dist_matrices[key_gram], minpts, eps)\n",
        "      noise_ratio = np.sum(cluster_labels == -1)/len(traces_ngrams)\n",
        "      cluster_labels_no_noise = cluster_labels[cluster_labels > -1]\n",
        "      indices_to_remove = [i for i, value in enumerate(cluster_labels) if value == -1]\n",
        "      dist_matrix_no_noise = dist_matrices[key_gram].drop(index=indices_to_remove, columns=indices_to_remove)\n",
        "      no_clusters_found = len(np.unique(cluster_labels_no_noise))\n",
        "      if (no_clusters_found < 2):\n",
        "        #in this case we cannot compute a silhouette score and we just output the centroids\n",
        "        #how do we determine the best choice of hyperparameters in this case?\n",
        "        i = 0 #add necessary code here\n",
        "      else:\n",
        "        sil_avg = silhouette_score(dist_matrix_no_noise, cluster_labels_no_noise, metric = 'precomputed')\n",
        "        if sil_avg > best_sil_avg:\n",
        "          best_sil_avg = sil_avg\n",
        "          best_sil_coeffs = silhouette_samples(dist_matrix_no_noise, cluster_labels_no_noise, metric = 'precomputed')\n",
        "          best_minpts = minpts\n",
        "          best_epsilon = eps\n",
        "          best_cluster_labels = cluster_labels_no_noise\n",
        "          best_noise_ratio = noise_ratio\n",
        "\n",
        "  #output silhouette plots\n",
        "  if (best_sil_avg > 0):\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_DBSCAN/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_DBSCAN_NGram_N_' + str(n) + '_eps_' + str(round(best_epsilon,2)) + '_minpts_' + str(best_minpts) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "    #output best silhouette average result\n",
        "    outputfilename = google_drive_parent_dir + 'Results_DBSCAN/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_DBSCAN_NGram_N_' + str(n) + '_' + tag_for_dir_and_filenames\n",
        "    params = [best_minpts, best_epsilon, best_noise_ratio]\n",
        "    paramlabels_list = ['minpts', 'epsilon', 'noise ratio']\n",
        "    output_best_silhouette_average_and_params(outputfilename, best_sil_avg, params, paramlabels_list)\n",
        "  else:\n",
        "    print(\"No clustering found for DBSCAN for N-gram playtraces with N=\" + str(n))\n",
        "    #TODO: Anything else we can do here?\n",
        "\n",
        "  #TODO:Visualisation and cluster centroids"
      ],
      "metadata": {
        "id": "uerIDa81inOj"
      },
      "id": "uerIDa81inOj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Performing spectral clustering for card count playtraces using fully connected graphs with l_k-norm...\")\n",
        "\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  best_sil_avg = -10000\n",
        "  best_sil_coeffs = None\n",
        "  best_gamma = 0\n",
        "  best_no_clusters = 0\n",
        "  best_cluster_labels = None\n",
        "  for no_clusters in range(clusters_min, clusters_max, clusters_stepsize):\n",
        "    for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "      key_gamma =  'gamma_' + str(gamma)\n",
        "      cluster_labels = sa_spectral_clustering_AM(connected_affinity_matrices[key_norm][key_gamma], no_clusters)\n",
        "      sil_avg = silhouette_score(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "      if sil_avg > best_sil_avg:\n",
        "        best_sil_avg = sil_avg\n",
        "        best_sil_coeffs = silhouette_samples(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "        best_gamma = gamma\n",
        "        best_no_clusters = no_clusters\n",
        "        best_cluster_labels = cluster_labels\n",
        "\n",
        "  #output silhouette plots\n",
        "  if (best_sil_avg > 0):\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPCluster_AM/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_SPCluster_AM_CardCount_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "    #output best silhouette average result\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPCluster_AM/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_SPCluster_AM_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    params = [best_no_clusters, best_gamma]\n",
        "    paramlabels_list = ['Clusters', 'gamma']\n",
        "    output_best_silhouette_average_and_params(outputfilename, best_sil_avg, params, paramlabels_list)\n",
        "  else:\n",
        "    print(\"No clustering found for Spectral Clustering with fully connected affinity matrix for card count playtraces with l_\" + str(k) + \"-norm\")\n",
        "    #TODO: Anything else we can do here?\n",
        "\n",
        "  #TODO:Visualisation and cluster centroids\n",
        "\n"
      ],
      "metadata": {
        "id": "WBK7CVg0mKmg"
      },
      "id": "WBK7CVg0mKmg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Performing spectral clustering for N-Gram playtraces using fully connected graphs with Jensen-Shannon distance metric...\")\n",
        "\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  key_gram = 'N_Gram_' + str(n)\n",
        "  best_sil_avg = -10000\n",
        "  best_sil_coeffs = None\n",
        "  best_gamma = 0\n",
        "  best_no_clusters = 0\n",
        "  best_cluster_labels = None\n",
        "  for no_clusters in range(clusters_min, clusters_max, clusters_stepsize):\n",
        "    for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "      key_gamma =  'gamma_' + str(gamma)\n",
        "      cluster_labels = sa_spectral_clustering_AM(connected_affinity_matrices[key_gram][key_gamma], no_clusters)\n",
        "      sil_avg = silhouette_score(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "      if sil_avg > best_sil_avg:\n",
        "        best_sil_avg = sil_avg\n",
        "        best_sil_coeffs = silhouette_samples(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "        best_gamma = gamma\n",
        "        best_no_clusters = no_clusters\n",
        "        best_cluster_labels = cluster_labels\n",
        "\n",
        "  #output silhouette plots\n",
        "  if (best_sil_avg > 0):\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPCluster_AM/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_SPCluster_AM_NGram_' + str(n) +'_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "    #output best silhouette average result\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPCluster_AM/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_SPCluster_AM_NGram_' + str(n) + '_' + tag_for_dir_and_filenames\n",
        "    params = [best_no_clusters, best_gamma]\n",
        "    paramlabels_list = ['Clusters', 'gamma']\n",
        "    output_best_silhouette_average_and_params(outputfilename, best_sil_avg, params, paramlabels_list)\n",
        "  else:\n",
        "    print(\"No clustering found for Spectral Clustering with fully connected affinity matrix for NGram playtraces with N=\" + str(n))\n",
        "    #TODO: Anything else we can do here?\n",
        "\n",
        "  #TODO:Visualisation and cluster centroids"
      ],
      "metadata": {
        "id": "-6kOLd7d5Ts_"
      },
      "id": "-6kOLd7d5Ts_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Performing spectral clustering for card count playtraces using K-Nearest Neighbours affinity matrix with l_k-norm...\")\n",
        "\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  best_sil_avg = -10000\n",
        "  best_sil_coeffs = None\n",
        "  best_nn = 0\n",
        "  best_no_clusters = 0\n",
        "  best_cluster_labels = None\n",
        "  for no_clusters in range(clusters_min, clusters_max, clusters_stepsize):\n",
        "    for nn in np.arange(nearest_neighbours_min, nearest_neighbours_max, nearest_neighbours_stepsize):\n",
        "      key_knn =  'knn_' + str(nn)\n",
        "      cluster_labels = sa_spectral_clustering_AM(knn_affinity_matrices[key_norm][key_knn], no_clusters)\n",
        "      sil_avg = silhouette_score(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "      if sil_avg > best_sil_avg:\n",
        "        best_sil_avg = sil_avg\n",
        "        best_sil_coeffs = silhouette_samples(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "        best_nn = nn\n",
        "        best_no_clusters = no_clusters\n",
        "        best_cluster_labels = cluster_labels\n",
        "\n",
        "  #output silhouette plots\n",
        "  if (best_sil_avg > 0):\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPCluster_AM/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_SPCluster_AM_CardCount_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "    #output best silhouette average result\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPCluster_AM/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_SPCluster_AM_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    params = [best_no_clusters, best_nn]\n",
        "    paramlabels_list = ['Clusters', 'k-NearestNeighbours']\n",
        "    output_best_silhouette_average_and_params(outputfilename, best_sil_avg, params, paramlabels_list)\n",
        "  else:\n",
        "    print(\"No clustering found for Spectral Clustering with k_Nearest Neighbours affinity matrix for card count playtraces with l_\" + str(k) + \"-norm\")\n",
        "    #TODO: Anything else we can do here?\n",
        "\n",
        "  #TODO:Visualisation and cluster centroids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP4YJ48R_XtQ",
        "outputId": "acb8238c-1ef1-444c-d128-e0090af640c6"
      },
      "id": "NP4YJ48R_XtQ",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing spectral clustering for card count playtraces using K-Nearest Neighbours affinity matrix with l_k-norm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Performing spectral clustering for N-Gram playtraces using K-Nearest Neighbours affinity matrix with Jensen-Shannon...\")\n",
        "\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  key_gram = 'N_Gram_' + str(n)\n",
        "  best_sil_avg = -10000\n",
        "  best_sil_coeffs = None\n",
        "  best_nn = 0\n",
        "  best_no_clusters = 0\n",
        "  best_cluster_labels = None\n",
        "  for no_clusters in range(clusters_min, clusters_max, clusters_stepsize):\n",
        "    for nn in np.arange(nearest_neighbours_min, nearest_neighbours_max, nearest_neighbours_stepsize):\n",
        "      key_knn =  'knn_' + str(nn)\n",
        "      cluster_labels = sa_spectral_clustering_AM(knn_affinity_matrices[key_gram][key_knn], no_clusters)\n",
        "      sil_avg = silhouette_score(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "      if sil_avg > best_sil_avg:\n",
        "        best_sil_avg = sil_avg\n",
        "        best_sil_coeffs = silhouette_samples(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "        best_nn = nn\n",
        "        best_no_clusters = no_clusters\n",
        "        best_cluster_labels = cluster_labels\n",
        "\n",
        "  #output silhouette plots\n",
        "  if (best_sil_avg > 0):\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPCluster_AM/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_SPCluster_AM_NGram_' + str(n) + '_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "    #output best silhouette average result\n",
        "    outputfilename = google_drive_parent_dir + 'Results_SPCluster_AM/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_SPCluster_AM_NGram_' + str(n) +  '_' + tag_for_dir_and_filenames\n",
        "    params = [best_no_clusters, best_nn]\n",
        "    paramlabels_list = ['Clusters', 'k-NearestNeighbours']\n",
        "    output_best_silhouette_average_and_params(outputfilename, best_sil_avg, params, paramlabels_list)\n",
        "  else:\n",
        "    print(\"No clustering found for Spectral Clustering with k_Nearest Neighbours affinity matrix for N-Gram playtraces with N=\" + str(n))\n",
        "    #TODO: Anything else we can do here?\n",
        "\n",
        "  #TODO:Visualisation and cluster centroids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnjklWxeqxIy",
        "outputId": "582867f3-a384-4a6d-d485-ba933621c1d5"
      },
      "id": "xnjklWxeqxIy",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing spectral clustering for N-Gram playtraces using K-Nearest Neighbours affinity matrix with Jensen-Shannon...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LL66kzxowZki"
      },
      "id": "LL66kzxowZki",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}