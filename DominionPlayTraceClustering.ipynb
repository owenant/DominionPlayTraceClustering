{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/owenant/PlayTraces/blob/main/DominionPlayTraceClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3d63dad0",
      "metadata": {
        "id": "3d63dad0"
      },
      "outputs": [],
      "source": [
        "#This notebook calculates silhouette averages and clusters for Card Count and NGram playtraces for Dominion produced by the\n",
        "#TableTop Games framework"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn-extra"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy3-UJsWmTH0",
        "outputId": "bd0a2441-0474-4932-aa38-27735b264939"
      },
      "id": "Dy3-UJsWmTH0",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn-extra in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d792f3be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d792f3be",
        "outputId": "b576740b-1992-417e-b4cd-cdb90d04b4a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pdb\n",
        "import re\n",
        "import nltk\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product, permutations, combinations\n",
        "from nltk import ngrams\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from google.colab import drive\n",
        "import csv\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list of clustering methods to use\n",
        "clustering_methods = ['KMeans', 'KMedoids', 'DBSCAN', 'SPCluster_KNN','SPCluster_RBF','SPCluster_DM']\n",
        "\n",
        "#filenames and directory locations\n",
        "google_drive_parent_dir = \"gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/\"\n",
        "card_count_data_dir = google_drive_parent_dir + \"DataCardCount/\"\n",
        "ngram_data_dir = google_drive_parent_dir + \"DataNGrams/\"\n",
        "card_count_data_filename = card_count_data_dir + \"trace_logfile_BMWG_vs_DW_GPM100_DomFix.txt\"\n",
        "ngram_data_filename = ngram_data_dir + \"ActionsReduced_BMWG_vs_DW_GPM100_DomFix.csv\"\n",
        "tag_for_dir_and_filenames = 'BMWG_vs_DW_GPM100_DomFix'\n",
        "#card_count_data_filename = card_count_data_dir + \"trace_logfile_Budget_500_vs_Budget_500_GPM100_SD_NoSelfPlay.txt\"\n",
        "#ngram_data_filename = ngram_data_dir + \"ActionsReduced_Budget500_vs_Budget500_GPM100_SD.csv\"\n",
        "#tag_for_dir_and_filenames = 'MCTS_b500_vs_b500_GPM100_SD'\n",
        "\n",
        "#create new directory for output files\n",
        "for method in clustering_methods:\n",
        "  new_dir_path = google_drive_parent_dir + 'Results_' + method + '/' + tag_for_dir_and_filenames + '/'\n",
        "  os.makedirs(new_dir_path, exist_ok=True)\n",
        "\n",
        "  # Verify that the directory has been created\n",
        "  if os.path.exists(new_dir_path):\n",
        "      print(f\"Directory '{new_dir_path}' created successfully.\")\n",
        "  else:\n",
        "      print(f\"Failed to create directory '{new_dir_path}'.\")\n",
        "\n",
        "#also create directory for round and score distributions\n",
        "new_dir_path = google_drive_parent_dir + 'RoundAndScoreDistributions/' + tag_for_dir_and_filenames + '/'\n",
        "os.makedirs(new_dir_path, exist_ok=True)\n",
        "\n",
        "# Verify that the directory has been created\n",
        "if os.path.exists(new_dir_path):\n",
        "    print(f\"Directory '{new_dir_path}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Failed to create directory '{new_dir_path}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duv7Pp_FAnZc",
        "outputId": "a87d4582-8f37-45ed-8c82-64d37cb4c4a4"
      },
      "id": "duv7Pp_FAnZc",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_KMeans/BMWG_vs_DW_GPM100_DomFix/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_KMedoids/BMWG_vs_DW_GPM100_DomFix/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_DBSCAN/BMWG_vs_DW_GPM100_DomFix/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_SPCluster_KNN/BMWG_vs_DW_GPM100_DomFix/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_SPCluster_RBF/BMWG_vs_DW_GPM100_DomFix/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/Results_SPCluster_DM/BMWG_vs_DW_GPM100_DomFix/' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/DominionPlayTraceClustering/RoundAndScoreDistributions/BMWG_vs_DW_GPM100_DomFix/' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters for notebook execution\n",
        "\n",
        "#kingdom card set\n",
        "kingdom_set = 'SD'\n",
        "\n",
        "#parameters if using TAG input data\n",
        "logs_from_tag = True\n",
        "agent_names = ['BMWG', 'DW']\n",
        "games_per_matchup = 100\n",
        "no_self_play = True\n",
        "\n",
        "#parameters for grid search for clustering methods\n",
        "\n",
        "#number of clusters to check across all clustering methods (excluding DBSCAN)\n",
        "clusters_min = 2\n",
        "clusters_max = 5\n",
        "clusters_stepsize = 1\n",
        "\n",
        "#DBSCAN\n",
        "minpts_min = 5\n",
        "minpts_max = 50\n",
        "minpts_stepsize = 5\n",
        "epsilon_min = 0.1\n",
        "epsilon_max = 1\n",
        "epsilon_stepsize = 0.1\n",
        "\n",
        "#Spectral clustering K-Nearest Neighbours\n",
        "nearest_neighbours_min = 5\n",
        "nearest_neighbours_max = 50\n",
        "nearest_neighbours_stepsize = 5\n",
        "\n",
        "#Spectral clustering radial basis function\n",
        "gamma_min = 0.1\n",
        "gamma_max = 1\n",
        "gamma_stepsize = 0.1\n",
        "\n",
        "#number of N-gram types to search over\n",
        "ngram_min = 1\n",
        "ngram_max = 2\n",
        "ngram_stepsize = 1\n",
        "\n",
        "#values of k for l_k norm\n",
        "k_norms = [0.1, 0.5, 1, 2]\n",
        "\n",
        "#threshold values for plotting probability distributions for N-Gram playtraces\n",
        "thresholds = {n: 0.01 for n in range(ngram_min, ngram_max + 1, ngram_stepsize)}"
      ],
      "metadata": {
        "id": "lM1jnEiFsQwI"
      },
      "id": "lM1jnEiFsQwI",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kingdom card types\n",
        "card_types_SD = ['ARTISAN', 'BANDIT', 'BUREAUCRAT', 'CHAPEL', 'FESTIVAL', 'GARDENS', 'SENTRY', 'THRONE_ROOM', 'WITCH',\n",
        "                 'WORKSHOP', 'CURSE', 'PROVINCE', 'DUCHY', 'ESTATE', 'GOLD', 'SILVER', 'COPPER']\n",
        "card_types_FG1E = ['CELLAR','MARKET','MILITIA','MINE','MOAT','REMODEL','SMITHY','VILLAGE',\n",
        "                'WOODCUTTER','WORKSHOP','CURSE','PROVINCE', 'DUCHY', 'ESTATE', 'GOLD', 'SILVER', 'COPPER']\n",
        "\n",
        "if kingdom_set == 'SD':\n",
        "  card_types = card_types_SD\n",
        "elif kingdom_set == 'FG1E':\n",
        "  card_types = card_types_FG1E\n",
        "else:\n",
        "  print('Unrecognised kingdom card set')"
      ],
      "metadata": {
        "id": "m_oEYop-1Ma2"
      },
      "id": "m_oEYop-1Ma2",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#functions to process data from TAG\n",
        "\n",
        "def gameID_to_matchup(game_id, player_no, matchup_list, no_games_per_matchup, min_game_id):\n",
        "    game_group = int((game_id - min_game_id)/no_games_per_matchup)\n",
        "    matchup = matchup_list[game_group]\n",
        "    agent1, agent2 = matchup\n",
        "    if player_no == 0:\n",
        "        return agent1\n",
        "    else:\n",
        "        return agent2\n",
        "\n",
        "def add_TAG_agent_names(agent_names, games_per_match_up, no_self_play, logs_from_tag, data):\n",
        "  NoOfGames = len(data['GameID'].unique())\n",
        "  min_GameID = data['GameID'].min()\n",
        "\n",
        "  #first generate match-ups\n",
        "  matchups = []\n",
        "  if no_self_play:\n",
        "    matchups = list(permutations(agent_names, 2))\n",
        "  else:\n",
        "      for agent1 in agent_names:\n",
        "          for agent2 in agent_names:\n",
        "              matchups.append((agent1, agent2))\n",
        "\n",
        "  #add agent names to data set\n",
        "  data['AgentName'] = data.apply(lambda row: gameID_to_matchup(row['GameID'], row['Player'], matchups, games_per_matchup, min_GameID), axis = 1)\n",
        "\n",
        "  #finally we also add the name of the agent of the opponent\n",
        "  min_GameID = data['GameID'].min()\n",
        "  data['Opponent'] = data.apply(lambda row: 1.0 if row['Player'] == 0.0 else 0.0, axis = 1)\n",
        "  if logs_from_tag:\n",
        "      data['AgentNameOpponent'] = data.apply(lambda row: gameID_to_matchup(row['GameID'], row['Opponent'], matchups, games_per_matchup, min_GameID), axis = 1)\n",
        "  else:\n",
        "      gameid_to_players_dict = data.groupby('GameID')['AgentName'].apply(list).to_dict()\n",
        "      data['AgentNameOpponent'] = data.apply(lambda row: other_dict_element(gameid_to_players_dict, row['GameID'], row['AgentName']), axis = 1)\n",
        "\n",
        "\n",
        "#functions to process card count data\n",
        "def copy_final_deck_at_game_end(group, roundMax, noPlayers):\n",
        "  #This function repeatedly copies the final decks of two players at the game end, so that the game is extended to\n",
        "  #have roundMax rounds\n",
        "  final_round = int(group['Round'].max())\n",
        "  if (roundMax-1) == final_round:\n",
        "      #in this case we dont need to extend the play trace\n",
        "      return group\n",
        "  else:\n",
        "      final_row_copy = pd.concat([group.iloc[-noPlayers:]] * ((roundMax-1) - final_round), ignore_index=True)\n",
        "      #we need to update the Round counter so that every other row it increments by one\n",
        "      final_row_copy['Round'] = [final_round + 1 + i // 2 for i in range(((roundMax-1) - final_round)*2)]\n",
        "      return pd.concat([group, final_row_copy], ignore_index=True)\n",
        "\n",
        "#given a dictionary whose elements are lists of length two, grab the other element not given by elem\n",
        "def other_dict_element(my_dict, my_key, my_elem):\n",
        "    index_of_given_element = my_dict[my_key].index(my_elem)\n",
        "    index_of_other_element =  1 if (index_of_given_element == 0) else 0\n",
        "    return my_dict[my_key][index_of_other_element]\n",
        "\n",
        "def process_card_count_data(cardcount_filename, agent_names, games_per_matchup,\n",
        "                            no_self_play, card_types, logs_from_tag):\n",
        "  data  = pd.read_csv(cardcount_filename, sep = '\\t')\n",
        "  add_TAG_agent_names(agent_names, games_per_matchup, no_self_play, logs_from_tag, data)\n",
        "  index_cols = ['Player', 'GameID']\n",
        "  non_card_types_round_indep_cols = ['AgentName', 'AgentNameOpponent', 'Win', 'FinalScore', 'TotalRounds']\n",
        "  cols = index_cols + non_card_types_round_indep_cols + ['Round'] + card_types #final set of cols to keep\n",
        "  data = data[data['Turn'] == 1] #only want cards at end of round\n",
        "  data = data.loc[:, cols]\n",
        "\n",
        "  #freeze decks and copy to max round number\n",
        "  no_players = 2\n",
        "  gameLengths = data.groupby(['GameID'])['Round'].max()\n",
        "  maxNoOfRounds = int(gameLengths.max()) + 1 #round counter starts at zero\n",
        "  noOfGames = len(data['GameID'].unique())\n",
        "  data = data.groupby('GameID').apply(copy_final_deck_at_game_end, maxNoOfRounds, no_players).reset_index(drop = True)\n",
        "\n",
        "  #check shape of data\n",
        "  print(\"Card count shape check:\")\n",
        "  print(\"Expected no rows: \" + str(maxNoOfRounds*no_players*noOfGames))\n",
        "  print(\"Expected no of cols: \" + str(len(card_types)+8))\n",
        "  print(data.shape)\n",
        "\n",
        "  return data\n",
        "\n",
        "def flatten_card_count_data(cardcount_data, card_types):\n",
        "  #next we need to flatten our data so that each trace is a single row.\n",
        "  #We also drop the round label as it is redundant\n",
        "  #and it will get reintroduced when flattening through the revised column names\n",
        "\n",
        "  #first create dataframe consisting of only non card type data types that are round\n",
        "  #independent\n",
        "  index_cols = ['Player', 'GameID']\n",
        "  non_card_types_round_indep_cols = ['AgentName', 'AgentNameOpponent', 'Win', 'FinalScore', 'TotalRounds']\n",
        "  non_card_data_round_indep = cardcount_data[index_cols + non_card_types_round_indep_cols].drop_duplicates()\n",
        "\n",
        "  #next need to Group by Player and GameID and then flatten card data by round\n",
        "  traces_tmp = cardcount_data[index_cols + card_types]\n",
        "  gameLengths = cardcount_data.groupby(['GameID'])['Round'].max()\n",
        "  maxNoOfRounds = int(gameLengths.max()) + 1 #round counter starts at zero\n",
        "  cols = [card_types[i] + \"_R\" + str(r)\n",
        "          for r in range(0, maxNoOfRounds) for i in range(0, len(card_types))]\n",
        "\n",
        "  extended_traces_flat = traces_tmp.groupby(index_cols).apply(lambda df: df[card_types].values.flatten())\n",
        "  extended_traces_flat = pd.DataFrame(extended_traces_flat, columns = ['Trace']).reset_index()\n",
        "  extended_traces_flat = pd.concat([extended_traces_flat[index_cols], extended_traces_flat['Trace'].apply(pd.Series)], axis=1)\n",
        "  extended_traces_flat.columns = index_cols + cols\n",
        "\n",
        "  #next we add back in the round independent data\n",
        "  extended_traces_flat = pd.merge(non_card_data_round_indep, extended_traces_flat, on = index_cols)\n",
        "\n",
        "  return extended_traces_flat\n",
        "\n",
        "#functions to process NGram data\n",
        "def format_action(action, cardtypes):\n",
        "  #there are various types of actions we need to format to identify these we use\n",
        "  #regular expressions\n",
        "  pattern_list = []\n",
        "  pattern_list.append(re.compile(r'End Current Phase'))\n",
        "  pattern_list.append(re.compile(r'BuyCard: (' + '|'.join(cardtypes) + r') by player (0|1)'))\n",
        "  pattern_list.append(re.compile(r'(' + '|'.join(cardtypes) + r') : Player (0|1)'))\n",
        "  pattern_list.append(re.compile(r'GainCard: (' + '|'.join(cardtypes) + r') by player (0|1)'))\n",
        "  pattern_list.append(re.compile(r'Player (0|1) trashes a (' + '|'.join(cardtypes) + r') from (?:HAND|DISCARD)'))\n",
        "  pattern_list.append(re.compile(r'DoNothing'))\n",
        "  pattern_list.append(re.compile(r'Player (0|1) moves (' + '|'.join(cardtypes) + r') from HAND to DRAW of player (0|1) \\(visible: (?:true|false)\\)'))\n",
        "  pattern_list.append(re.compile(r'Reveals Hand'))\n",
        "  pattern_list.append(re.compile(r'Sentry .*$')) #captures playing a sentry and then discard/trash two cards\n",
        "  pattern_list.append(re.compile(r'Player (0|1) discards (' + '|'.join(cardtypes) + r')'))\n",
        "  pattern_list.append(re.compile(r'Player (0|1) reveals a (' + '|'.join(cardtypes) + r')'))\n",
        "\n",
        "  match_list = [None] * len(pattern_list)\n",
        "\n",
        "  pattern_to_string_map = ['ECP', 'BUY', 'PLAY', 'GAIN', 'TRASHES',\n",
        "                            'DONOTHING', 'MOVES', 'REVEALSHAND', 'PLAYSSENTRY',\n",
        "                            'DISCARDS', 'REVEALS']\n",
        "\n",
        "  for index in range(0, len(pattern_list)):\n",
        "    matched = pattern_list[index].match(action)\n",
        "    pattern_index = index\n",
        "    if matched != None:\n",
        "      break\n",
        "\n",
        "  if matched == None:\n",
        "    pdb.set_trace()\n",
        "    raise Exception(\"Can't match action description\")\n",
        "\n",
        "  if pattern_index in [0, 5, 7, 8]:\n",
        "    formatted_action =  pattern_to_string_map[pattern_index]\n",
        "  elif pattern_index in [1, 2, 3]:\n",
        "    matched_card =  matched.group(1)\n",
        "    formatted_action =  pattern_to_string_map[pattern_index]  + matched_card\n",
        "  else:\n",
        "    matched_card = matched.group(2)\n",
        "    formatted_action =  pattern_to_string_map[pattern_index]  + matched_card\n",
        "\n",
        "  return formatted_action\n",
        "\n",
        "def process_ngram_data(actions_filename, agent_names, games_per_match_up, no_self_play,\n",
        "                       card_types, ngram_min, ngram_max, ngram_stepsize, logs_from_tag):\n",
        "  data  = pd.read_csv(actions_filename)\n",
        "  data = data[['GameID', 'Player', 'Round','Turn','ActionDescription']]\n",
        "  add_TAG_agent_names(agent_names, games_per_match_up, no_self_play, logs_from_tag, data)\n",
        "  data['ProcAction'] = data.apply(lambda row: format_action(row['ActionDescription'], card_types), axis = 1)\n",
        "  data = data.groupby(['GameID', 'Player','AgentName', 'AgentNameOpponent'])['ProcAction'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "  for n in range(ngram_min, ngram_max +1, ngram_stepsize):\n",
        "    col_name = 'NGrams_' + str(n)\n",
        "    data[col_name] = data.apply(lambda row: list(ngrams(nltk.word_tokenize(row['ProcAction']),n)), axis = 1)\n",
        "\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "HxQYGYs8wAIV"
      },
      "id": "HxQYGYs8wAIV",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process data\n",
        "traces_cardcount = process_card_count_data(card_count_data_filename, agent_names, games_per_matchup, no_self_play, card_types, logs_from_tag)\n",
        "traces_ngrams = process_ngram_data(ngram_data_filename, agent_names, games_per_matchup, no_self_play,\n",
        "                       card_types, ngram_min, ngram_max, ngram_stepsize, logs_from_tag)\n",
        "print(len(traces_ngrams))"
      ],
      "metadata": {
        "id": "t3I3J5tKvfoA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf518436-b7da-450f-87b3-35c2a2d13ebf"
      },
      "id": "t3I3J5tKvfoA",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Card count shape check:\n",
            "Expected no rows: 11200\n",
            "Expected no of cols: 25\n",
            "(11200, 25)\n",
            "400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to plot score and round distributions and output to file\n",
        "def plot_score_round_distributions(outputfilename, card_count_data):\n",
        "  fig, axs = plt.subplots(2, 1)\n",
        "  grouped_data = card_count_data.groupby('GameID')\n",
        "  score_data = grouped_data['FinalScore'].unique().explode()\n",
        "  axs[0].hist(score_data, bins=np.arange(score_data.min(), score_data.max()+1))\n",
        "  axs[0].set_xlabel('Final score')\n",
        "  axs[0].set_ylabel('Number of games')\n",
        "  axs[0].set_title('Score distribution')\n",
        "  round_data = grouped_data['TotalRounds'].unique().explode()\n",
        "  axs[1].hist(round_data, bins=np.arange(round_data.min(), round_data.max()+1))\n",
        "  axs[1].set_xlabel('Number of rounds')\n",
        "  axs[1].set_ylabel('Number of games')\n",
        "  axs[1].set_title('Round distribution')\n",
        "  fig.tight_layout()\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n"
      ],
      "metadata": {
        "id": "ZguwBNUQeveK"
      },
      "id": "ZguwBNUQeveK",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove outliers based on thresholds for score and length of game\n",
        "score_threshold = 300\n",
        "round_threshold = 300\n",
        "print(\"Round and score distribution before outliers removed:\")\n",
        "outputfilename = google_drive_parent_dir + 'RoundAndScoreDistributions/' + tag_for_dir_and_filenames + '/' + 'RoundAndScoreDistribution_' + tag_for_dir_and_filenames\n",
        "plot_score_round_distributions(outputfilename, traces_cardcount)\n",
        "\n",
        "traces_cardcount = traces_cardcount[(traces_cardcount['FinalScore'] <= score_threshold)\n",
        "                                           & (traces_cardcount['TotalRounds'] <= round_threshold)]\n",
        "new_game_id_list =  traces_cardcount['GameID'].unique()\n",
        "traces_ngrams = traces_ngrams[traces_ngrams['GameID'].isin(new_game_id_list)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwExLtTpgSeO",
        "outputId": "918672b6-bb35-4b13-c106-b94d6e40730e"
      },
      "id": "FwExLtTpgSeO",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round and score distribution before outliers removed:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Round and score distribution after outliers removed:\")\n",
        "outputfilename = google_drive_parent_dir + 'RoundAndScoreDistributions/' + tag_for_dir_and_filenames + '/' + 'RoundAndScoreDistribution_no_outliers_' + tag_for_dir_and_filenames\n",
        "plot_score_round_distributions(outputfilename, traces_cardcount)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSWZqXOBksgl",
        "outputId": "b32db511-b036-430f-8ef4-ce37eefcf074"
      },
      "id": "DSWZqXOBksgl",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round and score distribution after outliers removed:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flatten card count data so that we have a playtrace per row\n",
        "traces_cardcount = flatten_card_count_data(traces_cardcount, card_types)\n",
        "print(len(traces_cardcount))\n",
        "print(len(traces_ngrams))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6axA42kyBMfb",
        "outputId": "9c186f99-9490-4480-f1db-a26a7c542b51"
      },
      "id": "6axA42kyBMfb",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400\n",
            "400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute the all ngrams list by taking the total list of all observed n-grams for this tournament\n",
        "all_ngrams_list = {}\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  col_name = 'NGrams_' + str(n)\n",
        "  all_ngrams_list[n] = []\n",
        "  for row in traces_ngrams[col_name]:\n",
        "    for gram in row:\n",
        "      if gram not in all_ngrams_list[n]:\n",
        "        all_ngrams_list[n].append(gram)\n",
        "  print(\"Total number of Ngrams for N=\" + str(n) + \":\" + str(len(all_ngrams_list[n])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTi_VsyRI6Bl",
        "outputId": "2c2fd427-db1e-4845-af0b-c5c14c4a9258"
      },
      "id": "oTi_VsyRI6Bl",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of Ngrams for N=1:9\n",
            "Total number of Ngrams for N=2:29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0b646b11",
      "metadata": {
        "id": "0b646b11"
      },
      "outputs": [],
      "source": [
        "#functions to support NGram analysis\n",
        "\n",
        "#function to compute N-gram probabilities, returns either an array with probability values\n",
        "#in the same order as ngrams in ngrams_all, or a dictionary with the n-grams as key\n",
        "#Unobserved ngrams (i.e. ngrams in ngrams_all, that are not in the trace) are assigned\n",
        "#a default probability of zero.\n",
        "def calc_probabilities(ngrams_trace, ngrams_all, convertToArray = False):\n",
        "    # Compute the frequency of ngrams in the trace\n",
        "    frequency_counter = Counter(ngrams_trace)\n",
        "\n",
        "    #calculate frequencies of all ngrams in ngrams_all that appear in the playtrace\n",
        "    event_count = {gram: frequency_counter.get(gram, 0) for gram in ngrams_all}\n",
        "\n",
        "    #normalise each entry with the number of n-grams observed for that trace, to convert\n",
        "    #counts into probabilities\n",
        "    trace_n_gram_count = sum(frequency_counter.values())\n",
        "    probs = {key: value / (1.0*trace_n_gram_count) for key, value in event_count.items()}\n",
        "\n",
        "    if convertToArray:\n",
        "        probs = np.array(list(probs.values()))\n",
        "\n",
        "    return probs\n",
        "\n",
        "#function to take a probability dictionary and create an array\n",
        "def prob_dict_to_array(prob_dict):\n",
        "    return np.array(list(prob_dict.values()))\n",
        "\n",
        "#funciton to take probability array and convert to dictionary with n-grams as keys\n",
        "#assumes ordering has been maintained\n",
        "def prob_array_to_dict(prob_array, ngrams_all):\n",
        "    prob_dict = {}\n",
        "    index = 0\n",
        "    for gram in ngrams_all:\n",
        "        prob_dict[gram] = prob_array[index]\n",
        "        index+=1\n",
        "    return prob_dict\n",
        "\n",
        "#find the common set of ngrams between two probability dictionaries, with probabilities\n",
        "#above a given threshold\n",
        "def return_common_ngrams_above_threshold(prob_dict1, prob_dict2, threshold):\n",
        "    common_ngrams = []\n",
        "    #look for entries in the first dictionary with non-zero values\n",
        "    for key, value in prob_dict1.items():\n",
        "        if value > threshold:\n",
        "            common_ngrams.append(key)\n",
        "    #repeat for the second dictionary but avoiding duplicates\n",
        "    for key, value in prob_dict2.items():\n",
        "        if (value > threshold) and (key not in common_ngrams):\n",
        "             common_ngrams.append(key)\n",
        "    return common_ngrams\n",
        "\n",
        "#convert a list of ngram tuples into a list of strings\n",
        "def convert_ngram_tuples_to_strings(ngrams_list):\n",
        "    ngrams_str = []\n",
        "    for tuple_item in ngrams_list:\n",
        "        tuple_str = ''\n",
        "        for index, element in enumerate(tuple_item):\n",
        "            if index != (len(tuple_item)-1):\n",
        "                tuple_str += element + '|'\n",
        "            else:\n",
        "                tuple_str += element\n",
        "        ngrams_str.append(tuple_str)\n",
        "    return ngrams_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "aac5d703",
      "metadata": {
        "id": "aac5d703"
      },
      "outputs": [],
      "source": [
        "#add columns to trace data containing arrays for probability data\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  col_name_1 = 'ProbDict_' + str(n)\n",
        "  col_name_2 = 'ProbArray_' + str(n)\n",
        "  col_name_3 = 'NGrams_' + str(n)\n",
        "  traces_ngrams[col_name_1] = traces_ngrams.apply(lambda row: calc_probabilities(row[col_name_3], all_ngrams_list[n], False), axis = 1)\n",
        "  traces_ngrams[col_name_2] = traces_ngrams.apply(lambda row: calc_probabilities(row[col_name_3], all_ngrams_list[n], True), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ba9b0aaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba9b0aaa",
        "outputId": "6ffeefa0-8ce5-45c5-981c-6ebcec47c41e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "#a few quick sense checks\n",
        "example_dict = traces_ngrams['ProbDict_2'].iloc[0]\n",
        "example_array = traces_ngrams['ProbArray_2'].iloc[0]\n",
        "\n",
        "#check translation functions work\n",
        "example_dict_converted_to_array = prob_dict_to_array(example_dict)\n",
        "example_array_converted_to_dict = prob_array_to_dict(example_array, all_ngrams_list[2])\n",
        "\n",
        "print(np.array_equal(example_dict_converted_to_array, example_array))\n",
        "print(example_array_converted_to_dict == example_dict)\n",
        "\n",
        "#check probability array is normalised\n",
        "print(sum(example_array))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove columns that are unnecessary for clustering algorithms and distance measure calculations\n",
        "cols = ['Player', 'GameID', 'AgentName', 'AgentNameOpponent', 'Win', 'FinalScore', 'TotalRounds']\n",
        "traces_cardcount_slim = traces_cardcount.drop(cols, axis = 1)"
      ],
      "metadata": {
        "id": "R_q5YHlOfdQy"
      },
      "id": "R_q5YHlOfdQy",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "baf6b8d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baf6b8d0",
        "outputId": "cfa89250-6cca-48ae-ea41-ddcbb0f3d451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-c7ac6d87dcf4>:31: RuntimeWarning: invalid value encountered in divide\n",
            "  return np.sum(np.where(p < eps, 0, np.where(q < eps,0, p * np.log(p / (1.0*q)))))\n",
            "<ipython-input-18-c7ac6d87dcf4>:31: RuntimeWarning: divide by zero encountered in log\n",
            "  return np.sum(np.where(p < eps, 0, np.where(q < eps,0, p * np.log(p / (1.0*q)))))\n",
            "<ipython-input-18-c7ac6d87dcf4>:31: RuntimeWarning: invalid value encountered in multiply\n",
            "  return np.sum(np.where(p < eps, 0, np.where(q < eps,0, p * np.log(p / (1.0*q)))))\n",
            "<ipython-input-18-c7ac6d87dcf4>:31: RuntimeWarning: invalid value encountered in divide\n",
            "  return np.sum(np.where(p < eps, 0, np.where(q < eps,0, p * np.log(p / (1.0*q)))))\n",
            "<ipython-input-18-c7ac6d87dcf4>:31: RuntimeWarning: divide by zero encountered in log\n",
            "  return np.sum(np.where(p < eps, 0, np.where(q < eps,0, p * np.log(p / (1.0*q)))))\n",
            "<ipython-input-18-c7ac6d87dcf4>:31: RuntimeWarning: invalid value encountered in multiply\n",
            "  return np.sum(np.where(p < eps, 0, np.where(q < eps,0, p * np.log(p / (1.0*q)))))\n"
          ]
        }
      ],
      "source": [
        "#calculate distance and affinity matrices for jenen-shannon and l_k norm distance functions\n",
        "def symm_distance_matrix(df, distance_func, func_param = False):\n",
        "    traces = df.tolist()\n",
        "    index_combinations = list(combinations(range(len(traces)), 2))\n",
        "\n",
        "    #we branch here so we can use both lk-norm and Jensen-Shannon in this function\n",
        "    if not func_param:\n",
        "      distance_values = [distance_func(traces[i],traces[j]) for i, j in index_combinations]\n",
        "    else:\n",
        "      distance_values = [distance_func(func_param, traces[i],traces[j]) for i, j in index_combinations]\n",
        "\n",
        "    num_rows = len(df)\n",
        "    distance_matrix = pd.DataFrame(index=range(num_rows), columns=range(num_rows))\n",
        "\n",
        "    for (i, j), distance_value in zip(index_combinations, distance_values):\n",
        "        distance_matrix.at[i, j] = distance_value\n",
        "        distance_matrix.at[j, i] = distance_value  # mirror the value\n",
        "\n",
        "    return distance_matrix.fillna(0)  # fill NaN values with zeros for diagonal elements\n",
        "\n",
        "#calculate an affinity matrix from a distance matrix\n",
        "def affinity_func(x, gamma):\n",
        "  return np.exp(-gamma * (x**2))\n",
        "\n",
        "def affinity_matrix(dist_matrix, gamma):\n",
        "  return np.vectorize(affinity_func)(dist_matrix, gamma)\n",
        "\n",
        "#function to calculate Jensen-Shannon distance\n",
        "def kl_divergence(p, q):\n",
        "  eps = 0.00000001\n",
        "  return np.sum(np.where(p < eps, 0, np.where(q < eps,0, p * np.log(p / (1.0*q)))))\n",
        "\n",
        "def jensen_shannon_distance(p, q):\n",
        "  m = 0.5 * (p + q)\n",
        "  return 0.5 * (kl_divergence(p, m) + kl_divergence(q, m))\n",
        "\n",
        "#function to compute l_k norm, here p and q are arrays of doubles\n",
        "def l_k_norm(k, p, q):\n",
        "  return np.sum(np.abs(p - q) ** k) ** (1/(1.0*k))\n",
        "\n",
        "#calculate distance and affinity matrices for l_k norm for card count playtraces\n",
        "dist_matrices = {}\n",
        "affinity_matrices = {}\n",
        "traces_cardcount_arrays = traces_cardcount_slim.apply(lambda row: np.array(row), axis=1)\n",
        "#pdb.set_trace()\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  affinity_matrices[key_norm] = {}\n",
        "  dist_matrices[key_norm] = symm_distance_matrix(traces_cardcount_arrays, l_k_norm, k)\n",
        "  for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "    key_gamma =  'gamma_' + str(gamma)\n",
        "    affinity_matrices[key_norm][key_gamma] = affinity_matrix(dist_matrices[key_norm], gamma)\n",
        "\n",
        "#calculate distance and affinity matrices for N-Gram playtraces\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  key_gram = 'N_Gram_' + str(n)\n",
        "  affinity_matrices[key_gram] = {}\n",
        "  col_name = 'ProbArray_' + str(n)\n",
        "  dist_matrices[key_gram] = symm_distance_matrix(traces_ngrams[col_name], jensen_shannon_distance)\n",
        "  for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "    key_gamma =  'gamma_' + str(gamma)\n",
        "    affinity_matrices[key_gram][key_gamma] = affinity_matrix(dist_matrices[key_gram], gamma)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#functions to perfom clustering analysis\n",
        "def sa_kmedoids(dist_matrix, num_clusters):\n",
        "  clusterer = KMedoids(n_clusters=num_clusters,\n",
        "                       metric='precomputed',\n",
        "                       method='pam',\n",
        "                       init='k-medoids++',\n",
        "                       max_iter=300,\n",
        "                       random_state= 0).fit(dist_matrix)\n",
        "  #when we use precomputed as the metric, the algorithm returns the indices for the cluster centres only\n",
        "  return clusterer.inertia_, clusterer.medoid_indices_, clusterer.labels_\n",
        "\n",
        "def sa_kmeans(data, num_clusters):\n",
        "  clusterer = KMeans(n_clusters=num_clusters,\n",
        "                      init='k-means++',\n",
        "                      n_init= 'warn',\n",
        "                      max_iter=300,\n",
        "                      tol=0.0001,\n",
        "                      verbose=0,\n",
        "                      random_state=0,\n",
        "                      copy_x=True,\n",
        "                      algorithm='lloyd').fit(data)\n",
        "  return clusterer.inertia_, clusterer.cluster_centers_, clusterer.labels_\n",
        "\n",
        "def sa_dbscan(data, minPts, epsilon, use_dist_matrix = False, dist_matrix = None):\n",
        "  if not use_dist_matrix:\n",
        "    dbscan_clustering = DBSCAN(eps= epsilon, min_samples= minPts, metric = 'euclidean').fit(data)\n",
        "  else:\n",
        "    dbscan_clustering = DBSCAN(eps= epsilon, min_samples= minPts, metric = 'precomputed').fit(dist_matrix)\n",
        "  return dbscan_clustering.labels_\n",
        "\n",
        "def sa_spectral_clustering_KNN(data, num_clusters, num_nearest_neighbours):\n",
        "  #here the affinity matrix is based on K-NN\n",
        "  spec_clustering_knn = SpectralClustering(n_clusters= num_clusters,\n",
        "                                            random_state=0,\n",
        "                                            affinity = 'nearest_neighbors',\n",
        "                                            n_neighbors = num_nearest_neighbours,\n",
        "                                            assign_labels='kmeans').fit(data)\n",
        "  return spec_clustering_knn.labels_\n",
        "\n",
        "def sa_spectral_clustering_RBF(data, num_clusters, gamma_val):\n",
        "  spec_clustering_rbf = SpectralClustering(n_clusters= num_clusters,\n",
        "                                        random_state=0,\n",
        "                                        gamma = gamma_val,\n",
        "                                        affinity = 'rbf',\n",
        "                                        assign_labels='kmeans').fit(data)\n",
        "  return spec_clustering_rbf.labels_\n",
        "\n",
        "def sa_spectral_clustering_DM(dist_matrix, num_clusters):\n",
        "  spec_clustering_DM = SpectralClustering(n_clusters= num_clusters,\n",
        "                                        random_state=0,\n",
        "                                        affinity = 'precomputed',\n",
        "                                        assign_labels='kmeans').fit(dist_matrix) #this is wrong! The algorithm is expecting an affinity matrix not a distance matrix\n",
        "                                        #here we can compute affinity matrices based on either K-NN or radial basis function using JS or Lk norm\n",
        "  return spec_clustering_DM.labels_"
      ],
      "metadata": {
        "id": "Z_Jtc-amEd5Q"
      },
      "id": "Z_Jtc-amEd5Q",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#functions to generate plots and output files\n",
        "def output_silhouette_plot(outputfilename, silhouette_samples, silhouette_avg, cluster_labels):\n",
        "  n_clusters = len(np.unique(cluster_labels))\n",
        "\n",
        "  #Create a subplot with 1 row and 1 columns\n",
        "  fig, ax1 = plt.subplots(1,1, clear = True)\n",
        "  fig.set_size_inches(7, 3.5)\n",
        "\n",
        "  # The silhouette coefficient can range from -1, 1\n",
        "  ax1.set_xlim([-0.1, 1])\n",
        "  # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "  # plots of individual clusters, to demarcate them clearly.\n",
        "  ax1.set_ylim([0, len(silhouette_samples) + (n_clusters + 1) * 10])\n",
        "\n",
        "  y_lower = 10\n",
        "  for i in range(n_clusters):\n",
        "      # Aggregate the silhouette scores for samples belonging to\n",
        "      # cluster i, and sort them\n",
        "      ith_cluster_silhouette_values = silhouette_samples[cluster_labels == i]\n",
        "\n",
        "      ith_cluster_silhouette_values.sort()\n",
        "\n",
        "      size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "      y_upper = y_lower + size_cluster_i\n",
        "\n",
        "      color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "      ax1.fill_betweenx(\n",
        "          np.arange(y_lower, y_upper),\n",
        "          0,\n",
        "          ith_cluster_silhouette_values,\n",
        "          facecolor=color,\n",
        "          edgecolor=color,\n",
        "          alpha=0.7,\n",
        "      )\n",
        "\n",
        "      # Label the silhouette plots with their cluster numbers at the middle\n",
        "      ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "      # Compute the new y_lower for next plot\n",
        "      y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "  #ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "  ax1.set_xlabel(\"The silhouette coefficient values for K=\" + str(n_clusters))\n",
        "  ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "  # The vertical line for average silhouette score of all the values\n",
        "  ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "  ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "  ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "  #plt.suptitle(\n",
        "  #    \"Silhouette analysis for \" + cluster_method_str + \" clustering\",\n",
        "  #    fontsize=14,\n",
        "  #    fontweight=\"bold\",\n",
        "  #)\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "#output list of silhouette averages with a key (e.g. no of clusters)\n",
        "def output_silhouette_avgs(outputfilename, silhouette_avgs_dict):\n",
        "  #output silhouette averages to file\n",
        "  with open(outputfilename + '.csv', 'w', newline='') as csv_file:\n",
        "    fieldnames = silhouette_avgs_dict.keys()\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    # Write the header\n",
        "    writer.writeheader()\n",
        "    # Write the data\n",
        "    writer.writerow(silhouette_avgs_dict)\n",
        "\n",
        "  return None\n",
        "\n",
        "#output single best silhouette average and corresponding parameter values\n",
        "def output_best_silhouette_average_and_params(outputfilename, best_sil_avg, param_list, paramlabel_list):\n",
        "  with open(outputfilename + '.csv', 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow(['Best Silhouette Average: ', best_sil_avg])\n",
        "    for index in range(0, len(param_list)):\n",
        "      writer.writerow([paramlabel_list[index], param_list[index]])\n",
        "\n",
        "def output_inertia_plot(outputfilename, inertia_vals_dict, scalar):\n",
        "  #scale the inertia vals, typically the scalar value will be the inertia for clustering on one cluster\n",
        "  inertia_vals_array = np.array([value for value in inertia_vals_dict.values()])\n",
        "  inertia_vals_scaled = inertia_vals_array/scalar\n",
        "  cluster_list = np.array([cluster for cluster in inertia_vals_dict.keys()])\n",
        "  inertia_vals_scaled = np.insert(inertia_vals_scaled, 0, 1.0)\n",
        "  cluster_list = np.insert(cluster_list, 0, 1)\n",
        "\n",
        "  #plot as a line plot\n",
        "  fig, ax = plt.subplots(num=1,clear=True)\n",
        "  ax.plot(cluster_list, inertia_vals_scaled)\n",
        "  ax.set_xticks(cluster_list)\n",
        "  ax.set_xlabel(\"Number of clusters\")\n",
        "  ax.set_ylabel(\"Scaled inertia\")\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "#function to plot card count playtraces\n",
        "def cardcount_playtrace_comparison(outputfilename, trace_list, label_list, card_types, legendOn = True, ylimit = 0):\n",
        "  #look at evolution of number of cards of each type per round.\n",
        "  #traces should all be of the same length\n",
        "  maxRounds =   int(trace_list[0].shape[1]/17)\n",
        "  noOfCardTypes = len(card_types)\n",
        "  noOfSubPlotCols = 5\n",
        "  noOfSubPlotRows = max(2, math.floor(noOfCardTypes/noOfSubPlotCols) + 1)\n",
        "  fig, axs = plt.subplots(noOfSubPlotRows, noOfSubPlotCols, figsize = (10,10))\n",
        "  for i in range(0,noOfSubPlotRows):\n",
        "      for j in range(0,noOfSubPlotCols):\n",
        "          cardIndex = noOfSubPlotRows*j + i\n",
        "          if cardIndex >= len(card_types):\n",
        "              axs[i,j].set_visible(False)\n",
        "          else:\n",
        "              card_type = card_types[cardIndex]\n",
        "              card_col = [card_type + \"_R\" + str(r) for r in range(0,maxRounds)]\n",
        "              card_max = 0\n",
        "              for (index, trace) in enumerate(trace_list):\n",
        "                  axs[i,j].plot(range(0,maxRounds), trace[card_col].iloc[0], label = label_list[index])\n",
        "                  tmp_card_max = int(trace[card_col].iloc[0].max())\n",
        "                  if tmp_card_max > card_max:\n",
        "                      card_max = tmp_card_max\n",
        "\n",
        "              #set labels and limits\n",
        "              axs[i,j].set_title(card_type)\n",
        "              axs[i,j].set_xlabel('Round')\n",
        "              if ylimit == 0:\n",
        "                  axs[i,j].set_ylim((0,card_max+2))\n",
        "              else:\n",
        "                  axs[i,j].set_ylim((0,1))\n",
        "              #axs[i,j].set_ylim((0,card_max))\n",
        "              axs[i,j].set_xticks(ticks = range(0, maxRounds,10))\n",
        "\n",
        "          #tighten subplots layout\n",
        "          fig.tight_layout()\n",
        "\n",
        "  #add overal legend to figure\n",
        "  if legendOn:\n",
        "      axs[0,noOfSubPlotRows - 1].legend(loc = (1.2,-0.8))\n",
        "\n",
        "  #output to file\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "#plot cluster centroids based on cardcount playtraces, that are outputted directly from sklearn clustering algorithms\n",
        "def plot_cluster_centroids(outputfilename, cluster_centers, card_types, legendOn = True, ylimit = 0):\n",
        "  #cluster centres outputted from sklearn are 2D arrays with rows corresponding to clusters and columns corresponding to length of trace\n",
        "  no_of_clusters = cluster_centres.shape[0]\n",
        "  maxRounds = int(cluster_centres.shape[1]/17)\n",
        "  df_cluster_centres = pd.DataFrame(cluster_centers)\n",
        "  cols = [card_types[i] + \"_R\" + str(r) for r in range(0, maxRounds)\n",
        "          for i in range(0, len(card_types))]\n",
        "  df_cluster_centres.columns = cols\n",
        "\n",
        "  trace_list = []\n",
        "  label_list = []\n",
        "  for n in range(0, no_of_clusters):\n",
        "      trace_list.append(pd.DataFrame(df_cluster_centres.iloc[n]).transpose())\n",
        "      label_list.append(str('Cluster ') + str(n) + str(' Centroid'))\n",
        "  cardcount_playtrace_comparison(outputfilename, trace_list, label_list, card_types, legendOn, ylimit)\n",
        "\n",
        "#output a selection of cluster metrics. This includes:\n",
        "#1. Portion of traces in each cluster that have a given agent name\n",
        "#2. Portion of traces in a cluster that were from the player that moved first\n",
        "#3. Portfolio of traces in a cluster that won\n",
        "def ouput_cluster_metrics(outputfilename, traces, cluster_labels):\n",
        "  tmp_traces = traces.copy()\n",
        "  tmp_traces['Cluster'] = cluster_labels\n",
        "\n",
        "  #loop over attibutes to compute distributions\n",
        "  df_result = pd.DataFrame()\n",
        "  for att in ['AgentName', 'Player', 'Win']:\n",
        "    result = tmp_traces.groupby(['Cluster', att]).size().unstack().fillna(0)\n",
        "    results_percentage = result.div(result.sum(axis=1), axis=0) * 100\n",
        "    if att == 'AgentName':\n",
        "      df_result = results_percentage\n",
        "    else:\n",
        "      if att == 'Player':\n",
        "        results_percentage.columns = ['First Player', 'Second Player']\n",
        "      else:\n",
        "        results_percentage.columns = ['Win', 'Loss']\n",
        "      df_result = df_result.join(results_percentage)\n",
        "\n",
        "  #output to file\n",
        "  df_result.to_csv(outputfilename + '.csv')\n",
        "\n",
        "#convert a list of ngram tuples into a list of strings, used in plotting function below\n",
        "def convert_ngram_tuples_to_strings(ngrams_list):\n",
        "  ngrams_str = []\n",
        "  for tuple_item in ngrams_list:\n",
        "      tuple_str = ''\n",
        "      for index, element in enumerate(tuple_item):\n",
        "          if index != (len(tuple_item)-1):\n",
        "              tuple_str += element + '|'\n",
        "          else:\n",
        "              tuple_str += element\n",
        "      ngrams_str.append(tuple_str)\n",
        "  return ngrams_str\n",
        "\n",
        "#function to plot N-Gram distributions side by side\n",
        "def plot_distribution_comparison(outputfilename, prob_dicts, labels, threshold = 0):\n",
        "  #find a common domain where all probability values are greater than a given threshold\n",
        "  common_ngrams = []\n",
        "  for prob_dict in prob_dicts:\n",
        "    for key, value in prob_dict.items():\n",
        "      if (value > threshold) and (key not in common_ngrams):\n",
        "          common_ngrams.append(key)\n",
        "\n",
        "  #extract probability arrays for these common n-grams\n",
        "  prob_arrays = []\n",
        "  for prob_dict in prob_dicts:\n",
        "    prob_dict_reduced = {key: prob_dict[key] for key in common_ngrams}\n",
        "    prob_arrays.append(prob_dict_to_array(prob_dict_reduced))\n",
        "\n",
        "  #next plot probability distributions\n",
        "\n",
        "  #need to convert common_ngrams into a list of strings as opposed to tuples containing strings\n",
        "  common_ngrams_str = convert_ngram_tuples_to_strings(common_ngrams)\n",
        "\n",
        "  #plot discrete probability distributions side by side\n",
        "\n",
        "  # Set the width of the bars\n",
        "  bar_width = 0.35\n",
        "\n",
        "  counter = 0\n",
        "  x_values_list = []\n",
        "  for prob_array in prob_arrays:\n",
        "    # Calculate the x-coordinates for the bars\n",
        "    if counter == 0:\n",
        "      x_values = np.arange(len(common_ngrams_str))\n",
        "    else:\n",
        "      x_values = x_values_list[counter - 1] + bar_width\n",
        "    x_values_list.append(x_values)\n",
        "\n",
        "    plt.bar(x_values, prob_array, width=bar_width, label = labels[counter])\n",
        "    counter += 1\n",
        "\n",
        "  plt.xticks(x_values_list[0] + bar_width * len(prob_arrays) / 2, common_ngrams_str)\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.ylim(threshold)\n",
        "  plt.legend()\n",
        "\n",
        "  #output to file\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "7u9WU-Qiea27"
      },
      "id": "7u9WU-Qiea27",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We switch off interactive mode for matplotlib as plots will be output to file\n",
        "plt.ioff()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmLapEm9JbMU",
        "outputId": "aff77852-d8e8-4343-8abc-abec4a8e6735"
      },
      "id": "DmLapEm9JbMU",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7d3d56653e80>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perfom K-means clustering. We only do this for card-count playtraces due to restriction on using Euclidean playtraces\n",
        "print(\"Perfoming K-means clustering for card count playtraces using euclidean norm...\")\n",
        "sil_avg = {}\n",
        "inertia = {}\n",
        "for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "  inertia[n_clusters], cluster_centres, cluster_labels = sa_kmeans(traces_cardcount_slim, n_clusters)\n",
        "  sil_avg[n_clusters] = silhouette_score(traces_cardcount_slim, cluster_labels)\n",
        "  sil_coeffs = silhouette_samples(traces_cardcount_slim, cluster_labels)\n",
        "\n",
        "  #output silhouette plots\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "  output_silhouette_plot(outputfilename, sil_coeffs, sil_avg[n_clusters], cluster_labels)\n",
        "\n",
        "  #output cluster centroids\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "  plot_cluster_centroids(outputfilename, cluster_centres, card_types)\n",
        "\n",
        "  #output cluster metrics\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'cluster_metrics_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "  ouput_cluster_metrics(outputfilename, traces_cardcount, cluster_labels)\n",
        "\n",
        "#output silhouette averages to file\n",
        "outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'silhouette_averages_KMeans_CardCount_' + tag_for_dir_and_filenames\n",
        "output_silhouette_avgs(outputfilename, sil_avg)\n",
        "\n",
        "#output a scaled inertia value plot\n",
        "outputfilename = google_drive_parent_dir + 'Results_KMeans/' + tag_for_dir_and_filenames + '/' + 'inertia_plot_KMeans_CardCount_' + tag_for_dir_and_filenames\n",
        "scalar, _, _ = sa_kmeans(traces_cardcount_slim, 1)\n",
        "output_inertia_plot(outputfilename, inertia, scalar)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya_QVuU6LZ0O",
        "outputId": "c7e34c20-975c-485e-aec9-58c1395a0dec"
      },
      "id": "Ya_QVuU6LZ0O",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming K-means clustering for card count playtraces using euclidean norm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perform K-medoids clustering for card count playtraces.\n",
        "print(\"Perfoming K-medoids clustering for card count playtraces using l_k-norm...\")\n",
        "\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  sil_avg = {}\n",
        "  inertia = {}\n",
        "  for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "    inertia[n_clusters], cluster_indices, cluster_labels = sa_kmedoids(dist_matrices[key_norm], n_clusters)\n",
        "    sil_avg[n_clusters] = silhouette_score(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "    sil_coeffs = silhouette_samples(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, sil_coeffs, sil_avg[n_clusters], cluster_labels)\n",
        "\n",
        "    #output cluster centroids, converting indices to playtraces\n",
        "    cluster_centres = traces_cardcount_slim.iloc[cluster_indices]\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    plot_cluster_centroids(outputfilename, cluster_centres, card_types)\n",
        "\n",
        "    #output cluster metrics\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'cluster_metrics_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    ouput_cluster_metrics(outputfilename, traces_cardcount, cluster_labels)\n",
        "\n",
        "  #output silhouette averages to file\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'silhouette_averages_KMedoids_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "  output_silhouette_avgs(outputfilename, sil_avg)\n",
        "\n",
        "  #output a scaled inertia value plot\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'inertia_plot_KMedoids_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "  scalar, _, _ = sa_kmedoids(dist_matrices[key_norm], 1)\n",
        "  output_inertia_plot(outputfilename, inertia, scalar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLvqSxnkLQ1B",
        "outputId": "466b82f5-cf1e-4b7d-9793-8b9b1c82fac3"
      },
      "id": "KLvqSxnkLQ1B",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming K-medoids clustering for card count playtraces using l_k-norm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn_extra/cluster/_k_medoids.py:252: UserWarning: n_clusters should be larger than 2 if max_iter != 0 setting max_iter to 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn_extra/cluster/_k_medoids.py:252: UserWarning: n_clusters should be larger than 2 if max_iter != 0 setting max_iter to 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn_extra/cluster/_k_medoids.py:252: UserWarning: n_clusters should be larger than 2 if max_iter != 0 setting max_iter to 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn_extra/cluster/_k_medoids.py:252: UserWarning: n_clusters should be larger than 2 if max_iter != 0 setting max_iter to 0.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perform K-medoids clustering for N-Gram playtraces.\n",
        "print(\"Perfoming K-medoids clustering for N-Gram playtraces using Jensen-Shannon norm...\")\n",
        "\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  key_gram = 'N_Gram_' + str(n)\n",
        "  sil_avg = {}\n",
        "  inertia = {}\n",
        "  for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "    inertia[n_clusters], cluster_indices, cluster_labels = sa_kmedoids(dist_matrices[key_gram], n_clusters)\n",
        "    sil_avg[n_clusters] = silhouette_score(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "    sil_coeffs = silhouette_samples(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_KMedoids_NGram_' + str(n) + '_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, sil_coeffs, sil_avg[n_clusters], cluster_labels)\n",
        "\n",
        "    #output cluster centroids (probability distributions in this case)\n",
        "    col_name = 'ProbDict_' + str(n)\n",
        "    cluster_centres = traces_ngrams[col_name].iloc[cluster_indices]\n",
        "    labels = ['Cluster ' + str(n) for n in range(0, n_clusters)]\n",
        "    outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'cluster_centroids_KMedoids_NGram_' + str(n) + '_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    plot_distribution_comparison(outputfilename, cluster_centres, labels, thresholds[n])\n",
        "\n",
        "  #output silhouette averages to file\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'silhouette_averages_KMedoids_NGram_' + str(n) + '_' + tag_for_dir_and_filenames\n",
        "  output_silhouette_avgs(outputfilename, sil_avg)\n",
        "\n",
        "  #output a scaled inertia value plot\n",
        "  outputfilename = google_drive_parent_dir + 'Results_KMedoids/' + tag_for_dir_and_filenames + '/' + 'inertia_plot_KMedoids_NGram_' + str(n) + '_' + tag_for_dir_and_filenames\n",
        "  scalar, _, _ = sa_kmedoids(dist_matrices[key_gram], 1)\n",
        "  output_inertia_plot(outputfilename, inertia, scalar)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxE9HSXDc5Ne",
        "outputId": "57ea7bd6-0100-4883-ce44-a358c5fe0526"
      },
      "id": "QxE9HSXDc5Ne",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming K-medoids clustering for N-Gram playtraces using Jensen-Shannon norm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn_extra/cluster/_k_medoids.py:252: UserWarning: n_clusters should be larger than 2 if max_iter != 0 setting max_iter to 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn_extra/cluster/_k_medoids.py:252: UserWarning: n_clusters should be larger than 2 if max_iter != 0 setting max_iter to 0.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perform DBSCAN clustering for card count playtraces using l_k-norm\n",
        "print(\"Perfoming DBSCAN clustering for card count playtraces using l_k-norm...\")"
      ],
      "metadata": {
        "id": "lrVCiE9W75Ok"
      },
      "id": "lrVCiE9W75Ok",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#next we perfom our main loop, looping over clustering methods and types of play trace and outputting results to file\n",
        "clustering_methods = ['KMeans', 'KMedoids', 'DBSCAN', 'SPCluster_KNN','SPCluster_RBF', 'SPCluster_DM']\n",
        "\n",
        "\n",
        "\n",
        "  #loop over clustering methods\n",
        "  for cluster_method in clustering_methods:\n",
        "    print(\"Perfoming \" + str(cluster_method) + \" clustering, with playtrace type \" + str(playtrace_type))\n",
        "    #perfom Kmeans and KMedoids clustering\n",
        "\n",
        "\n",
        "    elif cluster_method == 'DBSCAN':\n",
        "      #note for DBSCAN inertia is not a valid metric (dependent on spherical clusters), so we look for the\n",
        "      #best silhouette average and just output this result.\n",
        "      clusters_greater_than_one_found = False\n",
        "      best_sil_avg = -10000\n",
        "      best_sil_coeffs = None\n",
        "      best_minpts = 0\n",
        "      best_epsilon = 0\n",
        "      best_cluster_labels = None\n",
        "      best_noise_ratio = 0 #this is the noise ratio in the case with the highest silhouette average\n",
        "      #note for DBSCAN our distances must be less than or equal to one. The JS distance has\n",
        "      #an upper bound of one, so no issues for NGram case. For CardCount case we need to\n",
        "      #normalise our playtrace vectors\n",
        "      if playtrace_type == 'CardCount':\n",
        "        #normalise trace vector\n",
        "        traces_normalised = traces.apply(lambda row: row/np.linalg.norm(row), axis = 1)\n",
        "      for minpts in range(minpts_min, minpts_max, minpts_stepsize):\n",
        "        for epsilon in np.arange(epsilon_min, epsilon_max, epsilon_stepsize):\n",
        "          if playtrace_type == 'CardCount':\n",
        "            cluster_labels = sa_dbscan(traces_normalised, minpts, epsilon)\n",
        "            #in DBSCAN anything with a label of '-1' is treated as noise\n",
        "            #so we need to do the following:\n",
        "            #1. Keep a record of the portion of traces that are classified as noise, too many and the results should be ignored\n",
        "            #2. Filter our traces to remove traces that are considered as noise, prior to computing the silhouette average\n",
        "            noise_ratio = np.sum(cluster_labels == -1)/len(traces_normalised)\n",
        "            traces_filtered = traces_normalised[cluster_labels > -1]\n",
        "            cluster_labels_filtered = cluster_labels[cluster_labels > -1]\n",
        "            #silhouette scores only make sense for two or more clusters\n",
        "            if (len(np.unique(cluster_labels_filtered))) > 1: #for DBSCAN it still could make sense to look at one cluster, ignoring noise\n",
        "              clusters_greater_than_one_found = True\n",
        "              sil_avg = silhouette_score(traces_filtered, cluster_labels_filtered)\n",
        "              if sil_avg > best_sil_avg:\n",
        "                best_sil_avg = sil_avg\n",
        "                best_sil_coeffs = silhouette_samples(traces_filtered, cluster_labels_filtered)\n",
        "                best_minpts = minpts\n",
        "                best_epsilon = epsilon\n",
        "                best_cluster_labels = cluster_labels_filtered\n",
        "                best_noise_ratio = noise_ratio\n",
        "          elif playtrace_type != 'CardCount':\n",
        "            cluster_labels = sa_dbscan(traces, minpts, epsilon, True, js_dist_matrix[n_gram_type])\n",
        "            noise_ratio = np.sum(cluster_labels == -1)/len(traces)\n",
        "            indices_to_remove = [i for i, value in enumerate(cluster_labels) if value == -1]\n",
        "            dist_matrix_filtered = js_dist_matrix[n_gram_type].drop(index=indices_to_remove, columns=indices_to_remove)\n",
        "            cluster_labels_filtered = cluster_labels[cluster_labels > -1]\n",
        "            if (len(np.unique(cluster_labels_filtered))) > 1:\n",
        "              clusters_greater_than_one_found = True\n",
        "              sil_avg = silhouette_score(dist_matrix_filtered, cluster_labels_filtered, metric = 'precomputed')\n",
        "              if sil_avg > best_sil_avg:\n",
        "                best_sil_avg = sil_avg\n",
        "                best_sil_coeffs = silhouette_samples(dist_matrix_filtered, cluster_labels_filtered, metric = 'precomputed')\n",
        "                best_minpts = minpts\n",
        "                best_epsilon = epsilon\n",
        "                best_cluster_labels = cluster_labels_filtered\n",
        "                best_noise_ratio = noise_ratio\n",
        "\n",
        "      #output silhouette plots\n",
        "      if (clusters_greater_than_one_found == True):\n",
        "        #pdb.set_trace()\n",
        "        outputfilename = google_drive_parent_dir + 'Results_' + cluster_method + '/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_' + cluster_method + '_' + playtrace_type + '_eps_' + str(round(best_epsilon,2)) + '_minpts_' + str(best_minpts) + '_' + tag_for_dir_and_filenames\n",
        "        output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "        #output best silhouette average result\n",
        "        outputfilename = google_drive_parent_dir + 'Results_' + cluster_method + '/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_' + cluster_method + '_' + playtrace_type + '_' + tag_for_dir_and_filenames\n",
        "        params = [best_minpts, best_epsilon, best_noise_ratio]\n",
        "        paramlabels_list = ['minpts', 'epsilon', 'noise ratio']\n",
        "        output_best_silhouette_average_and_params(outputfilename, sil_avg, params, paramlabels_list)\n",
        "      else:\n",
        "        print(\"No clustering found for DBSCAN for \" + playtrace_type)\n",
        "\n",
        "    elif cluster_method == 'SPCluster_KNN':\n",
        "      #this is only applicable for Euclidean distances. Note inertia is not applicable in this case\n",
        "      best_sil_avg = 0\n",
        "      best_sil_coeffs = 0\n",
        "      best_cluster_labels = None\n",
        "      best_n_clusters = 0\n",
        "      best_knn = 0\n",
        "      if playtrace_type == 'CardCount':\n",
        "        for knn in range(nearest_neighbours_min, nearest_neighbours_max, nearest_neighbours_stepsize):\n",
        "          for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "            cluster_labels = sa_spectral_clustering_KNN(traces, n_clusters, knn)\n",
        "            sil_avg = silhouette_score(traces, cluster_labels)\n",
        "            if sil_avg > best_sil_avg:\n",
        "              best_sil_avg = sil_avg\n",
        "              best_sil_coeffs = silhouette_samples(traces, cluster_labels)\n",
        "              best_n_clusters = n_clusters\n",
        "              best_knn = knn\n",
        "              best_cluster_labels = cluster_labels\n",
        "\n",
        "        #output silhouette plot\n",
        "        outputfilename = google_drive_parent_dir + 'Results_' + cluster_method + '/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_' + cluster_method + '_' + playtrace_type + '_K_' + str(best_n_clusters) + '_knn_' + str(best_knn) + '_' + tag_for_dir_and_filenames\n",
        "        output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "        #output silhouette averages to file\n",
        "        outputfilename = google_drive_parent_dir + 'Results_' + cluster_method + '/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_' + cluster_method + '_' + playtrace_type + '_' + tag_for_dir_and_filenames\n",
        "        params = [best_n_clusters, best_knn]\n",
        "        paramlabels_list = ['n_clusters', 'knn']\n",
        "        output_best_silhouette_average_and_params(outputfilename, best_sil_avg, params, paramlabels_list)\n",
        "\n",
        "    elif cluster_method == 'SPCluster_RBF':\n",
        "      #this is only applicable for Euclidean distances, and we aprroach in same way as for KNN\n",
        "      clusters_greater_than_one_found = False\n",
        "      best_sil_avg = 0\n",
        "      best_sil_coeffs = 0\n",
        "      best_cluster_labels = None\n",
        "      best_n_clusters = 0\n",
        "      best_gamma = 0\n",
        "      if playtrace_type == 'CardCount':\n",
        "        for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "          for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "            cluster_labels = sa_spectral_clustering_RBF(traces, n_clusters, gamma)\n",
        "            #for some reason RBF can return less clsuters than n_clusters. Dont undertand this....\n",
        "            if (len(np.unique(cluster_labels_filtered))) > 1:\n",
        "              clusters_greater_than_one_found = True\n",
        "              sil_avg = silhouette_score(traces, cluster_labels)\n",
        "              if sil_avg > best_sil_avg:\n",
        "                best_sil_avg = sil_avg\n",
        "                best_sil_coeffs = silhouette_samples(traces, cluster_labels)\n",
        "                best_n_clusters = n_clusters\n",
        "                best_gamma = gamma\n",
        "                best_cluster_labels = cluster_labels\n",
        "\n",
        "        #output silhouette plot\n",
        "        if clusters_greater_than_one_found:\n",
        "          outputfilename = google_drive_parent_dir + 'Results_' + cluster_method + '/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_' + cluster_method + '_' + playtrace_type + '_K_' + str(best_n_clusters) + '_gamma_' + str(best_gamma) + '_' + tag_for_dir_and_filenames\n",
        "          output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "          #output silhouette averages to file\n",
        "          outputfilename = google_drive_parent_dir + 'Results_' + cluster_method + '/' + tag_for_dir_and_filenames + '/' + 'best_silhouette_avg_' + cluster_method + '_' + playtrace_type + '_' + tag_for_dir_and_filenames\n",
        "          params = [best_n_clusters, best_gamma]\n",
        "          paramlabels_list = ['n_clusters', 'gamma']\n",
        "          output_best_silhouette_average_and_params(outputfilename, best_sil_avg, params, paramlabels_list)\n",
        "        else:\n",
        "          print(\"No clusters found for RBF\")\n",
        "    elif cluster_method == 'SPCluster_DM':\n",
        "      #this method using a pre-computed distance matrix and can be used with NGrams. In this case we just output the SA for each cluster value\n",
        "      #and also giev the corresponding silhouette plot\n",
        "      if playtrace_type != 'CardCount':\n",
        "        sil_avgs = {}\n",
        "        for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "          cluster_labels = sa_spectral_clustering_DM(js_dist_matrix[n_gram_type], n_clusters)\n",
        "          sil_avgs[n_clusters] = silhouette_score(js_dist_matrix[n_gram_type], cluster_labels, metric = 'precomputed')\n",
        "          sil_coeffs = silhouette_samples(js_dist_matrix[n_gram_type], cluster_labels, metric = 'precomputed')\n",
        "\n",
        "          #output silhouette plot\n",
        "          outputfilename = google_drive_parent_dir + 'Results_' + cluster_method + '/' + tag_for_dir_and_filenames + '/' + 'silhouette_plot_' + cluster_method + '_' + playtrace_type + '_K_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "          output_silhouette_plot(outputfilename, sil_coeffs, sil_avgs[n_clusters], cluster_labels)\n",
        "\n",
        "        #output silhouette averages to file\n",
        "        outputfilename = google_drive_parent_dir + 'Results_' + cluster_method + '/' + tag_for_dir_and_filenames + '/' + 'silhouette_avgs_' + cluster_method + '_' + playtrace_type + '_' + tag_for_dir_and_filenames\n",
        "        output_silhouette_avgs(outputfilename, sil_avgs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "57kjTd5QErUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b6e1a4e-2079-4e47-a76e-0e480a973644"
      },
      "id": "57kjTd5QErUn",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming KMeans clustering, with playtrace type CardCount\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming KMedoids clustering, with playtrace type CardCount\n",
            "Perfoming DBSCAN clustering, with playtrace type CardCount\n",
            "Perfoming SPCluster_KNN clustering, with playtrace type CardCount\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming SPCluster_RBF clustering, with playtrace type CardCount\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 2000 with accuracies \n",
            "[3.23107659e-15 1.49355116e-05 4.45057306e-05]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 1984 instead with accuracy \n",
            "1.8658391614654665e-05.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[3.25870518e-15 1.32782667e-05 4.26990625e-05]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 2000 with accuracies \n",
            "[8.28462458e-15 9.87764050e-07 2.36201608e-05 1.14101124e-05]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 1652 instead with accuracy \n",
            "3.2607905438093153e-06.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[4.13781085e-15 1.08153482e-06 2.45542049e-06 9.50619707e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 1292 with accuracies \n",
            "[1.03141233e-14 5.15167746e-07 5.60678473e-06 5.02190015e-06\n",
            " 1.60329612e-05]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 1292 instead with accuracy \n",
            "5.435362768265489e-06.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[2.68374354e-14 5.15178258e-07 5.60678647e-06 5.02192332e-06\n",
            " 1.60329531e-05]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 1075 with accuracies \n",
            "[4.54115508e-15 6.63066499e-06 5.64410246e-06 2.97248806e-06\n",
            " 3.02895127e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 1075 instead with accuracy \n",
            "3.655241355909e-06.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[3.68145551e-15 6.63119904e-06 5.64347648e-06 2.97248750e-06\n",
            " 3.02894904e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 2000 with accuracies \n",
            "[6.14122570e-15 2.70972813e-06 5.54274227e-06 1.84391541e-06\n",
            " 1.68120561e-05]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 1623 instead with accuracy \n",
            "3.1690712441665052e-06.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[3.28707853e-15 2.99652201e-06 2.27374377e-06 2.02548054e-06\n",
            " 8.55227288e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 1122 with accuracies \n",
            "[1.19678985e-15 6.75054024e-06 5.65319899e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 1018 instead with accuracy \n",
            "4.043764038617545e-06.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[2.48261468e-15 3.67691843e-06 8.45437353e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[2.83996900e-15 1.89272195e-06 2.80406961e-06 6.88923151e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 979 with accuracies \n",
            "[8.19100316e-16 7.43834342e-06 5.72068468e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 783 instead with accuracy \n",
            "4.066015994316203e-06.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[1.46722359e-15 3.20021113e-06 8.99783687e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 1000 with accuracies \n",
            "[2.01529981e-15 2.04284071e-06 1.09054572e-05 5.63109573e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 880 instead with accuracy \n",
            "3.6982173199629395e-06.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[3.03711607e-15 1.85349833e-06 5.48151751e-06 7.45785338e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 1151 with accuracies \n",
            "[2.54698651e-15 2.09713787e-06 3.34633921e-06 3.00979477e-06\n",
            " 7.18329863e-06 5.84763086e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 1025 instead with accuracy \n",
            "3.4581321573175887e-06.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[2.64816749e-15 1.45221046e-06 3.50207062e-06 3.41581982e-06\n",
            " 5.21319854e-06 7.16549364e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 676 with accuracies \n",
            "[1.12172613e-15 2.59843265e-05 5.61790169e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 442 instead with accuracy \n",
            "5.733938468685906e-06.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[1.67216206e-15 4.32789793e-06 1.28739175e-05]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 894 with accuracies \n",
            "[9.57954413e-16 1.90460986e-06 7.23023452e-06 1.19316272e-05\n",
            " 5.91247984e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 838 instead with accuracy \n",
            "4.28899770416961e-06.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[1.12213304e-15 1.54817101e-06 8.62470346e-06 3.32692723e-06\n",
            " 7.94518678e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 1893 with accuracies \n",
            "[1.92443477e-15 7.43927971e-07 2.53098349e-06 4.44985204e-06\n",
            " 1.07161507e-05 5.82756996e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 1832 instead with accuracy \n",
            "3.3974830945239335e-06.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[1.51736853e-15 7.41919693e-07 2.54676263e-06 4.94696338e-06\n",
            " 5.25212797e-06 6.89667793e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No clusters found for RBF\n",
            "Perfoming SPCluster_DM clustering, with playtrace type CardCount\n",
            "Perfoming KMeans clustering, with playtrace type NGram_1\n",
            "Perfoming KMedoids clustering, with playtrace type NGram_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn_extra/cluster/_k_medoids.py:252: UserWarning: n_clusters should be larger than 2 if max_iter != 0 setting max_iter to 0.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming DBSCAN clustering, with playtrace type NGram_1\n",
            "No clustering found for DBSCAN for NGram_1\n",
            "Perfoming SPCluster_KNN clustering, with playtrace type NGram_1\n",
            "Perfoming SPCluster_RBF clustering, with playtrace type NGram_1\n",
            "Perfoming SPCluster_DM clustering, with playtrace type NGram_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 28 with accuracies \n",
            "[6.97266713e-16 3.14080931e-06 6.47454964e-06 3.96684967e-06\n",
            " 9.02376635e-06 4.55939720e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 28 instead with accuracy \n",
            "4.5275620289519375e-06.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[7.04767678e-16 3.14080916e-06 6.47454966e-06 3.96684973e-06\n",
            " 9.02376621e-06 4.55939750e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming KMeans clustering, with playtrace type NGram_2\n",
            "Perfoming KMedoids clustering, with playtrace type NGram_2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn_extra/cluster/_k_medoids.py:252: UserWarning: n_clusters should be larger than 2 if max_iter != 0 setting max_iter to 0.\n",
            "  warnings.warn(\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 336, in set_trace\n",
            "    sys.settrace(self.trace_dispatch)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perfoming DBSCAN clustering, with playtrace type NGram_2\n",
            "> \u001b[0;32m<ipython-input-22-58a39fa3cfbc>\u001b[0m(103)\u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    101 \u001b[0;31m              \u001b[0;32mif\u001b[0m \u001b[0msil_avg\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_sil_avg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    102 \u001b[0;31m                \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 103 \u001b[0;31m                \u001b[0mbest_sil_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msil_avg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    104 \u001b[0;31m                \u001b[0mbest_sil_coeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msilhouette_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_matrix_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_labels_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'precomputed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    105 \u001b[0;31m                \u001b[0mbest_minpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminpts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 347, in set_continue\n",
            "    sys.settrace(None)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming SPCluster_KNN clustering, with playtrace type NGram_2\n",
            "Perfoming SPCluster_RBF clustering, with playtrace type NGram_2\n",
            "Perfoming SPCluster_DM clustering, with playtrace type NGram_2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited at iteration 48 with accuracies \n",
            "[5.74743323e-16 4.71047591e-06 6.32867283e-06 5.16295054e-06\n",
            " 5.80198679e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "Use iteration 48 instead with accuracy \n",
            "4.40081721401462e-06.\n",
            "\n",
            "  _, diffusion_map = lobpcg(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:393: UserWarning: Exited postprocessing with accuracies \n",
            "[6.13027164e-16 4.71047582e-06 6.32867293e-06 5.16295043e-06\n",
            " 5.80198685e-06]\n",
            "not reaching the requested tolerance 5.9604644775390625e-06.\n",
            "  _, diffusion_map = lobpcg(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}