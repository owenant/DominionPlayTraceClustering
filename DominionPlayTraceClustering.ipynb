{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/owenant/DominionPlayTraceClustering/blob/main/DominionPlayTraceClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "3d63dad0",
      "metadata": {
        "id": "3d63dad0"
      },
      "outputs": [],
      "source": [
        "#This notebook rpocesses card count and N-Gram data produced by the TableTopGames Frameowrk and also human player logs. It runs K-Means, K-Medoids, DBSCAN and SPectral Clustering, compuitng\n",
        "#silhouette averages, silhouette coefficients and MDS plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "Dy3-UJsWmTH0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy3-UJsWmTH0",
        "outputId": "7e6e3efb-6fac-4039-d320-17e35c00184c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn-extra in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn-extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "d792f3be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d792f3be",
        "outputId": "fa42dbf5-0aad-46f0-99ce-85addb3a41df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pdb\n",
        "import re\n",
        "import nltk\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from itertools import product, permutations, combinations\n",
        "from nltk import ngrams\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import random\n",
        "from google.colab import drive\n",
        "import csv\n",
        "import os\n",
        "import warnings\n",
        "import matplotlib.patches as mpatches\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "duv7Pp_FAnZc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duv7Pp_FAnZc",
        "outputId": "1d38d645-2b87-4f79-d9a0-88dcb9861473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'gdrive/My Drive/Colab Notebooks/UoB_MscThesis_Dominion_Clustering/DominionPlayTraceClustering/Results/b500_vs_b500_Restricted_NoWorkshop_SD/Results_KMeans' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/UoB_MscThesis_Dominion_Clustering/DominionPlayTraceClustering/Results/b500_vs_b500_Restricted_NoWorkshop_SD/Results_KMedoids' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/UoB_MscThesis_Dominion_Clustering/DominionPlayTraceClustering/Results/b500_vs_b500_Restricted_NoWorkshop_SD/Results_DBSCAN' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/UoB_MscThesis_Dominion_Clustering/DominionPlayTraceClustering/Results/b500_vs_b500_Restricted_NoWorkshop_SD/Results_SPClustering' created successfully.\n",
            "Directory 'gdrive/My Drive/Colab Notebooks/UoB_MscThesis_Dominion_Clustering/DominionPlayTraceClustering/Results/b500_vs_b500_Restricted_NoWorkshop_SD/Total_Average_Playtraces/' created successfully.\n"
          ]
        }
      ],
      "source": [
        "#list of clustering methods to use\n",
        "clustering_methods = ['KMeans', 'KMedoids', 'DBSCAN', 'SPClustering']\n",
        "\n",
        "#filenames and directory locations\n",
        "google_drive_parent_dir = \"gdrive/My Drive/Colab Notebooks/UoB_MscThesis_Dominion_Clustering/DominionPlayTraceClustering/\"\n",
        "card_count_data_dir = google_drive_parent_dir + \"DataCardCount/\"\n",
        "ngram_data_dir = google_drive_parent_dir + \"DataNGrams/\"\n",
        "card_count_data_filename = card_count_data_dir + \"trace_logfile_b500_vs_b500_Restricted_NoWorkshop_SD.txt\"\n",
        "ngram_data_filename = ngram_data_dir + \"ActionsReduced_b500_vs_b500_Restricted_NoWorkshop_SD.csv\"\n",
        "tag_for_dir_and_filenames = 'b500_vs_b500_Restricted_NoWorkshop_SD'\n",
        "\n",
        "#create new directory for output files\n",
        "for method in clustering_methods:\n",
        "  new_dir_path = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_' + method\n",
        "  os.makedirs(new_dir_path, exist_ok=True)\n",
        "\n",
        "  # Verify that the directory has been created\n",
        "  if os.path.exists(new_dir_path):\n",
        "      print(f\"Directory '{new_dir_path}' created successfully.\")\n",
        "  else:\n",
        "      print(f\"Failed to create directory '{new_dir_path}'.\")\n",
        "\n",
        "#also create directory for round and score distributions and average playtraces\n",
        "new_dir_path = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/RoundAndScoreDistributions/'\n",
        "os.makedirs(new_dir_path, exist_ok=True)\n",
        "new_dir_path = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Total_Average_Playtraces/'\n",
        "os.makedirs(new_dir_path, exist_ok=True)\n",
        "\n",
        "# Verify that the directory has been created\n",
        "if os.path.exists(new_dir_path):\n",
        "    print(f\"Directory '{new_dir_path}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Failed to create directory '{new_dir_path}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "lM1jnEiFsQwI",
      "metadata": {
        "id": "lM1jnEiFsQwI"
      },
      "outputs": [],
      "source": [
        "#parameters for notebook execution\n",
        "\n",
        "#run configuration\n",
        "run_config_cardcount = {'KMeans': True, 'KMedoids': True, 'DBSCAN': True, 'SP_FullyConnected': False, 'SP_KNN': False, 'Total_avg':True}\n",
        "run_config_ngrams = {'KMedoids': True, 'DBSCAN': True, 'SP_FullyConnected': False, 'SP_KNN': False, 'Total_avg': True}\n",
        "#run_config_cardcount = {'KMeans': True, 'KMedoids': False, 'DBSCAN': False, 'SP_FullyConnected': False, 'SP_KNN': False, 'Total_avg': False}\n",
        "#run_config_ngrams = {'KMedoids': True, 'DBSCAN': False, 'SP_FullyConnected': False, 'SP_KNN': False, 'Total_avg': False}\n",
        "\n",
        "\n",
        "#thresholds for outlier removal for score\n",
        "score_threshold = 100\n",
        "round_threshold = 50\n",
        "\n",
        "#game IDs to remove if needed\n",
        "gamesids_to_delete = []\n",
        "\n",
        "#kingdom card set\n",
        "kingdom_set = 'SD'\n",
        "\n",
        "#parameters if using TAG input data\n",
        "logs_from_tag = True\n",
        "agent_names = ['Medium', 'MediumRestrictedNoWorkShop']\n",
        "games_per_matchup = 250\n",
        "no_self_play = True\n",
        "\n",
        "#parameters for grid search for clustering methods\n",
        "\n",
        "#number of clusters to check across all clustering methods (excluding DBSCAN)\n",
        "clusters_min = 2\n",
        "clusters_max = 5\n",
        "#clusters_max = 2\n",
        "clusters_stepsize = 1\n",
        "\n",
        "#DBSCAN\n",
        "minpts_min = 5\n",
        "minpts_max = 250\n",
        "minpts_stepsize = 10\n",
        "epsilon_min = 0.01\n",
        "epsilon_max = 1\n",
        "epsilon_stepsize = 0.005\n",
        "\n",
        "#Spectral clustering K-Nearest Neighbours\n",
        "nearest_neighbours_min = 25\n",
        "nearest_neighbours_max = 250\n",
        "nearest_neighbours_stepsize = 10\n",
        "\n",
        "#Spectral clustering radial basis function\n",
        "gamma_min = 0.05\n",
        "gamma_max = 1\n",
        "gamma_stepsize = 0.025\n",
        "\n",
        "#number of N-gram types to search over\n",
        "ngram_min = 1\n",
        "ngram_max = 3\n",
        "#ngram_min = 1\n",
        "#ngram_max = 1\n",
        "ngram_stepsize = 1\n",
        "\n",
        "#values of k for l_k norm\n",
        "k_norms = [0.5, 1, 2]\n",
        "#k_norms = [0.5]\n",
        "\n",
        "#threshold values for plotting probability distributions for N-Gram playtraces\n",
        "thresholds = {1: 0.0, 2:0.02, 3:0.02}\n",
        "\n",
        "#for compatibility between different versions of TAG, newer versions differentiate between End Current Phase for play phase\n",
        "#and buy phase (set to true if you are using log files based on restricted play MCTS, otherwise set to false)\n",
        "split_end_phase = True\n",
        "\n",
        "#output config file containing parameters for the run\n",
        "config_filename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/config'\n",
        "with open(config_filename + '.csv', 'w', newline='') as csv_file:\n",
        "  #output run configuration\n",
        "  writer = csv.DictWriter(csv_file, fieldnames= run_config_cardcount.keys())\n",
        "  writer.writeheader()\n",
        "  writer.writerow(run_config_cardcount)\n",
        "  writer = csv.DictWriter(csv_file, fieldnames= run_config_ngrams.keys())\n",
        "  writer.writeheader()\n",
        "  writer.writerow(run_config_ngrams)\n",
        "  writer = csv.writer(csv_file)\n",
        "  writer.writerow(['Score Threshold: ', score_threshold])\n",
        "  writer.writerow(['Round Threshold: ', round_threshold])\n",
        "  writer.writerow(['GameIDs deleted: ', gamesids_to_delete])\n",
        "  writer.writerow(['Kingdom Set: ', 'kingdom_set'])\n",
        "  writer.writerow(['Logs from TAG: ', logs_from_tag])\n",
        "  writer.writerow(['Agent Names: ', agent_names])\n",
        "  writer.writerow(['Games per matchup: ', games_per_matchup])\n",
        "  writer.writerow(['No Self Play: ', no_self_play])\n",
        "  writer.writerow(['Min Clusters: ', clusters_min])\n",
        "  writer.writerow(['Max Clusters: ', clusters_max])\n",
        "  writer.writerow(['Clusters stepsize: ', clusters_stepsize])\n",
        "  writer.writerow(['Min minpts: ', minpts_min])\n",
        "  writer.writerow(['Max minpts: ', minpts_max])\n",
        "  writer.writerow(['Minpts stepsize: ', minpts_stepsize])\n",
        "  writer.writerow(['Min epsilon: ', epsilon_min])\n",
        "  writer.writerow(['Max epsilon: ', epsilon_max])\n",
        "  writer.writerow(['Epsilon stepsize: ', epsilon_stepsize])\n",
        "  writer.writerow(['Min nearest neighbours: ', nearest_neighbours_min])\n",
        "  writer.writerow(['Max nearest neighbours: ', nearest_neighbours_max])\n",
        "  writer.writerow(['Nearest neighbours stepsize: ', nearest_neighbours_stepsize])\n",
        "  writer.writerow(['Min ngrams: ', ngram_min])\n",
        "  writer.writerow(['Max ngrams: ', ngram_max])\n",
        "  writer.writerow(['Ngrams stepsize: ', ngram_stepsize])\n",
        "  writer.writerow(['knorms: ', k_norms])\n",
        "  writer.writerow(['Tri-gram thresholds: ', thresholds])\n",
        "  writer.writerow(['Split ECP between play and buy: ', split_end_phase])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "m_oEYop-1Ma2",
      "metadata": {
        "id": "m_oEYop-1Ma2"
      },
      "outputs": [],
      "source": [
        "#kingdom card types\n",
        "card_types_SD = ['ARTISAN', 'BANDIT', 'BUREAUCRAT', 'CHAPEL', 'FESTIVAL', 'GARDENS', 'SENTRY', 'THRONE_ROOM', 'WITCH',\n",
        "                 'WORKSHOP', 'CURSE', 'PROVINCE', 'DUCHY', 'ESTATE', 'GOLD', 'SILVER', 'COPPER']\n",
        "card_types_FG1E = ['CELLAR','MARKET','MILITIA','MINE','MOAT','REMODEL','SMITHY','VILLAGE',\n",
        "                'WOODCUTTER','WORKSHOP','CURSE','PROVINCE', 'DUCHY', 'ESTATE', 'GOLD', 'SILVER', 'COPPER']\n",
        "\n",
        "if kingdom_set == 'SD':\n",
        "  card_types = card_types_SD\n",
        "elif kingdom_set == 'FG1E':\n",
        "  card_types = card_types_FG1E\n",
        "else:\n",
        "  print('Unrecognised kingdom card set')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "HxQYGYs8wAIV",
      "metadata": {
        "id": "HxQYGYs8wAIV"
      },
      "outputs": [],
      "source": [
        "#functions to process data from TAG\n",
        "\n",
        "def gameID_to_matchup(game_id, player_no, matchup_list, no_games_per_matchup, min_game_id):\n",
        "    game_group = int((game_id - min_game_id)/no_games_per_matchup)\n",
        "    matchup = matchup_list[game_group]\n",
        "    agent1, agent2 = matchup\n",
        "    if player_no == 0:\n",
        "        return agent1\n",
        "    else:\n",
        "        return agent2\n",
        "\n",
        "def add_TAG_agent_names(agent_names, games_per_match_up, no_self_play, logs_from_tag, data):\n",
        "  NoOfGames = len(data['GameID'].unique())\n",
        "  min_GameID = data['GameID'].min()\n",
        "\n",
        "  #first generate match-ups\n",
        "  matchups = []\n",
        "  if no_self_play:\n",
        "    matchups = list(permutations(agent_names, 2))\n",
        "  else:\n",
        "      for agent1 in agent_names:\n",
        "          for agent2 in agent_names:\n",
        "              matchups.append((agent1, agent2))\n",
        "\n",
        "  #add agent names to data set\n",
        "  data['AgentName'] = data.apply(lambda row: gameID_to_matchup(row['GameID'], row['Player'], matchups, games_per_matchup, min_GameID), axis = 1)\n",
        "\n",
        "  #finally we also add the name of the agent of the opponent\n",
        "  min_GameID = data['GameID'].min()\n",
        "  data['Opponent'] = data.apply(lambda row: 1.0 if row['Player'] == 0.0 else 0.0, axis = 1)\n",
        "  if logs_from_tag:\n",
        "      data['AgentNameOpponent'] = data.apply(lambda row: gameID_to_matchup(row['GameID'], row['Opponent'], matchups, games_per_matchup, min_GameID), axis = 1)\n",
        "  else:\n",
        "      gameid_to_players_dict = data.groupby('GameID')['AgentName'].apply(list).to_dict()\n",
        "      data['AgentNameOpponent'] = data.apply(lambda row: other_dict_element(gameid_to_players_dict, row['GameID'], row['AgentName']), axis = 1)\n",
        "\n",
        "\n",
        "#functions to process card count data\n",
        "def copy_final_deck_at_game_end(group, roundMax, noPlayers):\n",
        "  #This function repeatedly copies the final decks of two players at the game end, so that the game is extended to\n",
        "  #have roundMax rounds\n",
        "  final_round = int(group['Round'].max())\n",
        "  if (roundMax-1) == final_round:\n",
        "      #in this case we dont need to extend the play trace\n",
        "      return group\n",
        "  else:\n",
        "      final_row_copy = pd.concat([group.iloc[-noPlayers:]] * ((roundMax-1) - final_round), ignore_index=True)\n",
        "      #we need to update the Round counter so that every other row it increments by one\n",
        "      final_row_copy['Round'] = [final_round + 1 + i // 2 for i in range(((roundMax-1) - final_round)*2)]\n",
        "      return pd.concat([group, final_row_copy], ignore_index=True)\n",
        "\n",
        "#given a dictionary whose elements are lists of length two, grab the other element not given by elem\n",
        "def other_dict_element(my_dict, my_key, my_elem):\n",
        "    index_of_given_element = my_dict[my_key].index(my_elem)\n",
        "    index_of_other_element =  1 if (index_of_given_element == 0) else 0\n",
        "    return my_dict[my_key][index_of_other_element]\n",
        "\n",
        "def process_card_count_data(cardcount_filename, agent_names, games_per_matchup,\n",
        "                            no_self_play, card_types, logs_from_tag, games_to_remove = []):\n",
        "  data = pd.read_csv(cardcount_filename, sep = '\\t')\n",
        "  #rename throne room column if needed\n",
        "  #if 'THRONE ROOM' in card_types:\n",
        "  #  data.rename(columns={'THRONE_ROOM':'THRONE ROOM'}, inplace = True)\n",
        "\n",
        "  add_TAG_agent_names(agent_names, games_per_matchup, no_self_play, logs_from_tag, data)\n",
        "  index_cols = ['Player', 'GameID']\n",
        "  non_card_types_round_indep_cols = ['AgentName', 'AgentNameOpponent', 'Win', 'FinalScore', 'TotalRounds']\n",
        "  cols = index_cols + non_card_types_round_indep_cols + ['Round'] + card_types #final set of cols to keep\n",
        "  if logs_from_tag:\n",
        "    data = data[data['Turn'] == 1] #only want cards at end of round\n",
        "  data = data.loc[:, cols]\n",
        "\n",
        "  #remove given list of gameIDs\n",
        "  data = data[~data['GameID'].isin(games_to_remove)]\n",
        "\n",
        "  #freeze decks and copy to max round number\n",
        "  no_players = 2\n",
        "  gameLengths = data.groupby(['GameID'])['Round'].max()\n",
        "  maxNoOfRounds = int(gameLengths.max()) + 1 #round counter starts at zero\n",
        "  noOfGames = len(data['GameID'].unique())\n",
        "  data = data.groupby('GameID').apply(copy_final_deck_at_game_end, maxNoOfRounds, no_players).reset_index(drop = True)\n",
        "\n",
        "  #check shape of data\n",
        "  print(\"Card count shape check:\")\n",
        "  print(\"Expected no rows: \" + str(maxNoOfRounds*no_players*noOfGames))\n",
        "  print(\"Expected no of cols: \" + str(len(card_types)+8))\n",
        "  print(data.shape)\n",
        "\n",
        "  return data\n",
        "\n",
        "def flatten_card_count_data(cardcount_data, card_types):\n",
        "  #next we need to flatten our data so that each trace is a single row.\n",
        "  #We also drop the round label as it is redundant\n",
        "  #and it will get reintroduced when flattening through the revised column names\n",
        "\n",
        "  #first create dataframe consisting of only non card type data types that are round\n",
        "  #independent\n",
        "  index_cols = ['Player', 'GameID']\n",
        "  non_card_types_round_indep_cols = ['AgentName', 'AgentNameOpponent', 'Win', 'FinalScore', 'TotalRounds']\n",
        "  non_card_data_round_indep = cardcount_data[index_cols + non_card_types_round_indep_cols].drop_duplicates()\n",
        "\n",
        "  #next need to Group by Player and GameID and then flatten card data by round\n",
        "  traces_tmp = cardcount_data[index_cols + card_types]\n",
        "  gameLengths = cardcount_data.groupby(['GameID'])['Round'].max()\n",
        "  maxNoOfRounds = int(gameLengths.max()) + 1 #round counter starts at zero\n",
        "  cols = [card_types[i] + \"_R\" + str(r)\n",
        "          for r in range(0, maxNoOfRounds) for i in range(0, len(card_types))]\n",
        "\n",
        "  extended_traces_flat = traces_tmp.groupby(index_cols).apply(lambda df: df[card_types].values.flatten())\n",
        "  extended_traces_flat = pd.DataFrame(extended_traces_flat, columns = ['Trace']).reset_index()\n",
        "  extended_traces_flat = pd.concat([extended_traces_flat[index_cols], extended_traces_flat['Trace'].apply(pd.Series)], axis=1)\n",
        "  extended_traces_flat.columns = index_cols + cols\n",
        "\n",
        "  #next we add back in the round independent data\n",
        "  extended_traces_flat = pd.merge(non_card_data_round_indep, extended_traces_flat, on = index_cols)\n",
        "\n",
        "  return extended_traces_flat\n",
        "\n",
        "#functions to process NGram data\n",
        "def format_action(action, cardtypes, split_ecp):\n",
        "  #there are various types of actions we need to format to identify these we use\n",
        "  #regular expressions\n",
        "  pattern_list = []\n",
        "  if split_ecp:\n",
        "    pattern_list.append(re.compile(r'End Phase: Play'))\n",
        "    pattern_list.append(re.compile(r'End Phase: Buy'))\n",
        "  else:\n",
        "    pattern_list.append(re.compile(r'End Current Phase'))\n",
        "  pattern_list.append(re.compile(r'BuyCard: (' + '|'.join(cardtypes) + r') by player (0|1)'))\n",
        "  pattern_list.append(re.compile(r'(' + '|'.join(cardtypes) + r') : Player (0|1)'))\n",
        "  pattern_list.append(re.compile(r'GainCard: (' + '|'.join(cardtypes) + r') by player (0|1)'))\n",
        "  pattern_list.append(re.compile(r'Player (0|1) trashes a (' + '|'.join(cardtypes) + r') from (?:HAND|DISCARD)'))\n",
        "  pattern_list.append(re.compile(r'DoNothing'))\n",
        "  pattern_list.append(re.compile(r'Player (0|1) moves (' + '|'.join(cardtypes) + r') from HAND to DRAW of player (0|1) \\(visible: (?:true|false)\\)'))\n",
        "  pattern_list.append(re.compile(r'Reveals Hand'))\n",
        "  pattern_list.append(re.compile(r'Sentry .*$')) #captures playing a sentry and then discard/trash two cards\n",
        "  pattern_list.append(re.compile(r'Player (0|1) discards (' + '|'.join(cardtypes) + r')'))\n",
        "  pattern_list.append(re.compile(r'Player (0|1) reveals a (' + '|'.join(cardtypes) + r')'))\n",
        "\n",
        "  match_list = [None] * len(pattern_list)\n",
        "\n",
        "  if split_ecp:\n",
        "    pattern_to_string_map = ['ECP', 'ECP', 'Buy', 'Play', 'Gain', 'Trashes',\n",
        "                          'DoNothing', 'Moves', 'RevealsHand', 'PlaySentry',\n",
        "                          'Discards', 'Reveals']\n",
        "  else:\n",
        "    pattern_to_string_map = ['ECP', 'Buy', 'Play', 'Gain', 'Trashes',\n",
        "                              'DoNothing', 'Moves', 'RevealsHand', 'PlaySentry',\n",
        "                              'Discards', 'Reveals']\n",
        "\n",
        "  for index in range(0, len(pattern_list)):\n",
        "    matched = pattern_list[index].match(action)\n",
        "    pattern_index = index\n",
        "    if matched != None:\n",
        "      break\n",
        "\n",
        "  if matched == None:\n",
        "    pdb.set_trace()\n",
        "    raise Exception(\"Can't match action description\")\n",
        "\n",
        "  if split_ecp:\n",
        "    if pattern_index in [0, 1, 6, 8, 9]:\n",
        "      formatted_action =  pattern_to_string_map[pattern_index]\n",
        "    elif pattern_index in [2, 3, 4]:\n",
        "      matched_card =  matched.group(1)\n",
        "      formatted_action =  pattern_to_string_map[pattern_index]  + matched_card.lower().capitalize()\n",
        "    else:\n",
        "      matched_card = matched.group(2)\n",
        "      formatted_action =  pattern_to_string_map[pattern_index]  + matched_card.lower().capitalize()\n",
        "  else:\n",
        "    if pattern_index in [0, 5, 7, 8]:\n",
        "      formatted_action =  pattern_to_string_map[pattern_index]\n",
        "    elif pattern_index in [1, 2, 3]:\n",
        "      matched_card =  matched.group(1)\n",
        "      formatted_action =  pattern_to_string_map[pattern_index]  + matched_card.lower().capitalize()\n",
        "    else:\n",
        "      matched_card = matched.group(2)\n",
        "      formatted_action =  pattern_to_string_map[pattern_index]  + matched_card.lower().capitalize()\n",
        "\n",
        "  return formatted_action\n",
        "\n",
        "def process_ngram_data(actions_filename, agent_names, games_per_match_up, no_self_play,\n",
        "                       card_types, ngram_min, ngram_max, ngram_stepsize, logs_from_tag, split_ecp, games_to_remove = []):\n",
        "  data  = pd.read_csv(actions_filename)\n",
        "  data = data[['GameID', 'Player', 'Round','Turn','ActionDescription']]\n",
        "  add_TAG_agent_names(agent_names, games_per_match_up, no_self_play, logs_from_tag, data)\n",
        "  data['ProcAction'] = data.apply(lambda row: format_action(row['ActionDescription'], card_types, split_ecp), axis = 1)\n",
        "  data = data.groupby(['GameID', 'Player','AgentName', 'AgentNameOpponent'])['ProcAction'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "  #remove given list of gameIDs\n",
        "  data = data[~data['GameID'].isin(games_to_remove)]\n",
        "\n",
        "  for n in range(ngram_min, ngram_max +1, ngram_stepsize):\n",
        "    col_name = 'NGrams_' + str(n)\n",
        "    data[col_name] = data.apply(lambda row: list(ngrams(nltk.word_tokenize(row['ProcAction']),n)), axis = 1)\n",
        "\n",
        "  return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "t3I3J5tKvfoA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3I3J5tKvfoA",
        "outputId": "08ee8f32-ae5f-484b-e166-9619d4c207fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Card count shape check:\n",
            "Expected no rows: 65000\n",
            "Expected no of cols: 25\n",
            "(65000, 25)\n",
            "1000\n",
            "65000\n"
          ]
        }
      ],
      "source": [
        "#process data\n",
        "traces_cardcount = process_card_count_data(card_count_data_filename, agent_names, games_per_matchup, no_self_play, card_types, logs_from_tag, gamesids_to_delete)\n",
        "traces_ngrams = process_ngram_data(ngram_data_filename, agent_names, games_per_matchup, no_self_play,\n",
        "                       card_types, ngram_min, ngram_max, ngram_stepsize, logs_from_tag, split_end_phase, gamesids_to_delete)\n",
        "print(len(traces_ngrams))\n",
        "print(len(traces_cardcount))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "ZguwBNUQeveK",
      "metadata": {
        "id": "ZguwBNUQeveK"
      },
      "outputs": [],
      "source": [
        "#function to plot score and round distributions and output to file\n",
        "def plot_score_round_distributions(outputfilename, card_count_data):\n",
        "  fig, axs = plt.subplots(2, 1)\n",
        "  grouped_data = card_count_data.groupby('GameID')\n",
        "  score_data = grouped_data['FinalScore'].unique().explode()\n",
        "  axs[0].hist(score_data, bins=np.arange(score_data.min(), score_data.max()+1))\n",
        "  axs[0].set_xlabel('Final score')\n",
        "  axs[0].set_ylabel('Number of games')\n",
        "  axs[0].set_title('Score distribution')\n",
        "  round_data = grouped_data['TotalRounds'].unique().explode()\n",
        "  axs[1].hist(round_data, bins=np.arange(round_data.min(), round_data.max()+1))\n",
        "  axs[1].set_xlabel('Number of rounds')\n",
        "  axs[1].set_ylabel('Number of games')\n",
        "  axs[1].set_title('Round distribution')\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "  #also output mean score and mean number of rounds\n",
        "  mean_score = np.mean(np.array(score_data))\n",
        "  mean_rounds = np.mean(np.array(round_data))\n",
        "  print(\"Mean score: \" + str(mean_score))\n",
        "  print(\"Mean no of rounds: \" + str(mean_rounds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "FwExLtTpgSeO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwExLtTpgSeO",
        "outputId": "39ad228e-617c-4260-e285-1fdac622f385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round and score distribution before outliers removed:\n",
            "Mean score: 51.91983556012333\n",
            "Mean no of rounds: 30.276\n",
            "Proportion of games removed: 0.034\n"
          ]
        }
      ],
      "source": [
        "#remove outliers based on thresholds for score and length of game\n",
        "print(\"Round and score distribution before outliers removed:\")\n",
        "outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/RoundAndScoreDistributions/' + 'RoundAndScoreDistribution_' + tag_for_dir_and_filenames\n",
        "plot_score_round_distributions(outputfilename, traces_cardcount)\n",
        "\n",
        "games_to_remove = traces_cardcount['GameID'][(traces_cardcount['FinalScore'] > score_threshold) | (traces_cardcount['TotalRounds'] > round_threshold)]\n",
        "games_to_remove = games_to_remove.unique()\n",
        "\n",
        "print(\"Proportion of games removed: \" + str(len(games_to_remove)/len(traces_cardcount['GameID'].unique())))\n",
        "\n",
        "traces_cardcount = traces_cardcount[~traces_cardcount['GameID'].isin(games_to_remove)]\n",
        "traces_ngrams = traces_ngrams[~traces_ngrams['GameID'].isin(games_to_remove)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "DSWZqXOBksgl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSWZqXOBksgl",
        "outputId": "08160788-a9a4-46fb-9ede-15aceaa45b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round and score distribution after outliers removed:\n",
            "Mean score: 51.329073482428115\n",
            "Mean no of rounds: 29.387163561076605\n"
          ]
        }
      ],
      "source": [
        "print(\"Round and score distribution after outliers removed:\")\n",
        "outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/RoundAndScoreDistributions/' + 'RoundAndScoreDistribution_no_outliers_' + tag_for_dir_and_filenames\n",
        "plot_score_round_distributions(outputfilename, traces_cardcount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "6axA42kyBMfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6axA42kyBMfb",
        "outputId": "83c1f68f-7967-4612-d17f-3f668399c744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "966\n",
            "966\n"
          ]
        }
      ],
      "source": [
        "#flatten card count data so that we have a playtrace per row\n",
        "traces_cardcount = flatten_card_count_data(traces_cardcount, card_types)\n",
        "print(len(traces_cardcount))\n",
        "print(len(traces_ngrams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "oTi_VsyRI6Bl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTi_VsyRI6Bl",
        "outputId": "592babe0-6999-4bc8-87f9-1f325b0281e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of Ngrams for N=1:76\n",
            "Total number of Ngrams for N=2:881\n",
            "Total number of Ngrams for N=3:3412\n"
          ]
        }
      ],
      "source": [
        "#Compute the all ngrams list by taking the total list of all observed n-grams for this tournament\n",
        "all_ngrams_list = {}\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  col_name = 'NGrams_' + str(n)\n",
        "  all_ngrams_list[n] = []\n",
        "  for row in traces_ngrams[col_name]:\n",
        "    for gram in row:\n",
        "      if gram not in all_ngrams_list[n]:\n",
        "        all_ngrams_list[n].append(gram)\n",
        "  print(\"Total number of Ngrams for N=\" + str(n) + \":\" + str(len(all_ngrams_list[n])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "0b646b11",
      "metadata": {
        "id": "0b646b11"
      },
      "outputs": [],
      "source": [
        "#functions to support NGram analysis\n",
        "\n",
        "#function to compute N-gram probabilities, returns either an array with probability values\n",
        "#in the same order as ngrams in ngrams_all, or a dictionary with the n-grams as key\n",
        "#Unobserved ngrams (i.e. ngrams in ngrams_all, that are not in the trace) are assigned\n",
        "#a default probability of zero.\n",
        "def calc_probabilities(ngrams_trace, ngrams_all, convertToArray = False):\n",
        "    # Compute the frequency of ngrams in the trace\n",
        "    frequency_counter = Counter(ngrams_trace)\n",
        "\n",
        "    #calculate frequencies of all ngrams in ngrams_all that appear in the playtrace\n",
        "    event_count = {gram: frequency_counter.get(gram, 0) for gram in ngrams_all}\n",
        "\n",
        "    #normalise each entry with the number of n-grams observed for that trace, to convert\n",
        "    #counts into probabilities\n",
        "    trace_n_gram_count = sum(frequency_counter.values())\n",
        "    probs = {key: value / (1.0*trace_n_gram_count) for key, value in event_count.items()}\n",
        "\n",
        "    if convertToArray:\n",
        "        probs = np.array(list(probs.values()))\n",
        "\n",
        "    return probs\n",
        "\n",
        "#function to take a probability dictionary and create an array\n",
        "def prob_dict_to_array(prob_dict):\n",
        "    return np.array(list(prob_dict.values()))\n",
        "\n",
        "#funciton to take probability array and convert to dictionary with n-grams as keys\n",
        "#assumes ordering has been maintained\n",
        "def prob_array_to_dict(prob_array, ngrams_all):\n",
        "    prob_dict = {}\n",
        "    index = 0\n",
        "    for gram in ngrams_all:\n",
        "        prob_dict[gram] = prob_array[index]\n",
        "        index+=1\n",
        "    return prob_dict\n",
        "\n",
        "#find the common set of ngrams between two probability dictionaries, with probabilities\n",
        "#above a given threshold\n",
        "def return_common_ngrams_above_threshold(prob_dict1, prob_dict2, threshold):\n",
        "    common_ngrams = []\n",
        "    #look for entries in the first dictionary with non-zero values\n",
        "    for key, value in prob_dict1.items():\n",
        "        if value > threshold:\n",
        "            common_ngrams.append(key)\n",
        "    #repeat for the second dictionary but avoiding duplicates\n",
        "    for key, value in prob_dict2.items():\n",
        "        if (value > threshold) and (key not in common_ngrams):\n",
        "             common_ngrams.append(key)\n",
        "    return common_ngrams\n",
        "\n",
        "#convert a list of ngram tuples into a list of strings\n",
        "def convert_ngram_tuples_to_strings(ngrams_list):\n",
        "    ngrams_str = []\n",
        "    for tuple_item in ngrams_list:\n",
        "        tuple_str = ''\n",
        "        for index, element in enumerate(tuple_item):\n",
        "            if index != (len(tuple_item)-1):\n",
        "                tuple_str += element + '|'\n",
        "            else:\n",
        "                tuple_str += element\n",
        "        ngrams_str.append(tuple_str)\n",
        "    return ngrams_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "aac5d703",
      "metadata": {
        "id": "aac5d703"
      },
      "outputs": [],
      "source": [
        "#add columns to trace data containing arrays for probability data\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  col_name_1 = 'ProbDict_' + str(n)\n",
        "  col_name_2 = 'ProbArray_' + str(n)\n",
        "  col_name_3 = 'NGrams_' + str(n)\n",
        "  traces_ngrams[col_name_1] = traces_ngrams.apply(lambda row: calc_probabilities(row[col_name_3], all_ngrams_list[n], False), axis = 1)\n",
        "  traces_ngrams[col_name_2] = traces_ngrams.apply(lambda row: calc_probabilities(row[col_name_3], all_ngrams_list[n], True), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "ba9b0aaa",
      "metadata": {
        "id": "ba9b0aaa"
      },
      "outputs": [],
      "source": [
        "#a few quick sense checks\n",
        "#example_dict = traces_ngrams['ProbDict_2'].iloc[0]\n",
        "#example_array = traces_ngrams['ProbArray_2'].iloc[0]\n",
        "\n",
        "#check translation functions work\n",
        "#example_dict_converted_to_array = prob_dict_to_array(example_dict)\n",
        "#example_array_converted_to_dict = prob_array_to_dict(example_array, all_ngrams_list[2])\n",
        "\n",
        "#print(np.array_equal(example_dict_converted_to_array, example_array))\n",
        "#print(example_array_converted_to_dict == example_dict)\n",
        "\n",
        "#check probability array is normalised\n",
        "#print(sum(example_array))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "C5YckSsdsq2E",
      "metadata": {
        "id": "C5YckSsdsq2E"
      },
      "outputs": [],
      "source": [
        "#look for specific action in uni-grams - just for exploratory purposes\n",
        "#action = 'GAINCURSE'\n",
        "#for index, row in traces_ngrams.iterrows():\n",
        "#  for gram, prob in row['ProbDict_1'].items():\n",
        "#    if gram == action:\n",
        "#      print(\"N-Gram: \" + str(gram) + \" Prob: \" + str(prob))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "R_q5YHlOfdQy",
      "metadata": {
        "id": "R_q5YHlOfdQy"
      },
      "outputs": [],
      "source": [
        "#remove columns that are unnecessary for clustering algorithms and distance measure calculations\n",
        "cols = ['Player', 'GameID', 'AgentName', 'AgentNameOpponent', 'Win', 'FinalScore', 'TotalRounds']\n",
        "traces_cardcount_slim = traces_cardcount.drop(cols, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "baf6b8d0",
      "metadata": {
        "id": "baf6b8d0"
      },
      "outputs": [],
      "source": [
        "#functions to compute distance and affinity matrices for jenen-shannon and l_k norm distances\n",
        "def symm_distance_matrix(df, distance_func, func_param = False):\n",
        "    traces = df.tolist()\n",
        "    index_combinations = list(combinations(range(len(traces)), 2))\n",
        "\n",
        "    #we branch here so we can use both lk-norm and Jensen-Shannon in this function\n",
        "    if not func_param:\n",
        "      distance_values = [distance_func(traces[i],traces[j]) for i, j in index_combinations]\n",
        "    else:\n",
        "      distance_values = [distance_func(func_param, traces[i],traces[j]) for i, j in index_combinations]\n",
        "\n",
        "    num_rows = len(df)\n",
        "    distance_matrix = pd.DataFrame(index=range(num_rows), columns=range(num_rows))\n",
        "\n",
        "    for (i, j), distance_value in zip(index_combinations, distance_values):\n",
        "        distance_matrix.at[i, j] = distance_value\n",
        "        distance_matrix.at[j, i] = distance_value  # mirror the value\n",
        "\n",
        "    return distance_matrix.fillna(0)  # fill NaN values with zeros for diagonal elements\n",
        "\n",
        "#affinity function used when computing fully connected affinity matrix\n",
        "def connected_affinity_func(x, gamma):\n",
        "  return np.exp(-gamma * (x**2))\n",
        "\n",
        "#calculate fully connected affinity matrix\n",
        "def connected_affinity_matrix(dist_matrix, gamma):\n",
        "  eps = 0.00000001\n",
        "  matrix = np.vectorize(connected_affinity_func)(dist_matrix, gamma)\n",
        "  #note we put a lower bound on the entries in the infinity matrix to make sure\n",
        "  #we have a fully connected graph\n",
        "  return np.vectorize(max)(matrix, eps)\n",
        "\n",
        "#calculate k-nearest neighbour affinity matrix\n",
        "def knn_affinity_matrix(dist_matrix, k):\n",
        "  # Find indices of k-nearest neighbors for each data point\n",
        "  neigh = NearestNeighbors(n_neighbors=k + 1, metric='precomputed').fit(dist_matrix)\n",
        "  _, indices = neigh.kneighbors()\n",
        "\n",
        "  # Create knn affinity matrix\n",
        "  aff_matrix = np.zeros_like(dist_matrix, dtype=float)\n",
        "  for i in range(dist_matrix.shape[0]):\n",
        "      aff_matrix[i, indices[i, 1:]] = 1.0  # Assign 1 to the k-nearest neighbors\n",
        "\n",
        "  # Make the matrix symmetric\n",
        "  aff_matrix = 0.5 * (aff_matrix + aff_matrix.T)\n",
        "\n",
        "  return aff_matrix\n",
        "\n",
        "#function to calculate Jensen-Shannon distance\n",
        "def kl_divergence(p, q):\n",
        "  eps = 1e-10\n",
        "  return np.sum(np.where(p < eps, 0, np.where(q < eps, 0, p * np.log((p + eps) / (1.0 * q + eps)))))\n",
        "\n",
        "def jensen_shannon_distance(p, q, takesqrt = True):\n",
        "  m = 0.5 * (p + q)\n",
        "  js = 0.5 * (kl_divergence(p, m) + kl_divergence(q, m))\n",
        "  if takesqrt:\n",
        "    js = math.sqrt(js)\n",
        "  return js\n",
        "\n",
        "#function to compute l_k norm, here p and q are arrays of doubles\n",
        "def l_k_norm(k, p, q):\n",
        "  return np.sum(np.abs(p - q) ** k) ** (1/(1.0*k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "0xSth8rW2j8a",
      "metadata": {
        "id": "0xSth8rW2j8a"
      },
      "outputs": [],
      "source": [
        "#calculate distance, fully connected and k-nearest neighbour affinity matrices for l_k norm for card count playtraces\n",
        "dist_matrices = {}\n",
        "connected_affinity_matrices = {}\n",
        "knn_affinity_matrices = {}\n",
        "traces_cardcount_arrays = traces_cardcount_slim.apply(lambda row: np.array(row), axis=1)\n",
        "traces_cardcount_slim_normalised = {}\n",
        "dist_matrices_normalised = {}\n",
        "#pdb.set_trace()\n",
        "for k in k_norms:\n",
        "  key_norm = 'CardCount_lknorm_' + str(k)\n",
        "  connected_affinity_matrices[key_norm] = {}\n",
        "  knn_affinity_matrices[key_norm] = {}\n",
        "  if run_config_cardcount['KMeans'] or run_config_cardcount['KMedoids'] or run_config_cardcount['DBSCAN'] or run_config_cardcount['SP_FullyConnected'] or run_config_cardcount['SP_KNN']:\n",
        "    dist_matrices[key_norm] = symm_distance_matrix(traces_cardcount_arrays, l_k_norm, k)\n",
        "  if run_config_cardcount['SP_FullyConnected']:\n",
        "    for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "      key_gamma =  'gamma_' + str(gamma)\n",
        "      connected_affinity_matrices[key_norm][key_gamma] = connected_affinity_matrix(dist_matrices[key_norm], gamma)\n",
        "  if run_config_cardcount['SP_KNN']:\n",
        "    for nn in range(nearest_neighbours_min, nearest_neighbours_max, nearest_neighbours_stepsize):\n",
        "      key_nn =  'knn_' + str(nn)\n",
        "      knn_affinity_matrices[key_norm][key_nn] = knn_affinity_matrix(dist_matrices[key_norm], nn)\n",
        "  if run_config_cardcount['DBSCAN']:\n",
        "    #for dbcan we also need to normalise the playtraces to reduce the space we need to search over\n",
        "    #epsilon to between zero and one. Note that the Jensen-Shannon distance measure is by defintion less than one\n",
        "    #so we only need to do this for our card count playtraces\n",
        "    traces_cardcount_slim_normalised[key_norm] = traces_cardcount_slim.apply(lambda row: np.array(row)/l_k_norm(k, np.array(row), np.zeros(len(np.array(row)))), axis = 1)\n",
        "    dist_matrices_normalised[key_norm] = symm_distance_matrix(traces_cardcount_slim_normalised[key_norm], l_k_norm, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "IHljexNP2q8w",
      "metadata": {
        "id": "IHljexNP2q8w"
      },
      "outputs": [],
      "source": [
        "#calculate distance, fully connected and k-nearest neighbour affinity matrices for N-Gram playtraces\n",
        "for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  key_gram = 'N_Gram_' + str(n)\n",
        "  connected_affinity_matrices[key_gram] = {}\n",
        "  knn_affinity_matrices[key_gram] = {}\n",
        "  col_name = 'ProbArray_' + str(n)\n",
        "  if run_config_ngrams['KMedoids'] or run_config_ngrams['DBSCAN'] or run_config_ngrams['SP_FullyConnected'] or run_config_ngrams['SP_KNN']:\n",
        "    dist_matrices[key_gram] = symm_distance_matrix(traces_ngrams[col_name], jensen_shannon_distance)\n",
        "  if run_config_ngrams['SP_FullyConnected']:\n",
        "    for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "      key_gamma =  'gamma_' + str(gamma)\n",
        "      connected_affinity_matrices[key_gram][key_gamma] = connected_affinity_matrix(dist_matrices[key_gram], gamma)\n",
        "  if run_config_ngrams['SP_KNN']:\n",
        "    for nn in range(nearest_neighbours_min, nearest_neighbours_max, nearest_neighbours_stepsize):\n",
        "      key_nn =  'knn_' + str(nn)\n",
        "      knn_affinity_matrices[key_gram][key_nn] = knn_affinity_matrix(dist_matrices[key_gram], nn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "Z_Jtc-amEd5Q",
      "metadata": {
        "id": "Z_Jtc-amEd5Q"
      },
      "outputs": [],
      "source": [
        "#functions to perfom clustering analysis\n",
        "def sa_kmedoids(dist_matrix, num_clusters):\n",
        "  clusterer = KMedoids(n_clusters=num_clusters,\n",
        "                       metric='precomputed',\n",
        "                       method='pam',\n",
        "                       init='k-medoids++',\n",
        "                       max_iter=300,\n",
        "                       random_state= 0).fit(dist_matrix)\n",
        "  #when we use precomputed as the metric, the algorithm returns the indices for the cluster centres only\n",
        "  return clusterer.inertia_, clusterer.medoid_indices_, clusterer.labels_\n",
        "\n",
        "def sa_kmeans(data, num_clusters):\n",
        "  clusterer = KMeans(n_clusters=num_clusters,\n",
        "                      init='k-means++',\n",
        "                      n_init= 'warn',\n",
        "                      max_iter=300,\n",
        "                      tol=0.0001,\n",
        "                      verbose=0,\n",
        "                      random_state=0,\n",
        "                      copy_x=True,\n",
        "                      algorithm='lloyd').fit(data)\n",
        "  return clusterer.inertia_, clusterer.cluster_centers_, clusterer.labels_\n",
        "\n",
        "def sa_dbscan(dist_matrix, minPts, epsilon):\n",
        "  dbscan_clustering = DBSCAN(eps= epsilon, min_samples= minPts, metric = 'precomputed').fit(dist_matrix)\n",
        "  return dbscan_clustering.labels_\n",
        "\n",
        "#spectral clcustering using pre-computed affinity matrx\n",
        "def sa_spectral_clustering_AM(affinity_matrix, num_clusters):\n",
        "  spec_clustering_AM = SpectralClustering(n_clusters= num_clusters,\n",
        "                                        random_state=0,\n",
        "                                        affinity = 'precomputed',\n",
        "                                        assign_labels='kmeans').fit(affinity_matrix)\n",
        "  return spec_clustering_AM.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "7u9WU-Qiea27",
      "metadata": {
        "id": "7u9WU-Qiea27"
      },
      "outputs": [],
      "source": [
        "#functions to generate plots and output files\n",
        "def output_silhouette_plot(outputfilename, silhouette_samples, silhouette_avg, cluster_labels, flip_clusters = False):\n",
        "  n_clusters = len(np.unique(cluster_labels))\n",
        "\n",
        "  #Create a subplot with 1 row and 1 columns\n",
        "  fig, ax1 = plt.subplots(1,1, clear = True)\n",
        "  fig.set_size_inches(7, 3.5)\n",
        "\n",
        "  # The silhouette coefficient can range from -1, 1\n",
        "  ax1.set_xlim([-0.1, 1])\n",
        "  # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "  # plots of individual clusters, to demarcate them clearly.\n",
        "  ax1.set_ylim([0, len(silhouette_samples) + (n_clusters + 1) * 10])\n",
        "\n",
        "  y_lower = 10\n",
        "  cluster_range = list(range(n_clusters))\n",
        "  for i in cluster_range:\n",
        "      # Aggregate the silhouette scores for samples belonging to\n",
        "      # cluster i, and sort them\n",
        "      cluster = i\n",
        "      if flip_clusters:\n",
        "        cluster = (n_clusters -1) - i\n",
        "      ith_cluster_silhouette_values = silhouette_samples[cluster_labels == cluster]\n",
        "\n",
        "      ith_cluster_silhouette_values.sort()\n",
        "\n",
        "      size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "      y_upper = y_lower + size_cluster_i\n",
        "\n",
        "      color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "      if i == 0:\n",
        "        color = 'red'\n",
        "      elif i == 1:\n",
        "        color = 'blue'\n",
        "\n",
        "      ax1.fill_betweenx(\n",
        "          np.arange(y_lower, y_upper),\n",
        "          0,\n",
        "          ith_cluster_silhouette_values,\n",
        "          facecolor=color,\n",
        "          edgecolor=color,\n",
        "          alpha=0.7,\n",
        "      )\n",
        "\n",
        "      # Label the silhouette plots with their cluster numbers at the middle\n",
        "      ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, i)\n",
        "\n",
        "      # Compute the new y_lower for next plot\n",
        "      y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "  #ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "  ax1.set_xlabel(\"The silhouette coefficient values for K=\" + str(n_clusters))\n",
        "  ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "  # The vertical line for average silhouette score of all the values\n",
        "  ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "  ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "  ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "#output list of dictionary values\n",
        "def output_dictionary(outputfilename, dict):\n",
        "  with open(outputfilename + '.csv', 'w', newline='') as csv_file:\n",
        "    writer = csv.DictWriter(csv_file, fieldnames= dict.keys())\n",
        "    # Write the header\n",
        "    writer.writeheader()\n",
        "    # Write the data\n",
        "    writer.writerow(dict)\n",
        "\n",
        "#output string, value pairs to file\n",
        "def output_values(outputfilename, values_list, valueslabels_list):\n",
        "  with open(outputfilename + '.csv', 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for index in range(0, len(values_list)):\n",
        "      writer.writerow([valueslabels_list[index], values_list[index]])\n",
        "\n",
        "def output_inertia_plot(outputfilename, inertia_vals_dict, scalar):\n",
        "  #scale the inertia vals, typically the scalar value will be the inertia for clustering on one cluster\n",
        "  inertia_vals_array = np.array([value for value in inertia_vals_dict.values()])\n",
        "  inertia_vals_scaled = inertia_vals_array/scalar\n",
        "  cluster_list = np.array([cluster for cluster in inertia_vals_dict.keys()])\n",
        "  inertia_vals_scaled = np.insert(inertia_vals_scaled, 0, 1.0)\n",
        "  cluster_list = np.insert(cluster_list, 0, 1)\n",
        "\n",
        "  #plot as a line plot\n",
        "  fig, ax = plt.subplots(num=1,clear=True)\n",
        "  ax.plot(cluster_list, inertia_vals_scaled)\n",
        "  ax.set_xticks(cluster_list)\n",
        "  ax.set_xlabel(\"Number of clusters\")\n",
        "  ax.set_ylabel(\"Scaled inertia\")\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "#function to plot card count playtraces\n",
        "def cardcount_playtrace_comparison(outputfilename, trace_list, label_list, card_types, legendOn = True, ylimit = 0, output_to_screen = False, flip_clusters = False):\n",
        "  if flip_clusters:\n",
        "    trace_list.reverse()\n",
        "    #label_list.reverse()\n",
        "\n",
        "  #look at evolution of number of cards of each type per round.\n",
        "  #traces should all be of the same length\n",
        "  maxRounds =   int(trace_list[0].shape[1]/17)\n",
        "  noOfCardTypes = len(card_types)\n",
        "  noOfSubPlotCols = 5\n",
        "  noOfSubPlotRows = max(2, math.floor(noOfCardTypes/noOfSubPlotCols) + 1)\n",
        "  fig, axs = plt.subplots(noOfSubPlotRows, noOfSubPlotCols, figsize = (10,10))\n",
        "\n",
        "  #choose colors\n",
        "  hexadecimal_alphabets = '0123456789ABCDEF'\n",
        "  colors = [\"#\" + ''.join([random.choice(hexadecimal_alphabets) for j in\n",
        "  range(6)]) for i in range(len(trace_list))]\n",
        "  colors[0] = 'red'\n",
        "  if len(colors) >=2:\n",
        "    colors[1] = 'blue'\n",
        "\n",
        "  for i in range(0,noOfSubPlotRows):\n",
        "      for j in range(0,noOfSubPlotCols):\n",
        "          cardIndex = noOfSubPlotRows*j + i\n",
        "          if cardIndex >= len(card_types):\n",
        "              axs[i,j].set_visible(False)\n",
        "          else:\n",
        "              card_type = card_types[cardIndex]\n",
        "              card_col = [card_type + \"_R\" + str(r) for r in range(0,maxRounds)]\n",
        "              card_max = 0\n",
        "              for (index, trace) in enumerate(trace_list):\n",
        "                  #axs[i,j].plot(range(0,maxRounds), trace[card_col].iloc[0], label = label_list[index], color = colors[index])\n",
        "                  axs[i,j].plot(range(0,maxRounds), trace[card_col].iloc[0], color = colors[index])\n",
        "                  tmp_card_max = int(trace[card_col].iloc[0].max())\n",
        "                  if tmp_card_max > card_max:\n",
        "                      card_max = tmp_card_max\n",
        "\n",
        "              #set labels and limits\n",
        "              if card_type == 'THRONE_ROOM':\n",
        "                axs[i,j].set_title('Throne Room', fontsize = 16)\n",
        "              else:\n",
        "                axs[i,j].set_title(card_type.lower().capitalize(), fontsize = 18)\n",
        "              axs[i,j].set_xlabel('Round', fontsize = 16)\n",
        "              if ylimit == 0:\n",
        "                  axs[i,j].set_ylim((0,card_max+2))\n",
        "              else:\n",
        "                  axs[i,j].set_ylim((0,1))\n",
        "              #axs[i,j].set_ylim((0,card_max))\n",
        "              axs[i,j].set_xticks(ticks = range(0, maxRounds,10))\n",
        "              axs[i,j].tick_params(axis='both', which='major', labelsize = 10)\n",
        "\n",
        "          #tighten subplots layout\n",
        "          fig.tight_layout()\n",
        "\n",
        "  #plt.xticks(fontsize=16)\n",
        "  #plt.yticks(fontsize=16)\n",
        "\n",
        "  #add overal legend to figure\n",
        "  if legendOn:\n",
        "      #axs[0,noOfSubPlotCols - 1].legend(loc = (0,-5.0))\n",
        "      #axs[0,noOfSubPlotCols - 1].legend(fontsize = 8)\n",
        "      #axs[0,noOfSubPlotCols - 1].legend(loc='lower center', bbox_to_anchor=(0.5, -1.825), fontsize= 10) - doesnt work\n",
        "      fig.legend(labels = ['Cluster 0', 'Cluster 1'], bbox_to_anchor=(0.99,0.75), fontsize=14)\n",
        "      #fig.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  if output_to_screen:\n",
        "    plt.show()\n",
        "  else:\n",
        "    #output to file\n",
        "    plt.savefig(outputfilename +'.png', format = 'png')\n",
        "    plt.close()\n",
        "\n",
        "#plot cluster centroids based on cardcount playtraces, that are outputted directly from sklearn clustering algorithms\n",
        "def plot_cluster_centroids(outputfilename, cluster_centers, card_types, labels, legendOn = True, ylimit = 0, output_to_screen = False, flip_clusters = False):\n",
        "  #cluster centres outputted from sklearn are 2D arrays with rows corresponding to clusters and columns corresponding to length of trace\n",
        "  no_of_clusters = cluster_centers.shape[0]\n",
        "  maxRounds = int(cluster_centers.shape[1]/17)\n",
        "  df_cluster_centres = pd.DataFrame(cluster_centers)\n",
        "  cols = [card_types[i] + \"_R\" + str(r) for r in range(0, maxRounds)\n",
        "          for i in range(0, len(card_types))]\n",
        "  df_cluster_centres.columns = cols\n",
        "\n",
        "  trace_list = []\n",
        "  for n in range(0, no_of_clusters):\n",
        "    trace_list.append(pd.DataFrame(df_cluster_centres.iloc[n]).transpose())\n",
        "  cardcount_playtrace_comparison(outputfilename, trace_list, labels, card_types, legendOn, ylimit, output_to_screen, flip_clusters)\n",
        "\n",
        "#output a selection of cluster metrics. This includes:\n",
        "#1. Portion of traces in each cluster that have a given agent name\n",
        "#2. Portion of traces in a cluster that were from the player that moved first\n",
        "#3. Portfolio of traces in a cluster that won\n",
        "def ouput_cluster_metrics(outputfilename, traces, cluster_labels):\n",
        "  tmp_traces = traces.copy()\n",
        "  tmp_traces['Cluster'] = cluster_labels\n",
        "\n",
        "  #loop over attibutes to compute distributions\n",
        "  df_result = pd.DataFrame()\n",
        "  for att in ['AgentName', 'Player', 'Win']:\n",
        "    result = tmp_traces.groupby(['Cluster', att]).size().unstack().fillna(0)\n",
        "    results_percentage = result.div(result.sum(axis=1), axis=0) * 100\n",
        "    if att == 'AgentName':\n",
        "      df_result = results_percentage\n",
        "    else:\n",
        "      if att == 'Player':\n",
        "        results_percentage.columns = ['First Player', 'Second Player']\n",
        "      else:\n",
        "        if len(results_percentage.columns) == 2:\n",
        "          results_percentage.columns = ['Win', 'Loss']\n",
        "        else:\n",
        "          results_percentage.columns = ['Win', 'Draw', 'Loss']\n",
        "      df_result = df_result.join(results_percentage)\n",
        "\n",
        "  #output to file\n",
        "  df_result.to_csv(outputfilename + '.csv')\n",
        "\n",
        "#convert a list of ngram tuples into a list of strings, used in plotting function below\n",
        "def convert_ngram_tuples_to_strings(ngrams_list):\n",
        "  ngrams_str = []\n",
        "  for tuple_item in ngrams_list:\n",
        "      tuple_str = ''\n",
        "      for index, element in enumerate(tuple_item):\n",
        "          if index != (len(tuple_item)-1):\n",
        "              tuple_str += element + '|'\n",
        "          else:\n",
        "              tuple_str += element\n",
        "      ngrams_str.append(tuple_str)\n",
        "  return ngrams_str\n",
        "\n",
        "#function to plot N-Gram distributions side by side\n",
        "def plot_distribution_comparison(outputfilename, prob_dicts, labels, threshold = 0, flip_clusters = False):\n",
        "  #find a common domain where all probability values are greater than a given threshold\n",
        "  common_ngrams = []\n",
        "  for prob_dict in prob_dicts:\n",
        "    for key, value in prob_dict.items():\n",
        "      if (value > threshold) and (key not in common_ngrams):\n",
        "          common_ngrams.append(key)\n",
        "\n",
        "  #extract probability arrays for these common n-grams\n",
        "  prob_arrays = []\n",
        "  for prob_dict in prob_dicts:\n",
        "    prob_dict_reduced = {key: prob_dict[key] for key in common_ngrams}\n",
        "    prob_arrays.append(prob_dict_to_array(prob_dict_reduced))\n",
        "\n",
        "  if flip_clusters:\n",
        "    prob_arrays.reverse()\n",
        "\n",
        "  #next plot probability distributions\n",
        "\n",
        "  #need to convert common_ngrams into a list of strings as opposed to tuples containing strings\n",
        "  common_ngrams_str = convert_ngram_tuples_to_strings(common_ngrams)\n",
        "\n",
        "  #plot discrete probability distributions side by side\n",
        "\n",
        "  # Set the width of the bars\n",
        "  bar_width = 0.35\n",
        "\n",
        "  #set spacing between bars representing different n-grams\n",
        "  spacing = 0.1\n",
        "\n",
        "  no_distributions = len(prob_arrays)\n",
        "  no_n_grams = len(common_ngrams)\n",
        "  counter = 0\n",
        "  x_values = {}\n",
        "\n",
        "  #choose colors\n",
        "  hexadecimal_alphabets = '0123456789ABCDEF'\n",
        "  colors = [\"#\" + ''.join([random.choice(hexadecimal_alphabets) for j in\n",
        "  range(6)]) for i in range(len(prob_arrays))]\n",
        "  colors[0] = 'red'\n",
        "  if len(colors) >= 2:\n",
        "    colors[1] = 'blue'\n",
        "\n",
        "  for prob_array in prob_arrays:\n",
        "    x_values[counter] = np.zeros(no_n_grams)\n",
        "    if counter == 0:\n",
        "      x_values[counter][0] = spacing + bar_width/2.0\n",
        "      for i in range(1, len(prob_array)):\n",
        "        x_values[counter][i] = x_values[counter][0] + (bar_width * (no_distributions -1) + spacing + bar_width) * i\n",
        "    else:\n",
        "      for i in range(0, len(prob_array)):\n",
        "        x_values[counter][i] = x_values[counter-1][i] + bar_width\n",
        "    plt.bar(x_values[counter], prob_array, width=bar_width, label = labels[counter], color= colors[counter])\n",
        "    counter += 1\n",
        "\n",
        "  #for ticks place them in the middle of each set of distributions for single n-gram\n",
        "  x_ticks = np.zeros(no_n_grams)\n",
        "  for i in range(0, no_n_grams):\n",
        "    avg_coord = 0\n",
        "    for no_dists in range(0, no_distributions):\n",
        "      avg_coord += x_values[no_dists][i]\n",
        "    x_ticks[i] = avg_coord/no_distributions\n",
        "\n",
        "  plt.xticks(x_ticks, common_ngrams_str)\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.ylim(threshold)\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "\n",
        "  #output to file\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "#function to plot N-Gram distributions side by side, but with a vertical arrangement so that it is easier to read the N-Grams\n",
        "def plot_distribution_comparison_vert(outputfilename, prob_dicts, labels, threshold = 0, flip_clusters = False):\n",
        "  #find a common domain where all probability values are greater than a given threshold\n",
        "  common_ngrams = []\n",
        "  for prob_dict in prob_dicts:\n",
        "    for key, value in prob_dict.items():\n",
        "      if (value > threshold) and (key not in common_ngrams):\n",
        "          common_ngrams.append(key)\n",
        "\n",
        "  #extract probability arrays for these common n-grams\n",
        "  prob_arrays = []\n",
        "  for prob_dict in prob_dicts:\n",
        "    prob_dict_reduced = {key: prob_dict[key] for key in common_ngrams}\n",
        "    prob_arrays.append(prob_dict_to_array(prob_dict_reduced))\n",
        "\n",
        "  if flip_clusters:\n",
        "    prob_arrays.reverse()\n",
        "\n",
        "  #next plot probability distributions\n",
        "\n",
        "  #need to convert common_ngrams into a list of strings as opposed to tuples containing strings\n",
        "  common_ngrams_str = convert_ngram_tuples_to_strings(common_ngrams)\n",
        "\n",
        "  #plot discrete probability distributions side by side\n",
        "\n",
        "  # Set the width of the bars\n",
        "  bar_width = 0.35\n",
        "\n",
        "  #set spacing between bars representing different n-grams\n",
        "  spacing = 0.1\n",
        "\n",
        "  no_distributions = len(prob_arrays)\n",
        "  no_n_grams = len(common_ngrams)\n",
        "  y_values = {}\n",
        "\n",
        "  #choose colors\n",
        "  hexadecimal_alphabets = '0123456789ABCDEF'\n",
        "  colors = [\"#\" + ''.join([random.choice(hexadecimal_alphabets) for j in\n",
        "  range(6)]) for i in range(len(prob_arrays))]\n",
        "  colors[0] = 'red'\n",
        "  if len(colors) >= 2:\n",
        "    colors[1] = 'blue'\n",
        "\n",
        "  #set-up y coordinates for each bar\n",
        "  counter = 0\n",
        "  for prob_array in prob_arrays:\n",
        "    y_values[counter] = np.zeros(no_n_grams)\n",
        "    if counter == 0:\n",
        "      y_values[counter][0] = spacing + bar_width/2.0\n",
        "      for i in range(1, no_n_grams):\n",
        "        y_values[counter][i] = y_values[counter][0] + (bar_width * (no_distributions -1) + spacing + bar_width) * i\n",
        "    else:\n",
        "      for i in range(0, no_n_grams):\n",
        "        y_values[counter][i] = y_values[counter-1][i] + bar_width\n",
        "    counter += 1\n",
        "\n",
        "  #next plot bars\n",
        "  for i in range(0, no_n_grams):\n",
        "    #for each N-gram plot bars for each distribution\n",
        "    for j in range(0, counter):\n",
        "      plt.barh(y_values[j][i], prob_arrays[j][i] * 100, height =bar_width, align = 'center', color= colors[j])\n",
        "\n",
        "  #for ticks place them in the middle of each set of distributions for single n-gram\n",
        "  y_ticks = np.zeros(no_n_grams)\n",
        "  for i in range(0, no_n_grams):\n",
        "    avg_coord = 0\n",
        "    for no_dists in range(0, no_distributions):\n",
        "      avg_coord += y_values[no_dists][i]\n",
        "    y_ticks[i] = avg_coord/no_distributions\n",
        "\n",
        "  plt.yticks(y_ticks, common_ngrams_str)\n",
        "  plt.yticks(rotation=0)\n",
        "  plt.xlim(threshold)\n",
        "\n",
        "  # Set labels and title\n",
        "  plt.xlabel('Frequency (%)', fontsize = 14)\n",
        "  #ax.set_title('MDS Plot with Cluster Labels', fontsize = 25)\n",
        "  plt.xticks(fontsize=12)\n",
        "  plt.yticks(fontsize=12)\n",
        "\n",
        "  #set-up labels for each distribution\n",
        "  handles_list = []\n",
        "  for j in range(0, counter):\n",
        "    patch = mpatches.Patch(color= colors[j], label=labels[j])\n",
        "    handles_list.append(patch)\n",
        "\n",
        "  #add x-axis title\n",
        "  plt.xlabel('Frequency (%)')\n",
        "\n",
        "  #Add legend\n",
        "  plt.legend(handles= handles_list, loc = 'upper right', fontsize=10)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  #output to file\n",
        "  plt.savefig(outputfilename +'.png', format = 'png')\n",
        "  plt.close()\n",
        "\n",
        "#find the highest density data points in a data set - i.e. the point with the largest number of neighbours within a distance eps,\n",
        "#this is done by cluster. Note that the choice of eps that helps with clustering is not a good choice of eps\n",
        "#to find a high denisty point.\n",
        "def find_highest_density_datapts(dist_matrix, cluster_labels):\n",
        "  indices_by_cluster = {}\n",
        "  for cluster in np.unique(cluster_labels):\n",
        "    if cluster == -1:\n",
        "      #ignore noise\n",
        "      continue\n",
        "    else:\n",
        "      #now we can remove and index items based on this index\n",
        "      indices_to_remove = [i for i, value in enumerate(cluster_labels) if value != cluster]\n",
        "      cluster_dist_matrix = dist_matrix.drop(index=indices_to_remove, columns=indices_to_remove)\n",
        "\n",
        "      #in order to find the highest density points we set espilon to be twice the smallest distance\n",
        "      pos_distances = np.array(cluster_dist_matrix)\n",
        "      pos_distances = pos_distances[pos_distances != 0]\n",
        "      lower_dist = np.min(pos_distances)\n",
        "      eps = np.min(pos_distances) * 2 #this is an arbitrary choice\n",
        "\n",
        "      #create a NearestNeighbors model with the precomputed distance matrix\n",
        "      nn_model = NearestNeighbors(radius=eps, metric='precomputed')\n",
        "      nn_model.fit(cluster_dist_matrix)\n",
        "\n",
        "      # Find neighbors within epsilon for each data point\n",
        "      neighbors = nn_model.radius_neighbors(return_distance=False)\n",
        "\n",
        "      # Calculate the number of neighbors for each data point\n",
        "      num_neighbors = [len(neighbors_i) for neighbors_i in neighbors]\n",
        "\n",
        "      # Find the indices of the data point with the most neighbors\n",
        "      #note we need to take the argmax and compare to the revised index for our data set\n",
        "      max_neighbors_indices = np.argmax(num_neighbors)\n",
        "      revised_index = dist_matrix.index.drop(indices_to_remove)\n",
        "      final_max_neighbor_index = revised_index[max_neighbors_indices]\n",
        "\n",
        "      #note these indices are still wrt to original full dist matrix and trace list\n",
        "      indices_by_cluster[cluster] = final_max_neighbor_index\n",
        "\n",
        "  return indices_by_cluster\n",
        "\n",
        "#compute averages over playtraces for each cluster and determines the point in each cluster\n",
        "#closest to the mean. This is done in this function using the l_k_norm\n",
        "def closest_to_mean_l_k_norm(traces, cluster_labels, k):\n",
        "  indices_for_cluster_center = {}\n",
        "  cluster_means = {}\n",
        "  for cluster in np.unique(cluster_labels):\n",
        "    if cluster == -1:\n",
        "      #skip noise in DBSCAN case\n",
        "      continue\n",
        "    else:\n",
        "      # Select data points belonging to the current cluster\n",
        "      cluster_data = traces[cluster_labels == cluster]\n",
        "\n",
        "      # Compute mean vector for the cluster\n",
        "      cluster_mean = np.mean(cluster_data, axis=0)\n",
        "\n",
        "      # Find the index of the point closest to the mean in the cluster\n",
        "      cluster_data['dist_to_mean'] = cluster_data.apply(lambda row: l_k_norm(k, np.array(row), np.array(cluster_mean)), axis = 1)\n",
        "      indices_for_cluster_center[cluster] = cluster_data['dist_to_mean'].idxmin()\n",
        "      cluster_means[cluster] = cluster_mean\n",
        "  return indices_for_cluster_center, cluster_means\n",
        "\n",
        "#same as above except for Jensen-Shannon distance\n",
        "def closest_to_mean_js_norm(traces, cluster_labels):\n",
        "  indices_for_cluster_center = {}\n",
        "  cluster_means = {}\n",
        "  for cluster in np.unique(cluster_labels):\n",
        "    if cluster == -1:\n",
        "      #skip noise in DBSCAN case\n",
        "      continue\n",
        "    else:\n",
        "      # Select data points belonging to the current cluster\n",
        "      cluster_data = traces[cluster_labels == cluster]\n",
        "\n",
        "      # Compute mean vector for the cluster\n",
        "      cluster_mean = np.mean(cluster_data, axis=0) # is this still normalised? Should be....and also doesnt really matter\n",
        "\n",
        "      # Find the index of the point closest to the mean in the cluster\n",
        "      cluster_data['dist_to_mean'] = cluster_data.apply(lambda row: jensen_shannon_distance(np.array(row), np.array(cluster_mean)))\n",
        "      indices_for_cluster_center[cluster] = cluster_data['dist_to_mean'].idxmin()\n",
        "      cluster_means[cluster] = cluster_mean\n",
        "  return indices_for_cluster_center, cluster_means\n",
        "\n",
        "#alternative calculation to the above, used ot check that means were being calculated correctly\n",
        "def closest_to_mean_js_norm_v2(traces, cluster_labels, trace_col_name):\n",
        "  indices_for_cluster_center = {}\n",
        "  cluster_means = {}\n",
        "  traces_with_clusters = traces.copy()\n",
        "  traces_with_clusters['Cluster'] = cluster_labels\n",
        "  for cluster_label, group in traces_with_clusters.groupby('Cluster'):\n",
        "    if cluster_label == -1:\n",
        "      #skip noise in DBSCAN case\n",
        "      continue\n",
        "    else:\n",
        "      mean_point = group[trace_col_name].mean()\n",
        "      closest_point_index = group[trace_col_name].apply(lambda x: jensen_shannon_distance(np.array(x), np.array(mean_point))).idxmin()\n",
        "      indices_for_cluster_center[cluster_label] = closest_point_index\n",
        "      cluster_means[cluster_label] = mean_point\n",
        "  return indices_for_cluster_center, cluster_means\n",
        "\n",
        "#and another check.....\n",
        "def closest_to_mean_js_norm_v3(traces, cluster_labels, ngram_type):\n",
        "  indicies_for_closest_to_mean_dict = {}\n",
        "  closest_to_means_dict = {}\n",
        "  cluster_means_dict = {}\n",
        "\n",
        "  tmp_traces = traces.copy()\n",
        "  tmp_traces['Cluster'] = cluster_labels\n",
        "  tmp_traces = tmp_traces[['Cluster', 'ProbArray_' + str(ngram_type)]]\n",
        "  cluster_means = tmp_traces.groupby('Cluster').mean()\n",
        "\n",
        "  tmp_traces_slim = tmp_traces['ProbArray_' + str(ngram_type)]\n",
        "  for cluster in cluster_means.index:\n",
        "    mean = np.array(cluster_means['ProbArray_' + str(ngram_type)].loc[cluster])\n",
        "    tmp_traces['dist_to_cluster_mean_' + str(cluster)] = tmp_traces_slim.apply(lambda row: jensen_shannon_distance(np.array(row),mean))\n",
        "\n",
        "  for cluster in cluster_means.index:\n",
        "    if cluster == -1:\n",
        "      #skip noise in DBSCAN case\n",
        "      continue\n",
        "    else:\n",
        "      max_row_index = tmp_traces['dist_to_cluster_mean_' + str(cluster)].idxmin()\n",
        "      max_row = tmp_traces_slim.loc[max_row_index]\n",
        "\n",
        "      indicies_for_closest_to_mean_dict[cluster] = max_row_index\n",
        "      closest_to_means_dict[cluster] = max_row\n",
        "      cluster_means_dict[cluster] = np.array(cluster_means['ProbArray_' + str(ngram_type)].loc[cluster])\n",
        "  return indicies_for_closest_to_mean_dict, closest_to_means_dict, cluster_means_dict\n",
        "\n",
        "def mds_2d_cluster_plot(outputfilename, dist_matrix, cluster_labels, pts_closest_to_mean = {}, medoid_indices = {}, high_density_indices = {}, output_to_screen = False,\n",
        "                        flip_two_clusters = False):\n",
        "  # Metric Multi-dimensional Scaling (MDS)\n",
        "  mds = MDS(n_components=2, metric = True, dissimilarity='precomputed', random_state= 0 )\n",
        "  embeddings = mds.fit_transform(dist_matrix)\n",
        "  #pdb.set_trace()\n",
        "\n",
        "  # Create a scatter plot\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Define colors for each cluster label and high-density point\n",
        "  cluster_colors = {-1: 'black', 0: 'red', 1: 'blue', 2: 'green', 3: 'purple', 4: 'grey', 5: 'brown', 6: 'orange', 7: 'cyan', 8: 'magenta'}  # Add more colors if needed\n",
        "  density_color = 'teal'\n",
        "  means_color = 'yellow'\n",
        "  medoids_color = 'yellow'\n",
        "\n",
        "  #Q: We are using cluster labels and indices derived from original N-dimensional data set and applying to embeddings. Is this OK?\n",
        "\n",
        "  # Plot points with cluster labels\n",
        "  for cluster_label in np.unique(cluster_labels):\n",
        "    indices = cluster_labels == cluster_label\n",
        "    if cluster_label == -1:\n",
        "      marker_style = '+'\n",
        "      label_txt = 'Noise'\n",
        "    else:\n",
        "      marker_style = 'o'\n",
        "      label_txt = f'Cluster {cluster_label}'\n",
        "    if flip_two_clusters:\n",
        "      #typically we just need to reverse for two clusters\n",
        "      if cluster_label == 0:\n",
        "        cluster_colors[0] = 'blue'\n",
        "        label_txt = 'Cluster 1'\n",
        "      elif cluster_label == 1:\n",
        "        cluster_colors[1] = 'red'\n",
        "        label_txt = 'Cluster 0'\n",
        "    ax.scatter(embeddings[indices, 0], embeddings[indices, 1], c=cluster_colors[cluster_label], label= label_txt, marker= marker_style, s=50)\n",
        "\n",
        "  # Plot points with high density labels\n",
        "  if len(high_density_indices) > 0:\n",
        "    density_indices = list(high_density_indices.values())\n",
        "    ax.scatter(embeddings[density_indices, 0], embeddings[density_indices, 1], c=density_color, label='High Density Points', marker='x', s=50, edgecolors='black')\n",
        "\n",
        "  #plot points closest to mean\n",
        "  if len(pts_closest_to_mean) > 0:\n",
        "    mean_indices = list(pts_closest_to_mean.values())\n",
        "    ax.scatter(embeddings[mean_indices, 0], embeddings[mean_indices, 1], c=means_color, label='Cluster Means', marker='v', s=50, edgecolors='black')\n",
        "\n",
        "  #medoids\n",
        "  if len(medoid_indices) > 0:\n",
        "    medoid_pts = list(medoid_indices.values())\n",
        "    ax.scatter(embeddings[medoid_pts, 0], embeddings[medoid_pts, 1], c=medoids_color, label='Cluster Medoids', marker='v', s=50, edgecolors='black')\n",
        "\n",
        "  # Set labels and title\n",
        "  ax.set_xlabel('MDS Dimension 1', fontsize = 16)\n",
        "  ax.set_ylabel('MDS Dimension 2', fontsize = 16)\n",
        "  #ax.set_title('MDS Plot with Cluster Labels', fontsize = 25)\n",
        "  plt.xticks(fontsize=8)\n",
        "  plt.yticks(fontsize=8)\n",
        "  ax.legend(fontsize=12)\n",
        "\n",
        "  #output to file\n",
        "  plt.tight_layout()\n",
        "  #plt.ticklabel_format(style='plain')\n",
        "  if output_to_screen:\n",
        "    plt.show()\n",
        "  else:\n",
        "    plt.savefig(outputfilename +'.png', format = 'png')\n",
        "    plt.close()\n",
        "\n",
        "  #calculate normalised stress\n",
        "  normalisation_factor = np.sum(np.sum(np.square(dist_matrix)))\n",
        "  normalised_stress = np.sqrt(mds.stress_/normalisation_factor)\n",
        "\n",
        "  #also return the stress value from the MDS algorithm for reference\n",
        "  return normalised_stress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "DmLapEm9JbMU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmLapEm9JbMU",
        "outputId": "d042da87-0000-40f9-8d1d-2b1482aa8a8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7de2fd259480>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "#We switch off interactive mode for matplotlib as plots will be output to file\n",
        "plt.ioff()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "Ya_QVuU6LZ0O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya_QVuU6LZ0O",
        "outputId": "b4ea2829-d602-47ea-b470-2537e3ea8e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming K-means clustering for card count playtraces using euclidean norm...\n"
          ]
        }
      ],
      "source": [
        "#perfom K-means clustering. We only do this for card-count playtraces due to restriction on using Euclidean playtraces\n",
        "if run_config_cardcount['KMeans']:\n",
        "  print(\"Perfoming K-means clustering for card count playtraces using euclidean norm...\")\n",
        "  sil_avg = {}\n",
        "  inertia = {}\n",
        "  stress = {}\n",
        "  key_norm = 'CardCount_lknorm_2'\n",
        "  for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "    inertia[n_clusters], cluster_centres, cluster_labels = sa_kmeans(traces_cardcount_slim, n_clusters)\n",
        "    sil_avg[n_clusters] = silhouette_score(traces_cardcount_slim, cluster_labels)\n",
        "    sil_coeffs = silhouette_samples(traces_cardcount_slim, cluster_labels)\n",
        "\n",
        "    #output silhouette plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMeans/' + 'silhouette_plot_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    output_silhouette_plot(outputfilename, sil_coeffs, sil_avg[n_clusters], cluster_labels, flip_clusters = False)\n",
        "\n",
        "    #output cluster centroids\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMeans/'  + 'cluster_centroids_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    labels = ['Cluster ' + str(n) +' Centroid' for n in range(0, n_clusters)]\n",
        "    plot_cluster_centroids(outputfilename, cluster_centres, card_types, labels, legendOn = True, ylimit = 0, output_to_screen = False, flip_clusters = True)\n",
        "\n",
        "    #output cluster metrics\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMeans/' + 'cluster_metrics_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    ouput_cluster_metrics(outputfilename, traces_cardcount, cluster_labels)\n",
        "\n",
        "    #output MDS plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames +  '/Results_KMeans/' + 'MDS_plot_KMeans_CardCount_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "    closest_to_mean_indices, _ = closest_to_mean_l_k_norm(traces_cardcount_slim, cluster_labels, 2)\n",
        "    stress[n_clusters] = mds_2d_cluster_plot(outputfilename, dist_matrices[key_norm], cluster_labels, pts_closest_to_mean = {}, medoid_indices = {}, high_density_indices = {}, output_to_screen = False,\n",
        "                          flip_two_clusters = False)\n",
        "\n",
        "  #output silhouette averages to file\n",
        "  outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames +  '/Results_KMeans/' + 'silhouette_averages_KMeans_CardCount_' + tag_for_dir_and_filenames\n",
        "  output_dictionary(outputfilename, sil_avg)\n",
        "\n",
        "  #output stress values to file\n",
        "  outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames +  '/Results_KMeans/' + 'stress_values_from_MDS_KMeans_CardCount_' + tag_for_dir_and_filenames\n",
        "  output_dictionary(outputfilename, stress)\n",
        "\n",
        "  #output a scaled inertia value plot\n",
        "  outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMeans/' + 'inertia_plot_KMeans_CardCount_' + tag_for_dir_and_filenames\n",
        "  scalar, _, _ = sa_kmeans(traces_cardcount_slim, 1)\n",
        "  output_inertia_plot(outputfilename, inertia, scalar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "KLvqSxnkLQ1B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLvqSxnkLQ1B",
        "outputId": "663ecc24-12ed-4ddc-eb27-c9f711ed9f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming K-medoids clustering for card count playtraces using l_k-norm...\n"
          ]
        }
      ],
      "source": [
        "#perform K-medoids clustering for card count playtraces.\n",
        "if run_config_cardcount['KMedoids']:\n",
        "  print(\"Perfoming K-medoids clustering for card count playtraces using l_k-norm...\")\n",
        "\n",
        "  for k in k_norms:\n",
        "    key_norm = 'CardCount_lknorm_' + str(k)\n",
        "    sil_avg = {}\n",
        "    inertia = {}\n",
        "    stress = {}\n",
        "    for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "      inertia[n_clusters], cluster_indices, cluster_labels = sa_kmedoids(dist_matrices[key_norm], n_clusters)\n",
        "      sil_avg[n_clusters] = silhouette_score(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "      sil_coeffs = silhouette_samples(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "\n",
        "      #output silhouette plots\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMedoids/' + 'silhouette_plot_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      output_silhouette_plot(outputfilename, sil_coeffs, sil_avg[n_clusters], cluster_labels, flip_clusters = True)\n",
        "\n",
        "      #output cluster centroids, converting indices to playtraces\n",
        "      cluster_centres = traces_cardcount_slim.iloc[cluster_indices]\n",
        "      #labels = ['Cluster ' + str(n) +' Centroid' for n in range(0, n_clusters)]\n",
        "      labels = ['Cluster ' + str(n) for n in range(0, n_clusters)]\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMedoids/' + 'cluster_centroids_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      plot_cluster_centroids(outputfilename, cluster_centres, card_types, labels, legendOn = True, ylimit = 0, output_to_screen = False, flip_clusters = True)\n",
        "\n",
        "      #output cluster metrics\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMedoids/' + 'cluster_metrics_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      ouput_cluster_metrics(outputfilename, traces_cardcount, cluster_labels)\n",
        "\n",
        "      #output MDS plots\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMedoids/' + 'MDS_plot_KMedoids_CardCount_N_' + str(n_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      medoids = {i: value for i, value in enumerate(cluster_indices)}\n",
        "      stress[n_clusters] = mds_2d_cluster_plot(outputfilename, dist_matrices[key_norm], cluster_labels, pts_closest_to_mean = {}, medoid_indices = {}, high_density_indices = {}, output_to_screen = False,\n",
        "                          flip_two_clusters = True)\n",
        "\n",
        "    #output silhouette averages to file\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMedoids/' + 'silhouette_averages_KMedoids_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    output_dictionary(outputfilename, sil_avg)\n",
        "\n",
        "    #output a scaled inertia value plot\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMedoids/' + 'inertia_plot_KMedoids_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    scalar, _, _ = sa_kmedoids(dist_matrices[key_norm], 1)\n",
        "    output_inertia_plot(outputfilename, inertia, scalar)\n",
        "\n",
        "    #output stress values to file\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMedoids/' + 'stress_values_from_MDS_KMedoids_CardCount' + tag_for_dir_and_filenames\n",
        "    output_dictionary(outputfilename, stress)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "QxE9HSXDc5Ne",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxE9HSXDc5Ne",
        "outputId": "2c1968ac-90d7-4588-e908-70c64688ce2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming K-medoids clustering for N-Gram playtraces using Jensen-Shannon norm...\n"
          ]
        }
      ],
      "source": [
        "#perform K-medoids clustering for N-Gram playtraces.\n",
        "if run_config_ngrams['KMedoids']:\n",
        "  print(\"Perfoming K-medoids clustering for N-Gram playtraces using Jensen-Shannon norm...\")\n",
        "\n",
        "  for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  #for n in [2]:\n",
        "    key_gram = 'N_Gram_' + str(n)\n",
        "    sil_avg = {}\n",
        "    inertia = {}\n",
        "    stress = {}\n",
        "    for n_clusters in range(clusters_min, clusters_max + 1, clusters_stepsize):\n",
        "      inertia[n_clusters], cluster_indices, cluster_labels = sa_kmedoids(dist_matrices[key_gram], n_clusters)\n",
        "      sil_avg[n_clusters] = silhouette_score(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "      sil_coeffs = silhouette_samples(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "\n",
        "      #output silhouette plots\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMedoids/' + 'silhouette_plot_KMedoids_NGram_' + str(n) + '_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "      output_silhouette_plot(outputfilename, sil_coeffs, sil_avg[n_clusters], cluster_labels, flip_clusters = False)\n",
        "\n",
        "      #output cluster centroids (probability distributions in this case)\n",
        "      col_name = 'ProbDict_' + str(n)\n",
        "      cluster_centres = traces_ngrams[col_name].iloc[cluster_indices]\n",
        "      labels = ['Cluster ' + str(n) for n in range(0, n_clusters)]\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMedoids/' + 'cluster_centroids_KMedoids_NGram_' + str(n) + '_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "      plot_distribution_comparison_vert(outputfilename, cluster_centres, labels, thresholds[n], flip_clusters = True)\n",
        "      #plot_distribution_comparison_vert(outputfilename, cluster_centres, labels, 0.02, flip_clusters = True)\n",
        "\n",
        "      #output MDS plots\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMedoids/' + 'MDS_plot_KMedoids_NGram_' + str(n) + '_N_' + str(n_clusters) + '_' + tag_for_dir_and_filenames\n",
        "      medoids = {i: value for i, value in enumerate(cluster_indices)}\n",
        "      stress[n_clusters] = mds_2d_cluster_plot(outputfilename, dist_matrices[key_gram], cluster_labels, pts_closest_to_mean = {}, medoid_indices = {}, high_density_indices = {}, output_to_screen = False,\n",
        "                          flip_two_clusters = True)\n",
        "\n",
        "    #output silhouette averages to file\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMedoids/' + 'silhouette_averages_KMedoids_NGram_' + str(n) + '_' + tag_for_dir_and_filenames\n",
        "    output_dictionary(outputfilename, sil_avg)\n",
        "\n",
        "    #output a scaled inertia value plot\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMedoids/' + 'inertia_plot_KMedoids_NGram_' + str(n) + '_' + tag_for_dir_and_filenames\n",
        "    scalar, _, _ = sa_kmedoids(dist_matrices[key_gram], 1)\n",
        "    output_inertia_plot(outputfilename, inertia, scalar)\n",
        "\n",
        "    #output stress values to file\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_KMedoids/' + 'stress_values_from_MDS_KMedoids_CardCount' + tag_for_dir_and_filenames\n",
        "    output_dictionary(outputfilename, stress)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "WRhHC9hyzHA7",
      "metadata": {
        "id": "WRhHC9hyzHA7"
      },
      "outputs": [],
      "source": [
        "#cluster_centres.iloc[0]\n",
        "#filter on non-zero entries and N-Grams that contain a particular action\n",
        "#action = 'GAINCURSE'\n",
        "#tmp = {key: value for key, value in cluster_centres.iloc[0].items() if value > 0}\n",
        "#for gram, prob in tmp.items():\n",
        "#  if gram[0] == action or gram[1] == action or gram[2] == action:\n",
        "#    print(\"N-Gram: \" + str(gram) + \" Prob: \" + str(prob))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "lrVCiE9W75Ok",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrVCiE9W75Ok",
        "outputId": "f06d6533-3d76-47c3-8ad3-aaf9136b745b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming DBSCAN clustering for card count playtraces using l_k-norm...\n"
          ]
        }
      ],
      "source": [
        "#perform DBSCAN clustering for card count playtraces using l_k-norm\n",
        "if run_config_cardcount['DBSCAN']:\n",
        "  print(\"Perfoming DBSCAN clustering for card count playtraces using l_k-norm...\")\n",
        "\n",
        "  for k in k_norms:\n",
        "  #for k in [2]:\n",
        "    key_norm = 'CardCount_lknorm_' + str(k)\n",
        "    #note that for DBSCAN, inertia is not a valid metric (dependent on spherical clusters), so we look for the\n",
        "    #best silhouette average and just output this result.\n",
        "    best_sil_avg_with_noise = -10000\n",
        "    best_sil_avg_no_noise = -10000\n",
        "    best_sil_coeffs_with_noise = None\n",
        "    best_sil_coeffs_no_noise = None\n",
        "    best_minpts = 0\n",
        "    best_epsilon = 0\n",
        "    best_cluster_labels_with_noise = None\n",
        "    best_cluster_labels_no_noise = None\n",
        "    best_noise_ratio = 0 #this is the noise ratio in the case with the highest silhouette average\n",
        "    best_no_clusters = 0\n",
        "    for minpts in range(minpts_min, minpts_max, minpts_stepsize):\n",
        "      for eps in np.arange(epsilon_min, epsilon_max, epsilon_stepsize):\n",
        "        cluster_labels_with_noise = sa_dbscan(dist_matrices_normalised[key_norm], minpts, eps)\n",
        "        #in DBSCAN anything with a label of '-1' is treated as noise\n",
        "        #so we need to do the following:\n",
        "        #1. Keep a record of the portion of traces that are classified as noise, too many and the results should be ignored\n",
        "        #2. Compute silhouette averages with and without noise present\n",
        "        noise_ratio = np.sum(cluster_labels_with_noise == -1)/len(traces_cardcount_slim)\n",
        "        cluster_labels_no_noise = cluster_labels_with_noise[cluster_labels_with_noise > -1]\n",
        "        indices_to_remove = [i for i, value in enumerate(cluster_labels_with_noise) if value == -1]\n",
        "        dist_matrix_no_noise = dist_matrices_normalised[key_norm].drop(index=indices_to_remove, columns=indices_to_remove)\n",
        "        no_clusters_no_noise = len(np.unique(cluster_labels_no_noise))\n",
        "        no_clusters_with_noise = len(np.unique(cluster_labels_with_noise))\n",
        "        if (no_clusters_with_noise == 1) and (no_clusters_no_noise == 0) and best_no_clusters == 0:\n",
        "          #in this case everything is labelled as noise and we cannot compute a silhouette score\n",
        "          best_no_clusters = 0\n",
        "          best_cluster_labels_with_noise = cluster_labels_with_noise\n",
        "        elif (no_clusters_no_noise == 1) and (best_no_clusters <=1):\n",
        "          #in this case we have one cluster with potentially some noise\n",
        "          best_no_clusters = 1\n",
        "          #in this case we can compute the silhouette average between the one single cluster and the noise (if it exists) and use that to\n",
        "          #optimise when we kind find only a single cluster\n",
        "          if no_clusters_with_noise == (no_clusters_no_noise + 1):\n",
        "            sil_avg_with_noise = silhouette_score(dist_matrices_normalised[key_norm], cluster_labels_with_noise, metric = 'precomputed')\n",
        "            if(sil_avg_with_noise > best_sil_avg_with_noise):\n",
        "              best_sil_avg_with_noise = sil_avg_with_noise\n",
        "              best_sil_coeffs_with_noise = silhouette_samples(dist_matrices_normalised[key_norm], cluster_labels_with_noise, metric = 'precomputed')\n",
        "              best_minpts = minpts\n",
        "              best_epsilon = eps\n",
        "              best_cluster_labels_with_noise = cluster_labels_with_noise\n",
        "              best_cluster_labels_no_noise = cluster_labels_no_noise\n",
        "              best_noise_ratio = noise_ratio\n",
        "          else:\n",
        "            #what happens if we don't have any noise and a single cluster, in this case we cant calculate silhouette coefficients and we just keep the labels\n",
        "            if (best_sil_avg_with_noise < 0):\n",
        "              best_cluster_labels_with_noise = cluster_labels_with_noise\n",
        "        elif no_clusters_no_noise >=2:\n",
        "          #in this case we have at least two clusters not identified as noise\n",
        "          #note, cluster results make more sense when we include noise in our optimisation condition\n",
        "          sil_avg_with_noise = silhouette_score(dist_matrices_normalised[key_norm], cluster_labels_with_noise, metric = 'precomputed')\n",
        "          if (sil_avg_with_noise > best_sil_avg_with_noise):\n",
        "            best_sil_avg_with_noise = sil_avg_with_noise\n",
        "            best_sil_coeffs_with_noise = silhouette_samples(dist_matrices_normalised[key_norm], cluster_labels_with_noise, metric = 'precomputed')\n",
        "            best_sil_avg_no_noise = silhouette_score(dist_matrix_no_noise, cluster_labels_no_noise, metric = 'precomputed')\n",
        "            best_sil_coeffs_no_noise = silhouette_samples(dist_matrix_no_noise, cluster_labels_no_noise, metric = 'precomputed')\n",
        "            best_minpts = minpts\n",
        "            best_epsilon = eps\n",
        "            best_cluster_labels_with_noise = cluster_labels_with_noise\n",
        "            best_cluster_labels_no_noise = cluster_labels_no_noise\n",
        "            best_noise_ratio = noise_ratio\n",
        "            best_no_clusters = len(np.unique(cluster_labels_no_noise))\n",
        "        else:\n",
        "          #for this case we have found something less interesting than on a previous iteration of our loops and we do nothing\n",
        "          pass\n",
        "\n",
        "    #output silhouette plots\n",
        "    if (best_no_clusters >=1) and (best_sil_avg_with_noise > 0):\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_DBSCAN/' + 'silhouette_plot_with_noise_DBSCAN_CardCount_eps_' + str(round(best_epsilon,2)) + '_minpts_' + str(best_minpts) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      output_silhouette_plot(outputfilename, best_sil_coeffs_with_noise, best_sil_avg_with_noise, best_cluster_labels_with_noise, flip_clusters = True)\n",
        "    if (best_no_clusters >=2) and (best_sil_avg_with_noise > 0):\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_DBSCAN/' + 'silhouette_plot_no_noise_DBSCAN_CardCount_eps_' + str(round(best_epsilon,2)) + '_minpts_' + str(best_minpts) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      output_silhouette_plot(outputfilename, best_sil_coeffs_no_noise, best_sil_avg_no_noise, best_cluster_labels_no_noise, flip_clusters = True)\n",
        "\n",
        "    #output MDS plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_DBSCAN/' + 'MDS_plot_DBSCAN_CardCount_eps_' + str(round(best_epsilon,2)) + '_minpts_' + str(best_minpts) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    if (best_no_clusters >=1):\n",
        "      closest_to_mean_indices, _ = closest_to_mean_l_k_norm(traces_cardcount_slim, best_cluster_labels_with_noise, k)\n",
        "    else:\n",
        "      closest_to_mean_indices = {}\n",
        "    stress = mds_2d_cluster_plot(outputfilename, dist_matrices[key_norm], best_cluster_labels_with_noise, output_to_screen = False, flip_two_clusters = True)\n",
        "\n",
        "    #output cluster centroids, converting indices to playtraces\n",
        "    if (best_no_clusters >=1):\n",
        "      closest_to_mean_indices_arr = [value for value in closest_to_mean_indices.values()]\n",
        "      #cluster_centres = traces_cardcount_slim.iloc[closest_to_mean_indices_arr]\n",
        "      cluster_centres = traces_cardcount_slim.loc[closest_to_mean_indices_arr]\n",
        "      labels = ['Cluster ' + str(n) for n in range(0, len(closest_to_mean_indices_arr))]\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_DBSCAN/' + 'cluster_centroids_DBSCAN_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      plot_cluster_centroids(outputfilename, cluster_centres, card_types, labels, flip_clusters = True)\n",
        "\n",
        "    #output values\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_DBSCAN/' + 'best_silhouette_avg_DBSCAN_CardCount' + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "    values = [best_sil_avg_no_noise, best_sil_avg_with_noise, best_minpts, best_epsilon, best_no_clusters, best_noise_ratio, stress]\n",
        "    valueslabels_list = ['sil_avg_no_noise', 'sil_avg_with_noise','minpts', 'epsilon', 'clusters', 'noise ratio', 'MDS_stress']\n",
        "    output_values(outputfilename, values, valueslabels_list)\n",
        "\n",
        "    if best_no_clusters == 0:\n",
        "      print(\"Only noise found for DBSCAN for card count playtraces with k=\" + str(k))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "uerIDa81inOj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "uerIDa81inOj",
        "outputId": "cca0bcf5-3b54-4cb5-bc89-bf9c983b3bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfoming DBSCAN clustering for N-Gram playtraces using Jensen-Shannon norm...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "positional indexers are out-of-bounds",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1617\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1618\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1619\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m    970\u001b[0m         \"\"\"\n\u001b[0;32m--> 971\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, allow_fill, fill_value, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m             taken = algos.take(\n\u001b[0m\u001b[1;32m   1091\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_na_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(arr, indices, axis, allow_fill, fill_value)\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0;31m# NumPy style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 974 is out of bounds for axis 0 with size 966",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-9498112457c1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mcol_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ProbDict_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mclosest_to_mean_indices_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclosest_to_mean_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m       \u001b[0mcluster_centres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraces_ngrams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclosest_to_mean_indices_arr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Cluster '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosest_to_mean_indices_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0moutputfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle_drive_parent_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Results/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtag_for_dir_and_filenames\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/Results_DBSCAN/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'cluster_centroids_DBSCAN_NGram_N_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtag_for_dir_and_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m         \u001b[0;31m# a single integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m             \u001b[0;31m# re-raise with different error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
          ]
        }
      ],
      "source": [
        "if run_config_ngrams['DBSCAN']:\n",
        "  print(\"Perfoming DBSCAN clustering for N-Gram playtraces using Jensen-Shannon norm...\")\n",
        "\n",
        "  for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "  #for n in [2]:\n",
        "    key_gram = 'N_Gram_' + str(n)\n",
        "    best_sil_avg_with_noise = -10000\n",
        "    best_sil_avg_no_noise = -10000\n",
        "    best_sil_coeffs_with_noise = None\n",
        "    best_sil_coeffs_no_noise = None\n",
        "    best_minpts = 0\n",
        "    best_epsilon = 0\n",
        "    best_cluster_labels_with_noise = None\n",
        "    best_cluster_labels_no_noise = None\n",
        "    best_noise_ratio = 0 #this is the noise ratio in the case with the highest silhouette average\n",
        "    best_no_clusters = 0\n",
        "    for minpts in range(minpts_min, minpts_max, minpts_stepsize):\n",
        "      for eps in np.arange(epsilon_min, epsilon_max, epsilon_stepsize):\n",
        "        cluster_labels_with_noise = sa_dbscan(dist_matrices[key_gram], minpts, eps)\n",
        "        #in DBSCAN anything with a label of '-1' is treated as noise\n",
        "        #so we need to do the following:\n",
        "        #1. Keep a record of the portion of traces that are classified as noise, too many and the results should be ignored\n",
        "        #2. Compute silhouette averages with and without noise present\n",
        "        noise_ratio = np.sum(cluster_labels_with_noise == -1)/len(traces_cardcount_slim)\n",
        "        cluster_labels_no_noise = cluster_labels_with_noise[cluster_labels_with_noise > -1]\n",
        "        indices_to_remove = [i for i, value in enumerate(cluster_labels_with_noise) if value == -1]\n",
        "        dist_matrix_no_noise = dist_matrices[key_gram].drop(index=indices_to_remove, columns=indices_to_remove)\n",
        "        no_clusters_no_noise = len(np.unique(cluster_labels_no_noise))\n",
        "        no_clusters_with_noise = len(np.unique(cluster_labels_with_noise))\n",
        "        if (no_clusters_with_noise == 1) and (no_clusters_no_noise == 0) and best_no_clusters == 0:\n",
        "          #in this case everything is labelled as noise and we cannot compute a silhouette score\n",
        "          best_no_clusters = 0\n",
        "          best_cluster_labels_with_noise = cluster_labels_with_noise\n",
        "        elif (no_clusters_no_noise == 1) and (best_no_clusters <=1):\n",
        "          #in this case we have one cluster with potentially some noise\n",
        "          best_no_clusters = 1\n",
        "          #in this case we can compute the silhouette average between the one single cluster and the noise (if it exists) and use that to\n",
        "          #optimise when we kind find only a single cluster\n",
        "          if no_clusters_with_noise == (no_clusters_no_noise + 1):\n",
        "            sil_avg_with_noise = silhouette_score(dist_matrices[key_gram], cluster_labels_with_noise, metric = 'precomputed')\n",
        "            if(sil_avg_with_noise > best_sil_avg_with_noise):\n",
        "              best_sil_avg_with_noise = sil_avg_with_noise\n",
        "              best_sil_coeffs_with_noise = silhouette_samples(dist_matrices[key_gram], cluster_labels_with_noise, metric = 'precomputed')\n",
        "              best_minpts = minpts\n",
        "              best_epsilon = eps\n",
        "              best_cluster_labels_with_noise = cluster_labels_with_noise\n",
        "              best_cluster_labels_no_noise = cluster_labels_no_noise\n",
        "              best_noise_ratio = noise_ratio\n",
        "          else:\n",
        "            #what happens if we don't have any noise and a single cluster, in this case we cant calculate silhouette coefficients and we just keep the labels\n",
        "            if (best_sil_avg_with_noise < 0):\n",
        "              best_cluster_labels_with_noise = cluster_labels_with_noise\n",
        "        elif no_clusters_no_noise >=2:\n",
        "          #in this case we have at least two clusters not identified as noise\n",
        "          #note, cluster results make more sense when we include noise in our optimisation condition\n",
        "          sil_avg_with_noise = silhouette_score(dist_matrices[key_gram], cluster_labels_with_noise, metric = 'precomputed')\n",
        "          if (sil_avg_with_noise > best_sil_avg_with_noise):\n",
        "            best_sil_avg_with_noise = sil_avg_with_noise\n",
        "            best_sil_coeffs_with_noise = silhouette_samples(dist_matrices[key_gram], cluster_labels_with_noise, metric = 'precomputed')\n",
        "            best_sil_avg_no_noise = silhouette_score(dist_matrix_no_noise, cluster_labels_no_noise, metric = 'precomputed')\n",
        "            best_sil_coeffs_no_noise = silhouette_samples(dist_matrix_no_noise, cluster_labels_no_noise, metric = 'precomputed')\n",
        "            best_minpts = minpts\n",
        "            best_epsilon = eps\n",
        "            best_cluster_labels_with_noise = cluster_labels_with_noise\n",
        "            best_cluster_labels_no_noise = cluster_labels_no_noise\n",
        "            best_noise_ratio = noise_ratio\n",
        "            best_no_clusters = len(np.unique(cluster_labels_no_noise))\n",
        "        else:\n",
        "          #for this case we have found something less interesting than on a previous iteration of our loops and we do nothing\n",
        "          pass\n",
        "\n",
        "    if (best_no_clusters >=1):\n",
        "      #output silhouette plots\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_DBSCAN/' + 'silhouette_plot_with_noise_DBSCAN_NGram_N_' + str(n) + '_eps_' + str(round(best_epsilon,2)) + '_minpts_' + str(best_minpts) + '_' + tag_for_dir_and_filenames\n",
        "      output_silhouette_plot(outputfilename, best_sil_coeffs_with_noise, best_sil_avg_with_noise, best_cluster_labels_with_noise, flip_clusters = False)\n",
        "    if (best_no_clusters >=2):\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_DBSCAN/' + 'silhouette_plot_no_noise_DBSCAN_NGram_N_' + str(n) + '_eps_' + str(round(best_epsilon,2)) + '_minpts_' + str(best_minpts) + '_' + tag_for_dir_and_filenames\n",
        "      output_silhouette_plot(outputfilename, best_sil_coeffs_no_noise, best_sil_avg_no_noise, best_cluster_labels_no_noise, flip_clusters = False)\n",
        "\n",
        "    #output MDS plots\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_DBSCAN/' + 'MDS_plot_DBSCAN_NGram_N_' + str(n) + '_eps_' + str(round(best_epsilon,2)) + '_minpts_' + str(best_minpts) + '_' + tag_for_dir_and_filenames\n",
        "    if (best_no_clusters >=1):\n",
        "      closest_to_mean_indices, _ = closest_to_mean_js_norm(traces_ngrams['ProbArray_' + str(n)], best_cluster_labels_with_noise)\n",
        "    else:\n",
        "      closest_to_mean_indices = {}\n",
        "    stress = mds_2d_cluster_plot(outputfilename, dist_matrices[key_gram], best_cluster_labels_with_noise, output_to_screen = False, flip_two_clusters = False)\n",
        "\n",
        "    #output values\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_DBSCAN/' + 'best_silhouette_avg_DBSCAN_NGram_N_' + str(n) + '_' + tag_for_dir_and_filenames\n",
        "    values = [best_sil_avg_no_noise, best_sil_avg_with_noise, best_minpts, best_epsilon, best_noise_ratio, stress]\n",
        "    valueslabels_list = ['sil_avg_no_noise', 'sil_avg_with_noise', 'minpts', 'epsilon', 'noise ratio', 'MDS_stress']\n",
        "    output_values(outputfilename, values, valueslabels_list)\n",
        "\n",
        "    #output cluster centroids (probability distributions in this case)\n",
        "    if (best_no_clusters >=1):\n",
        "      col_name = 'ProbDict_' + str(n)\n",
        "      closest_to_mean_indices_arr = [value for value in closest_to_mean_indices.values()]\n",
        "      #cluster_centres = traces_ngrams[col_name].iloc[closest_to_mean_indices_arr]\n",
        "      cluster_centres = traces_ngrams[col_name].loc[closest_to_mean_indices_arr]\n",
        "      labels = ['Cluster ' + str(n) for n in range(0, len(closest_to_mean_indices_arr))]\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_DBSCAN/' + 'cluster_centroids_DBSCAN_NGram_N_' + str(n)  + '_' + tag_for_dir_and_filenames\n",
        "      plot_distribution_comparison_vert(outputfilename, cluster_centres, labels, thresholds[n], flip_clusters = True)\n",
        "      #plot_distribution_comparison_vert(outputfilename, cluster_centres, labels, 0.025, flip_clusters = True)\n",
        "\n",
        "    if (best_no_clusters == 0):\n",
        "      print(\"Only noise found for DBSCAN for N-gram playtraces with N=\" + str(n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WBK7CVg0mKmg",
      "metadata": {
        "id": "WBK7CVg0mKmg"
      },
      "outputs": [],
      "source": [
        "if run_config_cardcount['SP_FullyConnected']:\n",
        "  print(\"Performing spectral clustering for card count playtraces using fully connected graphs with l_k-norm...\")\n",
        "\n",
        "  for k in k_norms:\n",
        "    key_norm = 'CardCount_lknorm_' + str(k)\n",
        "    best_sil_avg = -10000\n",
        "    best_sil_coeffs = None\n",
        "    best_gamma = 0\n",
        "    best_no_clusters = 0\n",
        "    best_cluster_labels = None\n",
        "    for no_clusters in range(clusters_min, clusters_max, clusters_stepsize):\n",
        "      for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "        key_gamma =  'gamma_' + str(gamma)\n",
        "        cluster_labels = sa_spectral_clustering_AM(connected_affinity_matrices[key_norm][key_gamma], no_clusters)\n",
        "        sil_avg = silhouette_score(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "        if sil_avg > best_sil_avg:\n",
        "          best_sil_avg = sil_avg\n",
        "          best_sil_coeffs = silhouette_samples(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "          best_gamma = gamma\n",
        "          best_no_clusters = no_clusters\n",
        "          best_cluster_labels = cluster_labels\n",
        "\n",
        "    #output silhouette plots\n",
        "    if (best_sil_avg > 0):\n",
        "      #output silhouette plots\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'silhouette_plot_SPCluster_AM_CardCount_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "      #output MDS plots\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'MDS_plot_SPCluster_AM_CardCount_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      closest_to_mean_indices, _ = closest_to_mean_l_k_norm(traces_cardcount_slim, best_cluster_labels, k)\n",
        "      stress = mds_2d_cluster_plot(outputfilename, dist_matrices[key_norm], best_cluster_labels, closest_to_mean_indices)\n",
        "\n",
        "      #output cluster centroids, converting indices to playtraces\n",
        "      closest_to_mean_indices_arr = [value for value in closest_to_mean_indices.values()]\n",
        "      #cluster_centres = traces_cardcount_slim.iloc[closest_to_mean_indices_arr]\n",
        "      cluster_centres = traces_cardcount_slim.loc[closest_to_mean_indices_arr]\n",
        "      labels = ['Cluster ' + str(n) for n in range(0, len(closest_to_mean_indices_arr))]\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'cluster_centroids_SPCluster_AM_CardCount_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      plot_cluster_centroids(outputfilename, cluster_centres, card_types, labels)\n",
        "\n",
        "      #output values\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'best_silhouette_avg_SPCluster_AM_CardCount_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      values = [best_sil_avg, best_gamma, best_no_clusters, stress]\n",
        "      valueslabels_list = ['sil_avg', 'gamma', 'clusters', 'MDS_stress']\n",
        "      output_values(outputfilename, values, valueslabels_list)\n",
        "    else:\n",
        "      print(\"No clustering found for Spectral Clustering with fully connected affinity matrix for card count playtraces with l_\" + str(k) + \"-norm\")\n",
        "      #TODO: Anything else we can do here?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-6kOLd7d5Ts_",
      "metadata": {
        "id": "-6kOLd7d5Ts_"
      },
      "outputs": [],
      "source": [
        "if run_config_ngrams['SP_FullyConnected']:\n",
        "  print(\"Performing spectral clustering for N-Gram playtraces using fully connected graphs with Jensen-Shannon distance metric...\")\n",
        "\n",
        "  for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "    key_gram = 'N_Gram_' + str(n)\n",
        "    best_sil_avg = -10000\n",
        "    best_sil_coeffs = None\n",
        "    best_gamma = 0\n",
        "    best_no_clusters = 0\n",
        "    best_cluster_labels = None\n",
        "    for no_clusters in range(clusters_min, clusters_max, clusters_stepsize):\n",
        "      for gamma in np.arange(gamma_min, gamma_max, gamma_stepsize):\n",
        "        key_gamma =  'gamma_' + str(gamma)\n",
        "        cluster_labels = sa_spectral_clustering_AM(connected_affinity_matrices[key_gram][key_gamma], no_clusters)\n",
        "        sil_avg = silhouette_score(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "        if sil_avg > best_sil_avg:\n",
        "          best_sil_avg = sil_avg\n",
        "          best_sil_coeffs = silhouette_samples(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "          best_gamma = gamma\n",
        "          best_no_clusters = no_clusters\n",
        "          best_cluster_labels = cluster_labels\n",
        "\n",
        "    #output silhouette plots\n",
        "    if (best_sil_avg > 0):\n",
        "      #output silhouette plots\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'silhouette_plot_SPCluster_AM_NGram_' + str(n) +'_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "      output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "      #output MDS plots\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'MDS_plot_SPCluster_AM_NGram_' + str(n) +'_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "      closest_to_mean_indices, _ = closest_to_mean_js_norm(traces_ngrams['ProbArray_' + str(n)], best_cluster_labels)\n",
        "      stress = mds_2d_cluster_plot(outputfilename, dist_matrices[key_gram], best_cluster_labels, closest_to_mean_indices)\n",
        "\n",
        "      #output cluster centroids (probability distributions in this case)\n",
        "      col_name = 'ProbDict_' + str(n)\n",
        "      closest_to_mean_indices_arr = [value for value in closest_to_mean_indices.values()]\n",
        "      #cluster_centres = traces_ngrams[col_name].iloc[closest_to_mean_indices_arr]\n",
        "      cluster_centres = traces_ngrams[col_name].loc[closest_to_mean_indices_arr]\n",
        "      labels = ['Cluster ' + str(n) for n in range(0, len(closest_to_mean_indices_arr))]\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'cluster_centroids_SPCluster_AM_NGram_' + str(n) +'_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "      plot_distribution_comparison_vert(outputfilename, cluster_centres, labels, thresholds[n])\n",
        "\n",
        "      #output values\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'best_silhouette_avg_SPCluster_AM_NGram_' + str(n) +'_gamma_' + str(round(best_gamma,2)) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "      values = [best_sil_avg, best_gamma, best_no_clusters, stress]\n",
        "      valueslabels_list = ['sil_avg', 'gamma', 'clusters', 'MDS_stress']\n",
        "      output_values(outputfilename, values, valueslabels_list)\n",
        "    else:\n",
        "      print(\"No clustering found for Spectral Clustering with fully connected affinity matrix for NGram playtraces with N=\" + str(n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NP4YJ48R_XtQ",
      "metadata": {
        "id": "NP4YJ48R_XtQ"
      },
      "outputs": [],
      "source": [
        "if run_config_cardcount['SP_KNN']:\n",
        "  print(\"Performing spectral clustering for card count playtraces using K-Nearest Neighbours affinity matrix with l_k-norm...\")\n",
        "\n",
        "  for k in k_norms:\n",
        "    key_norm = 'CardCount_lknorm_' + str(k)\n",
        "    best_sil_avg = -10000\n",
        "    best_sil_coeffs = None\n",
        "    best_nn = 0\n",
        "    best_no_clusters = 0\n",
        "    best_cluster_labels = None\n",
        "    for no_clusters in range(clusters_min, clusters_max, clusters_stepsize):\n",
        "      for nn in np.arange(nearest_neighbours_min, nearest_neighbours_max, nearest_neighbours_stepsize):\n",
        "        key_knn =  'knn_' + str(nn)\n",
        "        cluster_labels = sa_spectral_clustering_AM(knn_affinity_matrices[key_norm][key_knn], no_clusters)\n",
        "        sil_avg = silhouette_score(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "        if sil_avg > best_sil_avg:\n",
        "          best_sil_avg = sil_avg\n",
        "          best_sil_coeffs = silhouette_samples(dist_matrices[key_norm], cluster_labels, metric = 'precomputed')\n",
        "          best_nn = nn\n",
        "          best_no_clusters = no_clusters\n",
        "          best_cluster_labels = cluster_labels\n",
        "\n",
        "    #output silhouette plots\n",
        "    if (best_sil_avg > 0):\n",
        "      #output silhouette plots\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'silhouette_plot_SPCluster_KNN_CardCount_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels)\n",
        "\n",
        "      #output MDS plots\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'MDS_plot_SPCluster_KNN_CardCount_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      closest_to_mean_indices, _ = closest_to_mean_l_k_norm(traces_cardcount_slim, best_cluster_labels, k)\n",
        "      stress = mds_2d_cluster_plot(outputfilename, dist_matrices[key_norm], best_cluster_labels, closest_to_mean_indices)\n",
        "\n",
        "      #output cluster centroids, converting indices to playtraces\n",
        "      closest_to_mean_indices_arr = [value for value in closest_to_mean_indices.values()]\n",
        "      #cluster_centres = traces_cardcount_slim.iloc[closest_to_mean_indices_arr]\n",
        "      cluster_centres = traces_cardcount_slim.loc[closest_to_mean_indices_arr]\n",
        "      labels = ['Cluster ' + str(n) for n in range(0, len(closest_to_mean_indices_arr))]\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'cluster_centroids_SPCluster_KNN_CardCount_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      plot_cluster_centroids(outputfilename, cluster_centres, card_types, labels)\n",
        "\n",
        "      #output values\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'best_silhouette_avg_SPCluster_KNN_CardCount_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_k_' + str(k) + '_' + tag_for_dir_and_filenames\n",
        "      values = [best_sil_avg, best_nn, best_no_clusters, stress]\n",
        "      valueslabels_list = ['sil_avg', 'k-NearestNeighbours', 'clusters', 'MDS_stress']\n",
        "      output_values(outputfilename, values, valueslabels_list)\n",
        "    else:\n",
        "      print(\"No clustering found for Spectral Clustering with k_Nearest Neighbours affinity matrix for card count playtraces with l_\" + str(k) + \"-norm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xnjklWxeqxIy",
      "metadata": {
        "id": "xnjklWxeqxIy"
      },
      "outputs": [],
      "source": [
        "if run_config_ngrams['SP_KNN']:\n",
        "  print(\"Performing spectral clustering for N-Gram playtraces using K-Nearest Neighbours affinity matrix with Jensen-Shannon...\")\n",
        "\n",
        "  for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "    key_gram = 'N_Gram_' + str(n)\n",
        "    best_sil_avg = -10000\n",
        "    best_sil_coeffs = None\n",
        "    best_nn = 0\n",
        "    best_no_clusters = 0\n",
        "    best_cluster_labels = None\n",
        "    for no_clusters in range(clusters_min, clusters_max, clusters_stepsize):\n",
        "      for nn in np.arange(nearest_neighbours_min, nearest_neighbours_max, nearest_neighbours_stepsize):\n",
        "        key_knn =  'knn_' + str(nn)\n",
        "        cluster_labels = sa_spectral_clustering_AM(knn_affinity_matrices[key_gram][key_knn], no_clusters)\n",
        "        sil_avg = silhouette_score(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "        if sil_avg > best_sil_avg:\n",
        "          best_sil_avg = sil_avg\n",
        "          best_sil_coeffs = silhouette_samples(dist_matrices[key_gram], cluster_labels, metric = 'precomputed')\n",
        "          best_nn = nn\n",
        "          best_no_clusters = no_clusters\n",
        "          best_cluster_labels = cluster_labels\n",
        "\n",
        "    #output silhouette plots\n",
        "    if (best_sil_avg > 0):\n",
        "      #output silhouette plots\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'silhouette_plot_SPCluster_AM_NGram_' + str(n) + '_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "      output_silhouette_plot(outputfilename, best_sil_coeffs, best_sil_avg, best_cluster_labels, flip_clusters = False)\n",
        "\n",
        "      #output MDS plots\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'MDS_plot_SPCluster_AM_NGram_' + str(n) + '_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "      closest_to_mean_indices, _ = closest_to_mean_js_norm_v2(traces_ngrams, best_cluster_labels, 'ProbArray_' + str(n))\n",
        "      stress = mds_2d_cluster_plot(outputfilename, dist_matrices[key_gram], best_cluster_labels, closest_to_mean_indices, medoid_indices = {}, high_density_indices = {}, output_to_screen = False,\n",
        "                        flip_two_clusters = True)\n",
        "\n",
        "      #output cluster centroids (probability distributions in this case)\n",
        "      col_name = 'ProbDict_' + str(n)\n",
        "      closest_to_mean_indices_arr = [value for value in closest_to_mean_indices.values()]\n",
        "      #cluster_centres = traces_ngrams[col_name].iloc[closest_to_mean_indices_arr]\n",
        "      cluster_centres = traces_ngrams[col_name].loc[closest_to_mean_indices_arr]\n",
        "      labels = ['Cluster ' + str(n) for n in range(0, len(closest_to_mean_indices_arr))]\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'cluster_centroids_SPCluster_AM_NGram_' + str(n) + '_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "      plot_distribution_comparison_vert(outputfilename, cluster_centres, labels, thresholds[n], flip_clusters = False)\n",
        "\n",
        "      #output values\n",
        "      outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Results_SPClustering/' + 'best_silhouette_avg_SPCluster_AM_NGram_' + str(n) + '_kNN_' + str(best_nn) + '_N_' + str(best_no_clusters) + '_' + tag_for_dir_and_filenames\n",
        "      values = [best_sil_avg, best_nn, best_no_clusters, stress]\n",
        "      valueslabels_list = ['sil_avg', 'k-NearestNeighbours', 'clusters', 'MDS_stress']\n",
        "      output_values(outputfilename, values, valueslabels_list)\n",
        "    else:\n",
        "      print(\"No clustering found for Spectral Clustering with k_Nearest Neighbours affinity matrix for N-Gram playtraces with N=\" + str(n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PEAYxyffv74V",
      "metadata": {
        "id": "PEAYxyffv74V"
      },
      "outputs": [],
      "source": [
        "#finally compute the average playtraces and playtraces closet to that average to use in the cases where we see little to no clustering\n",
        "data_labels_list = np.ones(len(traces_cardcount_slim))\n",
        "labels = ['Mean playtrace', 'Closest to mean playtrace']\n",
        "\n",
        "#firstly for card count playtraces\n",
        "if run_config_cardcount['Total_avg']:\n",
        "  for k in k_norms:\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Total_Average_Playtraces/' + 'Total_avg_trace_cardcount_' + str(k)\n",
        "    closest_to_mean_index, mean_trace = closest_to_mean_l_k_norm(traces_cardcount_slim, data_labels_list, k)\n",
        "    closest_to_mean_trace = traces_cardcount_slim.iloc[closest_to_mean_index[1]]\n",
        "    #traces = [mean_trace[1], closest_to_mean_trace]\n",
        "    traces = [mean_trace[1]]\n",
        "    labels = ['Mean playtrace']\n",
        "    #traces_arr = np.array([np.array(traces[0]), np.array(traces[1])])\n",
        "    traces_arr = np.array([np.array(traces[0])])\n",
        "    plot_cluster_centroids(outputfilename, traces_arr, card_types, labels)\n",
        "\n",
        "#secondly for n-gram playtraces\n",
        "if run_config_ngrams['Total_avg']:\n",
        "  for n in range(ngram_min, ngram_max + 1, ngram_stepsize):\n",
        "    outputfilename = google_drive_parent_dir + 'Results/' + tag_for_dir_and_filenames + '/Total_Average_Playtraces/' + 'Total_avg_trace_ngrams_N_' + str(n)\n",
        "    closest_to_mean_index, mean_trace = closest_to_mean_js_norm_v2(traces_ngrams, data_labels_list, 'ProbArray_' + str(n))\n",
        "    closest_to_mean_trace = traces_ngrams['ProbDict_' + str(n)].iloc[closest_to_mean_index[1]]\n",
        "    #traces = [prob_array_to_dict(mean_trace[1],all_ngrams_list[n]), closest_to_mean_trace]\n",
        "    traces = [prob_array_to_dict(mean_trace[1],all_ngrams_list[n])]\n",
        "    labels = ['Mean playtrace']\n",
        "    pdb.set_trace()\n",
        "    plot_distribution_comparison_vert(outputfilename, traces, labels, 0.015)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HFfbaG6Zhlsp",
      "metadata": {
        "id": "HFfbaG6Zhlsp"
      },
      "outputs": [],
      "source": [
        "#filter on non-zero entries and N-Grams that contain a particular action\n",
        "#action_list = ['PLAYSMITHY', 'PLAYVILLAGE', 'PLAYMARKET']\n",
        "#vp_list = ['BUYPROVINCE', 'BUYDUCHY']\n",
        "#total_prob = 0\n",
        "#tmp = {key: value for key, value in prob_array_to_dict(mean_trace[1],all_ngrams_list[n]).items() if value > 0}\n",
        "#for gram, prob in tmp.items():\n",
        "#  if gram[0] in action_list and gram[1] in action_list and gram[2] in vp_list:\n",
        "#    total_prob += prob\n",
        "#    print(\"N-Gram: \" + str(gram) + \" Prob: \" + str(prob))\n",
        "#print(\"Total probability: \" + str(total_prob*100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}